<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/./atom.xml" title="做一个幸福的人" type="application/atom+xml"><meta name="google-site-verification" content="Qr_3yqLtyErDFKHW7mE8PJz4qDUX-bf_fMLpSRckQe4"><meta name="baidu-site-verification" content="zOwIvKMV7f"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"搜索文章",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet"><meta name="description" content="参考：神经网络推导-矩阵计算神经网络实现-numpy以LeNet-5为例，进行卷积神经网络的矩阵推导计算符号"><meta name="keywords" content="微积分,线性代数"><meta property="og:type" content="article"><meta property="og:title" content="卷积神经网络推导-单张图片矩阵计算"><meta property="og:url" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;posts&#x2F;3accb62a.html"><meta property="og:site_name" content="做一个幸福的人"><meta property="og:description" content="参考：神经网络推导-矩阵计算神经网络实现-numpy以LeNet-5为例，进行卷积神经网络的矩阵推导计算符号"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;卷积神经网络推导-矩阵计算&#x2F;LeNet-5.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;卷积神经网络推导-矩阵计算&#x2F;LeNet-5_v2.png"><meta property="og:updated_time" content="2020-02-15T05:36:35.871Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;卷积神经网络推导-矩阵计算&#x2F;LeNet-5.png"><link rel="canonical" href="https://www.zhujian.tech/posts/3accb62a.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>卷积神经网络推导-单张图片矩阵计算 | 做一个幸福的人</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e677aac1ac69b8826b9cfecb4e72e107";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">做一个幸福的人</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">面朝大海，春暖花开</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">129</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">48</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">169</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header> <a href="https://github.com/zjZSTU" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zhujian.tech/posts/3accb62a.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="zhujian"><meta itemprop="description" content="one bite at a time"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="做一个幸福的人"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 卷积神经网络推导-单张图片矩阵计算</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-05-20 17:13:24" itemprop="dateCreated datePublished" datetime="2019-05-20T17:13:24+00:00">2019-05-20</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-15 05:36:35" itemprop="dateModified" datetime="2020-02-15T05:36:35+00:00">2020-02-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/" itemprop="url" rel="index"><span itemprop="name">算法</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/deep-learning/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/math/" itemprop="url" rel="index"><span itemprop="name">数学</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/deep-learning/convolutional-neural-network/" itemprop="url" rel="index"><span itemprop="name">卷积神经网络</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>14k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>24 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>参考：</p><p><a href="https://www.zhujian.tech/posts/1dd3ebad.html#more">神经网络推导-矩阵计算</a></p><p><a href="https://www.zhujian.tech/posts/ba2ca878.html#more">神经网络实现-numpy</a></p><p>以<code>LeNet-5</code>为例，进行卷积神经网络的矩阵推导</p><h2 id="计算符号"><a href="#计算符号" class="headerlink" title="计算符号"></a>计算符号</h2><a id="more"></a><p>参考<a href="https://link.zhihu.com/?target=http%3A//yann.lecun.com/exdb/publis/pdf/lecun-98.pdf">lecun-98</a>，卷积层标记为$Cx$，池化层标记为$Sx$，全连接层标记为$Fx$，比如$C1$表示第一层是卷积层</p><h2 id="LeNet-5简介"><a href="#LeNet-5简介" class="headerlink" title="LeNet-5简介"></a><code>LeNet-5</code>简介</h2><p><code>LeNet-5</code>共有<code>7</code>层（不包含输入层）</p><p><img src="../imgs/卷积神经网络推导-矩阵计算/LeNet-5.png" alt></p><ul><li><p>$C1$有<code>6</code>个滤波器，感受野尺寸为$5\times 5$，步长为$1$，零填充为$0$，所以$C1$共有$5\cdot 5\cdot 6+6=156$个训练参数，输出激活图尺寸为$28\times 28$</p></li><li><p>$S2$感受野大小为$2\times 2$，步长为$2$，将输入数据体4个神经元相加后乘以一个可训练参数，再加上一个偏置值，最后进行$sigmoid$操作，所以$S2$共有$2\times 6=12$个训练参数，输出激活图尺寸为$14\times 14$</p></li><li><p>$C3$有<code>16</code>个滤波器，感受野尺寸为$5\times 5$，步长为$1$, 零填充为$0$，所以输出激活图尺寸为$10\times 10$</p><ul><li>$C3$的滤波器没有和$S2$完全连接：前6个滤波器和$S2$连续的3个激活图交互，接下来6个滤波器和$S2$连续的4个激活图交互，接下来3个滤波器和$S2$不连续的4个激活图交互，最后一个滤波器和$S2$全连接</li><li>$C3$的参数个数是$(5\cdot 5)\cdot (3\cdot 6+4\cdot 6+4\cdot 3+6\cdot 1)+16=1516$</li></ul></li><li><p>$S4$和$S2$一样，所以共有$2\times 16=32$个可学习参数，输出激活图尺寸为$5\times 5$</p></li><li><p>$C5$有<code>120</code>个滤波器，感受野大小为$5\times 5$，步长为$1$，零填充为$0$，所以$C5$共有$5\cdot 5\cdot 16\cdot 120+120=48120$个参数，输出激活图尺寸为$1\times 1$，输出大小为$1\times 1\times 120$</p></li><li><p>$F6$有<code>84</code>个神经元，激活函数是$tanh$，参数个数是$120\times 84+84=10164$个训练参数</p></li><li><p>$F7$有<code>10</code>个神经元，得到84个输入后，不再执行点积运算，而是执行欧氏径向基函数（euclidean radial basis function）：</p><script type="math/tex;mode=display">
  y_{i}=\sum_{j}(x_{i}-w_{ij})^{2}</script></li><li><p>损失函数是均方误差（<code>Mean Squared Error, MSE</code>）</p></li></ul><p>最早的<code>LeNet-5</code>在<code>1998</code>年提出，经过多年发展，一些实现细节发生了变化，其中一个版本如下</p><p><img src="../imgs/卷积神经网络推导-矩阵计算/LeNet-5_v2.png" alt></p><ul><li><p>$C1$有<code>6</code>个滤波器，感受野尺寸为$5\times 5$，步长为$1$，零填充为$0$，激活函数是$relu$，所以$C1$共有$5\cdot 5\cdot 6+6=156$个训练参数，输出激活图尺寸为$28\times 28$</p></li><li><p>$S2$感受野大小为$2\times 2$，步长为$2$，使用$\max$运算，所以输出激活图尺寸为$14\times 14$</p></li><li><p>$C3$有<code>16</code>个滤波器，感受野尺寸为$5\times 5$，步长为$1$, 零填充为$0$，激活函数是$relu$，所以$C3$参数个数是$5\cdot 5\cdot 6\cdot 16+16=2416$，输出激活图尺寸为$10\times 10$</p></li><li><p>$S4$和$S2$一样，步长为$2$，使用$\max$运算，所以输出激活图尺寸为$5\times 5$</p></li><li><p>$C5$有<code>120</code>个滤波器，感受野大小为$5\times 5$，步长为$1$，零填充为$0$，激活函数为$relu$，所以$C5$共有$5\cdot 5\cdot 16\cdot 120+120=48120$，输出激活图尺寸为$1\times 1$，输出大小为$1\times 120$</p></li><li><p>$F6$有<code>84</code>个神经元，激活函数是$relu$，参数个数是$120\times 84+84=10164$个训练参数，输出大小为$1\times 84$</p></li><li><p>$F7$有<code>10</code>个神经元，参数个数是$84\times 10+10=850$个，输出大小为$1\times 10$</p></li><li><p>评分函数使用$softmax$</p></li><li><p>损失函数是交叉熵损失（<code>Cross Entropy Loss</code>）</p></li></ul><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">输入</th><th style="text-align:center">卷积核</th><th style="text-align:center">步长</th><th style="text-align:center">零填充</th><th style="text-align:center">输出</th></tr></thead><tbody><tr><td style="text-align:center">C1</td><td style="text-align:center">32x32x3</td><td style="text-align:center">5x5</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">28x28x6</td></tr><tr><td style="text-align:center">S2</td><td style="text-align:center">28x28x6</td><td style="text-align:center">2x2</td><td style="text-align:center">2</td><td style="text-align:center">\</td><td style="text-align:center">14x14x6</td></tr><tr><td style="text-align:center">C3</td><td style="text-align:center">14x14x6</td><td style="text-align:center">5x5</td><td style="text-align:center">1</td><td style="text-align:center">0</td><td style="text-align:center">10x10x16</td></tr><tr><td style="text-align:center">S4</td><td style="text-align:center">10x10x16</td><td style="text-align:center">2x2</td><td style="text-align:center">2</td><td style="text-align:center">\</td><td style="text-align:center">5x5x16</td></tr><tr><td style="text-align:center">C5</td><td style="text-align:center">5x5x16</td><td style="text-align:center">120</td><td style="text-align:center">0</td><td style="text-align:center">0</td><td style="text-align:center">1x120</td></tr><tr><td style="text-align:center">F6</td><td style="text-align:center">1x120</td><td style="text-align:center">84</td><td style="text-align:center">\</td><td style="text-align:center">\</td><td style="text-align:center">1x84</td></tr><tr><td style="text-align:center">F7</td><td style="text-align:center">1x84</td><td style="text-align:center">10</td><td style="text-align:center">\</td><td style="text-align:center">\</td><td style="text-align:center">1x10</td></tr></tbody></table></div><h2 id="卷积层转全连接层"><a href="#卷积层转全连接层" class="headerlink" title="卷积层转全连接层"></a>卷积层转全连接层</h2><p>参考：<a href="http://cs231n.github.io/convolutional-networks/#fc" target="_blank" rel="noopener">Implementation as Matrix Multiplication</a></p><p>卷积层滤波器在输入数据体的局部区域执行点积操作，将每次局部连接数据体拉伸为行向量，那么卷积操作就等同于矩阵乘法，变成全连接层运算</p><p>比如输入图像大小为$32\times 32\times 3$，卷积层滤波器大小为$5\times 5\times 3$，步长为$1$，零填充为$0$，共有6个</p><p>那么等同于全连接层的输入维度为$5\cdot 5\cdot 3=125$，共有$((32-5)/1+1)=784$个局部连接，所以矩阵运算如下：</p><script type="math/tex;mode=display">
X\in R^{784\times 125}\\
W\in R^{125\times 6}\\
b\in R^{1\times 6}\\
Y=X\cdot W+B\in R^{784\times 6}</script><p>得到输出数据$Y$后再拉伸回$55\times 55\times 6$，就是下一层的输入数据体</p><h2 id="池化层转全连接层"><a href="#池化层转全连接层" class="headerlink" title="池化层转全连接层"></a>池化层转全连接层</h2><p>池化层滤波器在输入数据体的激活图上执行$\max$操作，将每次局部连接区域拉伸为行向量，同样可以将池化层操作转换成全连接层操作</p><p>比如输入数据体大小为$28\times 28\times 6$，池化层滤波器空间尺寸$2\times 2$，步长为$2$</p><p>那么每次连接的向量大小是$2\cdot 2=4$，每个激活图共有$(28/2)^{2}=196$次局部连接，整个输入数据体共有$196\cdot 6=1176$次局部连接，所以运算如下：</p><script type="math/tex;mode=display">
X\in R^{1176\times 4}\\
Y = \max(X)\in R^{1176\times 1}</script><p>得到输出数据$Y$后再拉伸回$14\times 14\times 6$，就是下一层的输入数据体</p><h2 id="矩阵计算"><a href="#矩阵计算" class="headerlink" title="矩阵计算"></a>矩阵计算</h2><p>进行MNIST数据集的分类</p><h3 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h3><p><strong>输入层</strong></p><script type="math/tex;mode=display">
X\in R^{32\times 32\times 1}</script><p><strong>卷积层$C1$</strong></p><p>共6个滤波器，每个滤波器空间尺寸为$5\times 5$，步长为$1$, 零填充为$0$</p><p>输出空间尺寸为$(32-5-2\cdot 0)/1+1=28$</p><p>所以单次卷积操作的向量大小为$5\cdot 5\cdot 1=25$，单个滤波器有$28\cdot 28=784$个局部连接</p><script type="math/tex;mode=display">
a^{(0)}\in R^{784\times 25}\\
W^{(1)}\in R^{25\times 6}\\
b^{(1)}\in R^{1\times 6}\\
\Rightarrow z^{(1)}=a^{(0)}\cdot W^{(1)}+b^{(1)}\in R^{784\times 6}\\
\Rightarrow y^{(1)}=relu(z^{(1)})\\</script><p>输出数据体$output^{(1)}\in R^{28\times 28\times 6}$</p><p><strong>池化层$S2$</strong></p><p>执行$\max$运算，每个滤波器空间尺寸$2\times 2$，步长为$2$</p><p>输出空间尺寸为$(28-2)/2+1=14$</p><p>所以单次$\max$操作的向量大小为$2\cdot 2=4$，单个滤波器有$14\cdot 14\cdot 6=1176$个局部连接</p><script type="math/tex;mode=display">
a^{(1)}\in R^{1176\times 4}\\
z^{(2)}=\max (a^{(1)})\in R^{1176\times 1}</script><p>$argz^{(2)} = argmax(a^{(1)})\in R^{1176}$，每个值表示$a^{(1)}$中每行最大值下标</p><p>输出数据体$output^{(2)}\in R^{14\times 14\times 6}$</p><p><strong>卷积层$C3$</strong></p><p>共16个滤波器，每个滤波器空间尺寸为$5\times 5$，步长为$1$, 零填充为$0$</p><p>输出空间尺寸为$(14-5+2\cdot 0)/1+1=10$</p><p>所以单次卷积操作的向量大小为$5\cdot 5\cdot 6=150$，单个滤波器有$10\cdot 10=100$个局部连接</p><script type="math/tex;mode=display">
a^{(2)}\in R^{100\times 150}\\
W^{(3)}\in R^{150\times 16}\\
b^{(3)}\in R^{1\times 16}\\
\Rightarrow z^{(3)}=a^{(2)}\cdot W^{(3)}+b^{(3)}\in R^{100\times 16}\\
\Rightarrow y^{(3)}=relu(z^{(3)})\\</script><p>输出数据体$output^{(3)}\in R^{10\times 10\times 16}$</p><p><strong>池化层$S4$</strong></p><p>执行$\max$运算，每个滤波器空间尺寸$2\times 2$，步长为$2$</p><p>输出空间尺寸为$(10-2)/2+1=5$</p><p>所以单次$\max$操作的向量大小为$2\cdot 2=4$，单个滤波器有$5\cdot 5\cdot 16=400$个局部连接</p><script type="math/tex;mode=display">
a^{(3)}\in R^{400\times 4}\\
z^{(4)}=\max (a^{(3)})\in R^{400\times 1}</script><p>$argz^{(4)} = argmax(a^{(3)})\in R^{400}$，每个值表示$a^{(3)}$中每行最大值下标</p><p>输出数据体$output^{(4)}\in R^{5\times 5\times 16}$</p><p><strong>卷积层$C5$</strong></p><p>共120个滤波器，每个滤波器空间尺寸为$5\times 5$，步长为$1$, 零填充为$0$</p><p>输出空间尺寸为$(5-5+2\cdot 0)/1+1=1$</p><p>所以单次卷积操作的向量大小为$5\cdot 5\cdot 16=400$，单个滤波器有$1\cdot 1=1$个局部连接</p><script type="math/tex;mode=display">
a^{(4)}\in R^{1\times 400}\\
W^{(5)}\in R^{400\times 120}\\
b^{(5)}\in R^{1\times 120}\\
\Rightarrow z^{(5)}=a^{(4)}\cdot W^{(5)}+b^{(5)}\in R^{1\times 120}\\
\Rightarrow y^{(5)}=relu(z^{(5)})\\</script><p>输出数据体$output^{(5)}\in R^{1\times 120}$</p><p><strong>全连接层$F6$</strong></p><p>神经元个数为$84$</p><script type="math/tex;mode=display">
a^{(5)}=y^{(5)}\in R^{1\times 120}\\
W^{(6)}\in R^{120\times 84}\\
b^{(6)}\in R^{1\times 84}\\
\Rightarrow z^{(6)}=a^{(5)}\cdot W^{(6)}+b^{(6)}\in R^{1\times 84}\\
\Rightarrow y^{(6)}=relu(z^{(6)})\\</script><p>输出数据体$output^{(6)}\in R^{1\times 84}$</p><p><strong>输出层$F7$</strong></p><p>神经元个数为$10$</p><script type="math/tex;mode=display">
a^{(6)}=y^{(6)}\in R^{1\times 84}\\
W^{(7)}\in R^{84\times 10}\\
b^{(7)}\in R^{1\times 10}\\
\Rightarrow z^{(7)}=a^{(6)}\cdot W^{(7)}+b^{(7)}\in R^{1\times 10}\\</script><p>输出数据体$output^{(7)}\in R^{1\times 10}$</p><p><strong>分类概率</strong></p><script type="math/tex;mode=display">
probs=h(z^{(7)})=\frac {exp(z^{(7)})}{exp(z^{(7)})\cdot A\cdot B^{T}}</script><p><strong>$A\in R^{10\times 1}, B\in R^{10\times 1}$都是全$1$向量</strong></p><p><strong>损失值</strong></p><script type="math/tex;mode=display">
dataLoss = -\frac {1}{N} 1^{T}\cdot \ln \frac {exp(z^{(7)}* Y\cdot A)}{exp(z^{(7)})\cdot A}</script><script type="math/tex;mode=display">
regLoss = 0.5\cdot reg\cdot (||W^{(1)}||^{2} + ||W^{(3)}||^{2} + ||W^{(5)}||^{2} + ||W^{(6)}||^{2} + ||W^{(7)}||^{2})</script><script type="math/tex;mode=display">
J(z^{(7)})=dataLoss + regLoss</script><p><strong>$Y\in R^{1\times 10}$，仅有正确类别为<code>1</code>, 其余为<code>0</code></strong></p><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p><strong>输出层$F7$</strong></p><p>求输入向量$z^{(7)}$梯度</p><script type="math/tex;mode=display">
d(dataloss)=d(1^{T}\cdot \ln \frac {exp(z^{(7)}* Y\cdot A)}{exp(z^{(7)})\cdot A})=tr((probs^{T} - Y^{T})\cdot dz^{(7)})</script><script type="math/tex;mode=display">
\Rightarrow D_{z^{(7)}}f(z^{(7)})=probs^{T} - Y^{T}\\
\Rightarrow \bigtriangledown_{z^{(7)}}f(z^{(7)})=probs - Y</script><p>其他梯度</p><script type="math/tex;mode=display">
z^{(7)}=a^{(6)}\cdot W^{(7)}+b^{(7)} \\
dz^{(7)}=da^{(6)}\cdot W^{(7)} + a^{(6)}\cdot dW^{(7)} + db^{(7)}\\
d(dataloss)=tr(D_{z^{(7)}}f(z^{(7)})\cdot dz^{(7)})\\
=tr(D_{z^{(7)}}f(z^{(7)})\cdot (da^{(6)}\cdot W^{(7)} + a^{(6)}\cdot dW^{(7)} + db^{(7)}))\\
=tr(D_{z^{(7)}}f(z^{(7)})\cdot da^{(6)}\cdot W^{(7)})
+tr(D_{z^{(7)}}f(z^{(7)})\cdot a^{(6)}\cdot dW^{(7)})
+tr(D_{z^{(7)}}f(z^{(7)})\cdot db^{(7)}))</script><p>求权重矩阵$W^{(7)}$梯度</p><script type="math/tex;mode=display">
d(dataloss)=tr(D_{z^{(7)}}f(z^{(7)})\cdot a^{(6)}\cdot dW^{(7)})</script><script type="math/tex;mode=display">
\Rightarrow D_{W^{(7)}}f(W^{(7)})=D_{z^{(7)}}f(z^{(7)})\cdot a^{(6)}\\
\Rightarrow \bigtriangledown_{W^{(7)}}f(W^{(7)})=(a^{(6)})^{T}\cdot \bigtriangledown_{z^{(7)}}f(z^{(7)})</script><p>求偏置向量$b^{(7)}$梯度</p><script type="math/tex;mode=display">
d(dataloss)=tr(D_{z^{(7)}}f(z^{(7)})\cdot db^{(7)}))</script><script type="math/tex;mode=display">
\Rightarrow D_{b^{(7)}}f(b^{(7)})=D_{z^{(7)}}f(z^{(7)})\\
\Rightarrow \bigtriangledown_{b^{(7)}}f(b^{(7)})=\bigtriangledown_{z^{(7)}}f(z^{(7)})</script><p>求上一层输出向量$a^{(6)}$梯度</p><script type="math/tex;mode=display">
d(dataloss)=tr(D_{z^{(7)}}f(z^{(7)})\cdot da^{(6)}\cdot W^{(7)})
=tr(W^{(7)}\cdot D_{z^{(7)}}f(z^{(7)})\cdot da^{(6)})</script><script type="math/tex;mode=display">
\Rightarrow D_{a^{(6)}}f(a^{(6)})=W^{(7)}\cdot D_{z^{(7)}}f(z^{(7)})\\
\Rightarrow \bigtriangledown_{a^{(6)}}f(a^{(6)})=\bigtriangledown_{z^{(7)}}f(z^{(7)})\cdot (W^{(7)})^{T}</script><p><strong>全连接层$F6$</strong></p><p>求输入向量$z^{(6)}$梯度</p><script type="math/tex;mode=display">
a^{(6)}=y^{(6)}=relu(z^{(6)})\\
da^{(6)}=1(z^{(6)}\geq 0)* dz^{(6)}</script><script type="math/tex;mode=display">
d(dataloss)=tr(D_{a^{(6)}}f(a^{(6)}) da^{(6)})=tr(D_{a^{(6)}}f(a^{(6)})\cdot (1(z^{(6)}\geq 0)* dz^{(6)}))\\
=tr(D_{a^{(6)}}f(a^{(6)})* 1(z^{(6)}\geq 0)^{T}\cdot dz^{(6)})</script><script type="math/tex;mode=display">
\Rightarrow D_{z^{(6)}}f(z^{(6)})=D_{a^{(6)}}f(a^{(6)})* 1(z^{(6)}\geq 0)^{T}\\
\Rightarrow \bigtriangledown_{z^{(6)}}f(z^{(6)})=\bigtriangledown_{a^{(6)}}f(a^{(6)})* 1(z^{(6)}\geq 0)</script><p>其他梯度</p><script type="math/tex;mode=display">
z^{(6)}=a^{(5)}\cdot W^{(6)}+b^{(6)} \\
dz^{(6)}=da^{(5)}\cdot W^{(6)}+a^{(5)}\cdot dW^{(6)}+db^{(6)}\\
d(dataloss)=tr(D_{z^{(6)}}f(z^{(6)})\cdot dz^{(6)})\\
=tr(D_{z^{(6)}}f(z^{(6)})\cdot (da^{(5)}\cdot W^{(6)} + a^{(5)}\cdot dW^{(6)} + db^{(6)}))\\
=tr(D_{z^{(6)}}f(z^{(6)})\cdot da^{(5)}\cdot W^{(6)})
+tr(D_{z^{(6)}}f(z^{(6)})\cdot a^{(5)}\cdot dW^{(6)})
+tr(D_{z^{(6)}}f(z^{(6)})\cdot db^{(6)}))</script><p>求权重矩阵$w^{(6)}$梯度</p><script type="math/tex;mode=display">
d(dataloss)=tr(D_{z^{(6)}}f(z^{(6)})\cdot a^{(5)}\cdot dW^{(6)})</script><script type="math/tex;mode=display">
\Rightarrow D_{W^{(6)}}f(W^{(6)})=D_{z^{(6)}}f(z^{(6)})\cdot a^{(5)}\\
\Rightarrow \bigtriangledown_{W^{(6)}}f(W^{(6)})=(a^{(5)})^{T}\cdot \bigtriangledown_{z^{(6)}}f(z^{(6)})</script><p>求偏置向量$b^{(6)}$梯度</p><script type="math/tex;mode=display">
d(dataloss)=tr(D_{z^{(6)}}f(z^{(6)})\cdot db^{(6)}))</script><script type="math/tex;mode=display">
\Rightarrow D_{b^{(6)}}f(b^{(6)})=D_{z^{(6)}}f(z^{(6)})\\
\Rightarrow \bigtriangledown_{b^{(6)}}f(b^{(6)})=\bigtriangledown_{z^{(6)}}f(z^{(6)})</script><p>求上一层输出向量$a^{(5)}$梯度</p><script type="math/tex;mode=display">
d(dataloss)=tr(D_{z^{(6)}}f(z^{(6)})\cdot da^{(5)}\cdot W^{(6)})
=tr(W^{(6)}\cdot D_{z^{(6)}}f(z^{(6)})\cdot da^{(5)})</script><script type="math/tex;mode=display">
\Rightarrow D_{a^{(5)}}f(a^{(5)})=W^{(6)}\cdot D_{z^{(6)}}f(z^{(6)})\\
\Rightarrow \bigtriangledown_{a^{(5)}}f(a^{(5)})=\bigtriangledown_{z^{(6)}}f(z^{(6)})\cdot (W^{(6)})^{T}</script><p><strong>卷积层$C5$</strong></p><p>求输入向量$z^{(5)}$梯度</p><script type="math/tex;mode=display">
a^{(5)}=y^{(5)}=relu(z^{(5)})\\
da^{(5)}=1(z^{(5)}\geq 0)* dz^{(5)}\\</script><script type="math/tex;mode=display">
d(dataloss)=tr(D_{a^{(5)}}f(a^{(5)}) da^{(5)})=tr(D_{a^{(5)}}f(a^{(5)})\cdot (1(z^{(5)}\geq 0)* dz^{(5)}))\\
=tr(D_{a^{(5)}}f(a^{(5)})* 1(z^{(5)}\geq 0)^{T}\cdot dz^{(5)})</script><script type="math/tex;mode=display">
\Rightarrow D_{z^{(5)}}f(z^{(5)})=D_{a^{(5)}}f(a^{(5)})* 1(z^{(5)}\geq 0)^{T}\\
\Rightarrow \bigtriangledown_{z^{(5)}}f(z^{(5)})=\bigtriangledown_{a^{(5)}}f(a^{(5)})* 1(z^{(5)}\geq 0)</script><p>其他梯度</p><script type="math/tex;mode=display">
z^{(5)}=a^{(4)}\cdot W^{(5)}+b^{(5)} \\
dz^{(5)}=da^{(4)}\cdot W^{(5)}+a^{(4)}\cdot dW^{(5)}+db^{(5)}\\
d(dataloss)=tr(D_{z^{(5)}}f(z^{(5)})\cdot dz^{(5)})\\
=tr(D_{z^{(5)}}f(z^{(5)})\cdot (da^{(4)}\cdot W^{(5)} + a^{(4)}\cdot dW^{(5)} + db^{(5)}))\\
=tr(D_{z^{(5)}}f(z^{(5)})\cdot da^{(4)}\cdot W^{(5)})
+tr(D_{z^{(5)}}f(z^{(5)})\cdot a^{(4)}\cdot dW^{(5)})
+tr(D_{z^{(5)}}f(z^{(5)})\cdot db^{(5)}))</script><p>求权重矩阵$W^{(5)}$梯度</p><script type="math/tex;mode=display">
d(dataloss)=tr(D_{z^{(5)}}f(z^{(5)})\cdot a^{(4)}\cdot dW^{(5)})</script><script type="math/tex;mode=display">
\Rightarrow D_{W^{(5)}}f(W^{(5)})=D_{z^{(5)}}f(z^{(5)})\cdot a^{(4)}\\
\Rightarrow \bigtriangledown_{W^{(5)}}f(W^{(5)})=(a^{(4)})^{T}\cdot \bigtriangledown_{z^{(5)}}f(z^{(5)})</script><p>求偏置向量$b^{(5)}$梯度</p><script type="math/tex;mode=display">
d(dataloss)=tr(D_{z^{(5)}}f(z^{(5)})\cdot db^{(5)})</script><script type="math/tex;mode=display">
\Rightarrow D_{b^{(5)}}f(b^{(5)})=D_{z^{(5)}}f(z^{(5)})\\
\Rightarrow \bigtriangledown_{b^{(5)}}f(b^{(5)})=\bigtriangledown_{z^{(5)}}f(z^{(5)})</script><p>求上一层输出向量$a^{(4)}$梯度</p><script type="math/tex;mode=display">
d(dataloss)=tr(D_{z^{(5)}}f(z^{(5)})\cdot da^{(4)}\cdot W^{(5)})
=tr(W^{(5)}\cdot D_{z^{(5)}}f(z^{(5)})\cdot da^{(4)})</script><script type="math/tex;mode=display">
\Rightarrow D_{a^{(4)}}f(a^{(4)})=W^{(5)}\cdot D_{z^{(5)}}f(z^{(5)})\\
\Rightarrow \bigtriangledown_{a^{(4)}}f(a^{(4)})=\bigtriangledown_{z^{(5)}}f(z^{(5)})\cdot (W^{(5)})^{T}</script><p><strong>池化层$S4$</strong></p><p>将$a^{(4)}$梯度重置回$output^{(4)}$梯度，再重置为$y^{(4)}$梯度</p><p>因为卷积层$C5$滤波器的空间尺寸和$output^{(4)}$的空间尺寸一致，所以不需要将$a^{(4)}$先转换成$output^{(4)}$，再转换成$y^{(4)}$，可以一步到位</p><p>求输入向量$z^{(4)}$梯度</p><script type="math/tex;mode=display">
z^{(4)}\in R^{400\times 1},\ a^{(4)}\in R^{1\times 400}\\
\Rightarrow a^{(4)} = (z^{(4)})^{T}\\
\Rightarrow da^{(4)} = d(z^{(4)})^{T}</script><script type="math/tex;mode=display">
d(dataloss)=tr(D_{a^{(4)}}f(a^{(4)}) da^{(4)})=tr(D_{a^{(4)}}f(a^{(4)})\cdot d(z^{(4)})^{T})\\
=tr(D_{a^{(4)}}f(a^{(4)})^{T}\cdot dz^{(4)})</script><script type="math/tex;mode=display">
\Rightarrow D_{z^{(4)}}f(z^{(4)})=D_{a^{(4)}}f(a^{(4)})^{T}\\
\Rightarrow \bigtriangledown_{z^{(4)}}f(z^{(4)})=D_{a^{(4)}}f(a^{(4)})</script><p>上一层输出向量$a^{(3)}$梯度</p><script type="math/tex;mode=display">
z^{(4)}=\max (a^{(3)})\\
dz^{(4)}=1(a^{(3)}\ is\ the\ max)* da^{(3)}</script><p>配合$argz^{(4)}$，最大值梯度和$z^{(4)}$一致，其余梯度为$0$</p><script type="math/tex;mode=display">
d(dataloss)=tr(D_{z^{(4)}}f(z^{(4)}) dz^{(4)})\\
=tr(D_{z^{(4)}}f(z^{(4)})\cdot 1(a^{(3)}\ is\ the\ max)* da^{(3)})
=tr(D_{z^{(4)}}f(z^{(4)})* 1(a^{(3)}\ is\ the\ max)^{T}\cdot da^{(3)}</script><script type="math/tex;mode=display">
\Rightarrow D_{a^{(3)}}f(a^{(3)})=D_{z^{(4)}}f(z^{(4)})* 1(a^{(3)}\ is\ the\ max)^{T}\\
\Rightarrow \bigtriangledown_{a^{(3)}}f(a^{(3)})=\bigtriangledown_{z^{(4)}}f(z^{(4)})* 1(a^{(3)}\ is\ the\ max)</script><p><strong>卷积层$C3$</strong></p><p>将$a^{(3)}$梯度重置回$output^{(3)}$梯度，再重置为$y^{(3)}$梯度</p><p>求输入向量$z^{(3)}$梯度</p><script type="math/tex;mode=display">
y^{(3)} = relu(z^{(3)})\\
dy^{(3)} = 1(z^{(3)} \geq 0)*dz^{(3)}</script><script type="math/tex;mode=display">
d(dataloss)
=tr(D_{y^{(3)}}f(y^{(3)})\cdot dy^{(3)})\\
=tr(D_{y^{(3)}}f(y^{(3)})\cdot (1(z^{(3)} \geq 0)*dz^{(3)}))\\
=tr(D_{y^{(3)}}f(y^{(3)})* 1(z^{(3)} \geq 0)^{T}\cdot dz^{(3)})</script><script type="math/tex;mode=display">
\Rightarrow D_{z^{(3)}}f(z^{(3)})=D_{y^{(3)}}f(y^{(3)})* 1(z^{(3)} \geq 0)^{T}\\
\Rightarrow \bigtriangledown_{z^{(3)}}f(z^{(3)})=\bigtriangledown_{y^{(3)}}f(y^{(3)})* 1(z^{(3)} \geq 0)</script><p>其他梯度</p><script type="math/tex;mode=display">
z^{(3)}=a^{(2)}\cdot W^{(3)}+b^{(3)} \\
dz^{(3)}=da^{(2)}\cdot W^{(3)}+a^{(2)}\cdot dW^{(3)}+db^{(3)}\\
d(dataloss)=tr(D_{z^{(3)}}f(z^{(3)})\cdot dz^{(3)})\\
=tr(D_{z^{(3)}}f(z^{(3)})\cdot (da^{(2)}\cdot W^{(3)} + a^{(2)}\cdot dW^{(3)} + db^{(3)}))\\
=tr(D_{z^{(3)}}f(z^{(3)})\cdot da^{(2)}\cdot W^{(3)})
+tr(D_{z^{(3)}}f(z^{(3)})\cdot a^{(2)}\cdot dW^{(3)})
+tr(D_{z^{(3)}}f(z^{(3)})\cdot db^{(3)}))</script><p>求权重矩阵$W^{(3)}$梯度</p><script type="math/tex;mode=display">
d(dataloss)=tr(D_{z^{(3)}}f(z^{(3)})\cdot a^{(2)}\cdot dW^{(3)})</script><script type="math/tex;mode=display">
\Rightarrow D_{W^{(3)}}f(W^{(3)})=D_{z^{(3)}}f(z^{(3)})\cdot a^{(2)}\\
\Rightarrow \bigtriangledown_{W^{(3)}}f(W^{(3)})=(a^{(2)})^{T}\cdot \bigtriangledown_{z^{(3)}}f(z^{(3)})</script><p>求偏置向量$b^{(3)}$梯度</p><script type="math/tex;mode=display">
d(dataloss)=\frac {1}{N} \sum_{i=1}^{N} tr(D_{z^{(3)}}f(z^{(3)})\cdot db^{(3)})</script><script type="math/tex;mode=display">
\Rightarrow D_{b^{(3)}}f(b^{(3)})=\frac {1}{N} \sum_{i=1}^{N} D_{z^{(3)}}f(z^{(3)})\\
\Rightarrow \bigtriangledown_{b^{(3)}}f(b^{(3)})=\frac {1}{N} \sum_{i=1}^{N} \bigtriangledown_{z^{(3)}}f(z^{(3)})</script><p>$N$表示$dz^{(3)}$的行数</p><p>求上一层输出向量$a^{(2)}$梯度</p><script type="math/tex;mode=display">
d(dataloss)=tr(D_{z^{(3)}}f(z^{(3)})\cdot da^{(2)}\cdot W^{(3)})
=tr(W^{(3)}\cdot D_{z^{(3)}}f(z^{(3)})\cdot da^{(2)})</script><script type="math/tex;mode=display">
\Rightarrow D_{a^{(2)}}f(a^{(2)})=W^{(3)}\cdot D_{z^{(3)}}f(z^{(3)})\\
\Rightarrow \bigtriangledown_{a^{(2)}}f(a^{(2)})=\bigtriangledown_{z^{(3)}}f(z^{(3)})\cdot (W^{(3)})^{T}</script><p><strong>池化层$S2$</strong></p><p>$C3$输入层梯度大小为$100\times 150$，是在$S2$输出数据体$output^{(2)}$上采样获得，将$da^{(2)}$重采样回$output^{(2)}$梯度矩阵，再重置回$1176\times 1$大小，就是$z^{(2)}$的梯度</p><p>上一层输出向量$a^{(1)}$梯度</p><script type="math/tex;mode=display">
z^{(2)}=\max (a^{(1)})\\
dz^{(2)}=1(a^{(1)}\ is\ the\ max)* da^{(1)}</script><p>配合$argz^{(2)}$，最大值梯度和$z^{(2)}$一致，其余梯度为$0$</p><script type="math/tex;mode=display">
d(dataloss)=tr(D_{z^{(2)}}f(z^{(2)}) dz^{(2)})\\
=tr(D_{z^{(2)}}f(z^{(2)})\cdot 1(a^{(1)}\ is\ the\ max)* da^{(1)})
=tr(D_{z^{(2)}}f(z^{(2)})* 1(a^{(1)}\ is\ the\ max)^{T}\cdot da^{(1)}</script><script type="math/tex;mode=display">
\Rightarrow D_{a^{(1)}}f(a^{(1)})=D_{z^{(2)}}f(z^{(2)})* 1(a^{(1)}\ is\ the\ max)^{T}\\
\Rightarrow \bigtriangledown_{a^{(1)}}f(a^{(1)})=\bigtriangledown_{z^{(2)}}f(z^{(2)})* 1(a^{(1)}\ is\ the\ max)</script><p><strong>卷积层$C1$</strong></p><p>将$a^{(1)}$梯度重置回$output^{(1)}$梯度，再重置为$y^{(1)}$梯度</p><p>求输入向量$z^{(1)}$梯度</p><script type="math/tex;mode=display">
y^{(1)} = relu(z^{(1)})\\
dy^{(1)} = 1(z^{(1)} \geq 0)*dz^{(1)}</script><script type="math/tex;mode=display">
d(dataloss)
=tr(D_{y^{(1)}}f(y^{(1)})\cdot dy^{(1)})\\
=tr(D_{y^{(1)}}f(y^{(1)})\cdot (1(z^{(1)} \geq 0)*dz^{(1)}))\\
=tr(D_{y^{(1)}}f(y^{(1)})* 1(z^{(1)} \geq 0)^{T}\cdot dz^{(1)})</script><script type="math/tex;mode=display">
\Rightarrow D_{z^{(1)}}f(z^{(1)})=D_{y^{(1)}}f(y^{(1)})* 1(z^{(1)} \geq 0)^{T}\\
\Rightarrow \bigtriangledown_{z^{(1)}}f(z^{(1)})=\bigtriangledown_{y^{(1)}}f(y^{(1)})* 1(z^{(1)} \geq 0)</script><p>其他梯度</p><script type="math/tex;mode=display">
z^{(1)}=a^{(0)}\cdot W^{(1)}+b^{(1)} \\
dz^{(1)}=da^{(0)}\cdot W^{(1)}+a^{(0)}\cdot dW^{(1)}+db^{(1)}\\
d(dataloss)=tr(D_{z^{(1)}}f(z^{(1)})\cdot dz^{(1)})\\
=tr(D_{z^{(1)}}f(z^{(1)})\cdot (da^{(0)}\cdot W^{(1)} + a^{(0)}\cdot dW^{(1)} + db^{(1)}))\\
=tr(D_{z^{(1)}}f(z^{(1)})\cdot da^{(0)}\cdot W^{(1)})
+tr(D_{z^{(1)}}f(z^{(1)})\cdot a^{(0)}\cdot dW^{(1)})
+tr(D_{z^{(1)}}f(z^{(1)})\cdot db^{(1)}))</script><p>求权重矩阵$W^{(1)}$梯度</p><script type="math/tex;mode=display">
d(dataloss)=tr(D_{z^{(1)}}f(z^{(1)})\cdot a^{(0)}\cdot dW^{(1)})</script><script type="math/tex;mode=display">
\Rightarrow D_{W^{(1)}}f(W^{(1)})=D_{z^{(1)}}f(z^{(1)})\cdot a^{(0)}\\
\Rightarrow \bigtriangledown_{W^{(1)}}f(W^{(1)})=(a^{(0)})^{T}\cdot \bigtriangledown_{z^{(1)}}f(z^{(1)})</script><p>求偏置向量$b^{(1)}$梯度</p><script type="math/tex;mode=display">
d(dataloss)=\frac {1}{N} \sum_{i=1}^{N} tr(D_{z^{(1)}}f(z^{(1)})\cdot db^{(1)})</script><script type="math/tex;mode=display">
\Rightarrow D_{b^{(1)}}f(b^{(1)})=\frac {1}{N} \sum_{i=1}^{N} D_{z^{(1)}}f(z^{(1)})\\
\Rightarrow \bigtriangledown_{b^{(1)}}f(b^{(1)})=\frac {1}{N} \sum_{i=1}^{N} \bigtriangledown_{z^{(1)}}f(z^{(1)})</script><p>$N$表示$dz^{(1)}$的行数</p></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.jpg" alt="zhujian 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="zhujian 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zhujian</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zhujian.tech/posts/3accb62a.html" title="卷积神经网络推导-单张图片矩阵计算">https://www.zhujian.tech/posts/3accb62a.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/calculus/" rel="tag"># 微积分</a> <a href="/tags/linear-albegra/" rel="tag"># 线性代数</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/3b660279.html" rel="next" title="卷积神经网络概述"><i class="fa fa-chevron-left"></i> 卷积神经网络概述</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/posts/ab1e719c.html" rel="prev" title="卷积神经网络推导-批量图片矩阵计算">卷积神经网络推导-批量图片矩阵计算<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#计算符号"><span class="nav-number">1.</span> <span class="nav-text">计算符号</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LeNet-5简介"><span class="nav-number">2.</span> <span class="nav-text">LeNet-5简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#卷积层转全连接层"><span class="nav-number">3.</span> <span class="nav-text">卷积层转全连接层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#池化层转全连接层"><span class="nav-number">4.</span> <span class="nav-text">池化层转全连接层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#矩阵计算"><span class="nav-number">5.</span> <span class="nav-text">矩阵计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#前向传播"><span class="nav-number">5.1.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播"><span class="nav-number">5.2.</span> <span class="nav-text">反向传播</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zhujian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">zhujian</p><div class="site-description" itemprop="description">one bite at a time</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">169</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">129</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/./atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zjZSTU" title="Github &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;zjZSTU" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/u012005313" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;u012005313" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i> CSDN</a></span><span class="links-of-author-item"><a href="/mailto:zjzstu@gmail.com" title="Gmail &amp;rarr; mailto:zjzstu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> Gmail</a></span><span class="links-of-author-item"><a href="/mailto:505169307@gmail.com" title="QQmail &amp;rarr; mailto:505169307@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> QQmail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://cloud.tencent.com/" title="https:&#x2F;&#x2F;cloud.tencent.com" rel="noopener" target="_blank">腾讯云</a></li><li class="links-of-blogroll-item"> <a href="https://www.aliyun.com/" title="https:&#x2F;&#x2F;www.aliyun.com" rel="noopener" target="_blank">阿里云</a></li><li class="links-of-blogroll-item"> <a href="https://developer.android.com/" title="https:&#x2F;&#x2F;developer.android.com&#x2F;" rel="noopener" target="_blank">Android Dev</a></li><li class="links-of-blogroll-item"> <a href="https://jenkins.io/" title="https:&#x2F;&#x2F;jenkins.io&#x2F;" rel="noopener" target="_blank">Jenkins</a></li><li class="links-of-blogroll-item"> <a href="http://cs231n.github.io/" title="http:&#x2F;&#x2F;cs231n.github.io&#x2F;" rel="noopener" target="_blank">cs231n</a></li><li class="links-of-blogroll-item"> <a href="https://pytorch.org/docs/stable/index.html" title="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;index.html" rel="noopener" target="_blank">pytorch</a></li><li class="links-of-blogroll-item"> <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" title="https:&#x2F;&#x2F;docs.docker.com&#x2F;install&#x2F;linux&#x2F;docker-ce&#x2F;ubuntu&#x2F;" rel="noopener" target="_blank">docker</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">zhujian</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">948k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">26:20</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-bar-chart-o"></i></span> <a href="https://tongji.baidu.com/web/27249108/overview/index?siteId=13647183" rel="noopener" target="_blank">百度统计</a></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div><div class="beian"> <a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备 19026415号</a> <span class="post-meta-divider">|</span> <img src="/images/beian_icon.png" style="display:inline-block;text-decoration:none;height:13px"> <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001748" rel="noopener" target="_blank">浙公网安备 33011802001748号</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span> <span class="site-uv" title="总访客量">访客数：<span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="site-pv" title="总访问量">阅读量：<span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="17,63,61" opacity="1" zindex="-1" count="199" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '8f22595c6dd29c94467d',
      clientSecret: 'e66bd90ebb9aa66c562c753dec6c90ceecbd51f2',
      repo: 'guestbook',
      owner: 'zjZSTU',
      admin: ['zjZSTU'],
      id: '77d194ad8de09306eacbbbc59bd256ea',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script></body></html>