<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/./atom.xml" title="做一个幸福的人" type="application/atom+xml"><meta name="google-site-verification" content="Qr_3yqLtyErDFKHW7mE8PJz4qDUX-bf_fMLpSRckQe4"><meta name="baidu-site-verification" content="zOwIvKMV7f"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"搜索文章",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet"><meta name="description" content="参考：神经网络推导-矩阵计算神经网络实现-numpy参考线性SVM分类器实现神经网络分类器cs231n assignment2中实现了自定义层数和数量的神经网络模型，参考其实现完成单个类的神经网络分类器"><meta name="keywords" content="python,numpy,神经网络"><meta property="og:type" content="article"><meta property="og:title" content="神经网络分类器"><meta property="og:url" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;posts&#x2F;81a57a7.html"><meta property="og:site_name" content="做一个幸福的人"><meta property="og:description" content="参考：神经网络推导-矩阵计算神经网络实现-numpy参考线性SVM分类器实现神经网络分类器cs231n assignment2中实现了自定义层数和数量的神经网络模型，参考其实现完成单个类的神经网络分类器"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2020-02-15T05:36:35.875Z"><meta name="twitter:card" content="summary"><link rel="canonical" href="https://www.zhujian.tech/posts/81a57a7.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>神经网络分类器 | 做一个幸福的人</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e677aac1ac69b8826b9cfecb4e72e107";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">做一个幸福的人</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">面朝大海，春暖花开</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">129</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">48</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">169</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header> <a href="https://github.com/zjZSTU" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zhujian.tech/posts/81a57a7.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="zhujian"><meta itemprop="description" content="one bite at a time"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="做一个幸福的人"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 神经网络分类器</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-07-17 20:25:05" itemprop="dateCreated datePublished" datetime="2019-07-17T20:25:05+00:00">2019-07-17</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-15 05:36:35" itemprop="dateModified" datetime="2020-02-15T05:36:35+00:00">2020-02-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/" itemprop="url" rel="index"><span itemprop="name">算法</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">编程</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/deep-learning/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/programming-language/" itemprop="url" rel="index"><span itemprop="name">编程语言</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/data-learning/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/codebase/" itemprop="url" rel="index"><span itemprop="name">代码库</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/classifier/" itemprop="url" rel="index"><span itemprop="name">分类器</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>9.8k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>16 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>参考：</p><p><a href="https://www.zhujian.tech/posts/1dd3ebad.html">神经网络推导-矩阵计算</a></p><p><a href="https://www.zhujian.tech/posts/ba2ca878.html">神经网络实现-numpy</a></p><p>参考<a href="https://www.zhujian.tech/posts/ebe205e.html">线性SVM分类器</a>实现神经网络分类器</p><p><code>cs231n assignment2</code>中实现了自定义层数和数量的神经网络模型，参考其实现完成单个类的神经网络分类器</p><a id="more"></a><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># @Time    : 19-7-18 上午10:15</span><br><span class="line"># @Author  : zj</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class NN(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, hidden_dims, input_dim=32 * 32 * 3, num_classes=10, weight_scale=1e-2, learning_rate=1e-3,</span><br><span class="line">                 reg=0.0, dtype=np.float64):</span><br><span class="line">        self.reg = reg</span><br><span class="line">        self.lr = learning_rate</span><br><span class="line">        self.dtype = dtype</span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.num_layers = len(hidden_dims) + 1</span><br><span class="line"></span><br><span class="line">        if hidden_dims is None:</span><br><span class="line">            self.params[&apos;W1&apos;] = weight_scale * np.random.randn(input_dim, num_classes)</span><br><span class="line">            self.params[&apos;b1&apos;] = np.zeros((1, num_classes))</span><br><span class="line">        else:</span><br><span class="line">            for i in range(self.num_layers):</span><br><span class="line">                if i == 0:</span><br><span class="line">                    in_dim = input_dim</span><br><span class="line">                    out_dim = hidden_dims[i]</span><br><span class="line">                elif i == (self.num_layers - 1):</span><br><span class="line">                    in_dim = hidden_dims[i - 1]</span><br><span class="line">                    out_dim = num_classes</span><br><span class="line">                else:</span><br><span class="line">                    in_dim = hidden_dims[i - 1]</span><br><span class="line">                    out_dim = hidden_dims[i]</span><br><span class="line"></span><br><span class="line">                self.params[&apos;W%d&apos; % (i + 1)] = weight_scale * np.random.randn(in_dim, out_dim)</span><br><span class="line">                self.params[&apos;b%d&apos; % (i + 1)] = np.zeros((1, out_dim))</span><br><span class="line"></span><br><span class="line">        self.configs = &#123;&#125;</span><br><span class="line">        config = &#123;&apos;learning_rate&apos;: learning_rate&#125;</span><br><span class="line">        for k in self.params.keys():</span><br><span class="line">            self.configs[k] = config.copy()</span><br><span class="line"></span><br><span class="line">        # Cast all parameters to the correct datatype</span><br><span class="line">        for k, v in self.params.items():</span><br><span class="line">            self.params[k] = v.astype(dtype)</span><br><span class="line"></span><br><span class="line">    def train(self, X, y, num_iters=100, batch_size=200, verbose=False):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Inputs:</span><br><span class="line">        - X: A numpy array of shape (N, D) containing training data; there are N</span><br><span class="line">          training samples each of dimension D.</span><br><span class="line">        - y: A numpy array of shape (N,) containing training labels; y[i] = c</span><br><span class="line">          means that X[i] has label 0 &lt;= c &lt; C for C classes.</span><br><span class="line">        - num_iters: (integer) number of steps to take when optimizing</span><br><span class="line">        - batch_size: (integer) number of training examples to use at each step.</span><br><span class="line">        - verbose: (boolean) If true, print progress during optimization.</span><br><span class="line"></span><br><span class="line">        Outputs:</span><br><span class="line">        A list containing the value of the loss function at each training iteration.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        X = X.astype(self.dtype)</span><br><span class="line">        num_train, dim = X.shape</span><br><span class="line"></span><br><span class="line">        # Run stochastic gradient descent to optimize W</span><br><span class="line">        loss_history = []</span><br><span class="line">        range_list = np.arange(0, num_train, step=batch_size)</span><br><span class="line">        for it in range(num_iters):</span><br><span class="line">            total_loss = 0</span><br><span class="line">            for i in range_list:</span><br><span class="line">                X_batch = X[i:i + batch_size]</span><br><span class="line">                y_batch = y[i:i + batch_size]</span><br><span class="line"></span><br><span class="line">                # evaluate loss and gradient</span><br><span class="line">                loss, grads = self.loss(X_batch, y_batch)</span><br><span class="line">                total_loss += loss</span><br><span class="line"></span><br><span class="line">                for k in self.params.keys():</span><br><span class="line">                    #     config = self.configs[k]</span><br><span class="line">                    w = self.params[k]</span><br><span class="line">                    dw = grads[k]</span><br><span class="line"></span><br><span class="line">                    # next_w, next_config = self.adam(w, dw, config)</span><br><span class="line">                    next_w = w - self.lr * dw</span><br><span class="line"></span><br><span class="line">                    self.params[k] = next_w</span><br><span class="line">                    # self.configs[k] = config</span><br><span class="line"></span><br><span class="line">            avg_loss = total_loss / len(range_list)</span><br><span class="line">            loss_history.append(avg_loss)</span><br><span class="line"></span><br><span class="line">            if verbose and it % 100 == 0:</span><br><span class="line">                print(&apos;iteration %d / %d: loss %f&apos; % (it, num_iters, avg_loss))</span><br><span class="line"></span><br><span class="line">        return loss_history</span><br><span class="line"></span><br><span class="line">    def predict(self, X):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Use the trained weights of this linear classifier to predict labels for</span><br><span class="line">        data points.</span><br><span class="line"></span><br><span class="line">        Inputs:</span><br><span class="line">        - X: A numpy array of shape (N, D) containing training data; there are N</span><br><span class="line">          training samples each of dimension D.</span><br><span class="line"></span><br><span class="line">        Returns:</span><br><span class="line">        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional</span><br><span class="line">          array of length N, and each element is an integer giving the predicted</span><br><span class="line">          class.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        scores, caches = self.forward(X)</span><br><span class="line">        scores -= np.atleast_2d(np.max(scores, axis=1)).T</span><br><span class="line">        exp_scores = np.exp(scores)</span><br><span class="line">        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)</span><br><span class="line"></span><br><span class="line">        y_pred = np.argmax(probs, axis=1)</span><br><span class="line">        return y_pred</span><br><span class="line"></span><br><span class="line">    def loss(self, X_batch, y_batch):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Compute the loss function and its derivative.</span><br><span class="line">        Subclasses will override this.</span><br><span class="line"></span><br><span class="line">        Inputs:</span><br><span class="line">        - X_batch: A numpy array of shape (N, D) containing a minibatch of N</span><br><span class="line">          data points; each point has dimension D.</span><br><span class="line">        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.</span><br><span class="line"></span><br><span class="line">        Returns: A tuple containing:</span><br><span class="line">        - loss as a single float</span><br><span class="line">        - gradient with respect to self.W; an array of the same shape as W</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        scores, caches = self.forward(X_batch)</span><br><span class="line">        data_loss, dout = self.softmax_loss(scores, y_batch)</span><br><span class="line"></span><br><span class="line">        reg_loss = 0</span><br><span class="line">        for i in range(self.num_layers):</span><br><span class="line">            reg_loss += 0.5 * self.reg * np.sum(self.params[&apos;W%d&apos; % (i + 1)] ** 2)</span><br><span class="line">        loss = data_loss + reg_loss</span><br><span class="line"></span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">        dx = None</span><br><span class="line">        for i in reversed(range(self.num_layers)):</span><br><span class="line">            cache = caches[&apos;cache%d&apos; % (i + 1)]</span><br><span class="line">            if i == (self.num_layers - 1):</span><br><span class="line">                dx, dw, db = self.affine_backward(dout, cache)</span><br><span class="line">            else:</span><br><span class="line">                dx, dw, db = self.affine_relu_backward(dx, cache)</span><br><span class="line">            grads[&apos;W%d&apos; % (i + 1)] = dw + self.reg * self.params[&apos;W%d&apos; % (i + 1)]</span><br><span class="line">            grads[&apos;b%d&apos; % (i + 1)] = db</span><br><span class="line"></span><br><span class="line">        return loss, grads</span><br><span class="line"></span><br><span class="line">    def forward(self, X):</span><br><span class="line">        a = None</span><br><span class="line">        z = None</span><br><span class="line">        caches = &#123;&#125;</span><br><span class="line">        for i in range(self.num_layers):</span><br><span class="line">            if i == 0:</span><br><span class="line">                a = X</span><br><span class="line">            if i == (self.num_layers - 1):</span><br><span class="line">                z, caches[&apos;cache%d&apos; % self.num_layers] = self.affine_forward(a,</span><br><span class="line">                                                                             self.params[&apos;W%d&apos; % (self.num_layers)],</span><br><span class="line">                                                                             self.params[&apos;b%d&apos; % (self.num_layers)])</span><br><span class="line">            else:</span><br><span class="line">                a, caches[&apos;cache%d&apos; % (i + 1)] = self.affine_relu_forward(a,</span><br><span class="line">                                                                          self.params[&apos;W%d&apos; % (i + 1)],</span><br><span class="line">                                                                          self.params[&apos;b%d&apos; % (i + 1)])</span><br><span class="line"></span><br><span class="line">        scores = z</span><br><span class="line">        return scores, caches</span><br><span class="line"></span><br><span class="line">    def affine_relu_forward(self, x, w, b):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Convenience layer that perorms an affine transform followed by a ReLU</span><br><span class="line"></span><br><span class="line">        Inputs:</span><br><span class="line">        - x: Input to the affine layer</span><br><span class="line">        - w, b: Weights for the affine layer</span><br><span class="line"></span><br><span class="line">        Returns a tuple of:</span><br><span class="line">        - out: Output from the ReLU</span><br><span class="line">        - cache: Object to give to the backward pass</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        a, fc_cache = self.affine_forward(x, w, b)</span><br><span class="line">        out, relu_cache = self.relu_forward(a)</span><br><span class="line">        cache = (fc_cache, relu_cache)</span><br><span class="line">        return out, cache</span><br><span class="line"></span><br><span class="line">    def affine_relu_backward(self, dout, cache):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Backward pass for the affine-relu convenience layer</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        fc_cache, relu_cache = cache</span><br><span class="line">        da = self.relu_backward(dout, relu_cache)</span><br><span class="line">        dx, dw, db = self.affine_backward(da, fc_cache)</span><br><span class="line">        return dx, dw, db</span><br><span class="line"></span><br><span class="line">    def affine_forward(self, x, w, b):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Computes the forward pass for an affine (fully-connected) layer.</span><br><span class="line"></span><br><span class="line">        The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N</span><br><span class="line">        examples, where each example x[i] has shape (d_1, ..., d_k). We will</span><br><span class="line">        reshape each input into a vector of dimension D = d_1 * ... * d_k, and</span><br><span class="line">        then transform it to an output vector of dimension M.</span><br><span class="line"></span><br><span class="line">        Inputs:</span><br><span class="line">        - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)</span><br><span class="line">        - w: A numpy array of weights, of shape (D, M)</span><br><span class="line">        - b: A numpy array of biases, of shape (M,)</span><br><span class="line"></span><br><span class="line">        Returns a tuple of:</span><br><span class="line">        - out: output, of shape (N, M)</span><br><span class="line">        - cache: (x, w, b)</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        inputs = x.reshape(x.shape[0], -1)</span><br><span class="line">        out = inputs.dot(w) + b.reshape(1, -1)</span><br><span class="line"></span><br><span class="line">        cache = (x, w, b)</span><br><span class="line">        return out, cache</span><br><span class="line"></span><br><span class="line">    def affine_backward(self, dout, cache):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Computes the backward pass for an affine layer.</span><br><span class="line"></span><br><span class="line">        Inputs:</span><br><span class="line">        - dout: Upstream derivative, of shape (N, M)</span><br><span class="line">        - cache: Tuple of:</span><br><span class="line">          - x: Input data, of shape (N, d_1, ... d_k)</span><br><span class="line">          - w: Weights, of shape (D, M)</span><br><span class="line">          - b: Biases, of shape (M,)</span><br><span class="line"></span><br><span class="line">        Returns a tuple of:</span><br><span class="line">        - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)</span><br><span class="line">        - dw: Gradient with respect to w, of shape (D, M)</span><br><span class="line">        - db: Gradient with respect to b, of shape (M,)</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        x, w, b = cache</span><br><span class="line"></span><br><span class="line">        dx = dout.dot(w.T).reshape(x.shape)</span><br><span class="line">        dw = x.reshape(x.shape[0], -1).T.dot(dout)</span><br><span class="line">        db = np.sum(dout, axis=0)</span><br><span class="line"></span><br><span class="line">        return dx, dw, db</span><br><span class="line"></span><br><span class="line">    def relu_forward(self, x):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Computes the forward pass for a layer of rectified linear units (ReLUs).</span><br><span class="line"></span><br><span class="line">        Input:</span><br><span class="line">        - x: Inputs, of any shape</span><br><span class="line"></span><br><span class="line">        Returns a tuple of:</span><br><span class="line">        - out: Output, of the same shape as x</span><br><span class="line">        - cache: x</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        out = x.copy()</span><br><span class="line">        out[x &lt; 0] = 0</span><br><span class="line"></span><br><span class="line">        cache = x</span><br><span class="line">        return out, cache</span><br><span class="line"></span><br><span class="line">    def relu_backward(self, dout, cache):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Computes the backward pass for a layer of rectified linear units (ReLUs).</span><br><span class="line"></span><br><span class="line">        Input:</span><br><span class="line">        - dout: Upstream derivatives, of any shape</span><br><span class="line">        - cache: Input x, of same shape as dout</span><br><span class="line"></span><br><span class="line">        Returns:</span><br><span class="line">        - dx: Gradient with respect to x</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        dx, x = None, cache</span><br><span class="line"></span><br><span class="line">        dx = dout</span><br><span class="line">        dx[x &lt; 0] = 0</span><br><span class="line"></span><br><span class="line">        return dx</span><br><span class="line"></span><br><span class="line">    def softmax_loss(self, scores, y):</span><br><span class="line">        num = y.shape[0]</span><br><span class="line"></span><br><span class="line">        exp_scores = np.exp(scores)</span><br><span class="line">        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)</span><br><span class="line"></span><br><span class="line">        data_loss = -1.0 / num * np.sum(np.log(probs[range(num), y]))</span><br><span class="line"></span><br><span class="line">        dscores = scores</span><br><span class="line">        dscores[range(num), y] -= 1</span><br><span class="line">        dscores /= num</span><br><span class="line"></span><br><span class="line">        return data_loss, dscores</span><br><span class="line"></span><br><span class="line">    def adam(self, w, dw, config=None):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        Uses the Adam update rule, which incorporates moving averages of both the</span><br><span class="line">        gradient and its square and a bias correction term.</span><br><span class="line"></span><br><span class="line">        config format:</span><br><span class="line">        - learning_rate: Scalar learning rate.</span><br><span class="line">        - beta1: Decay rate for moving average of first moment of gradient.</span><br><span class="line">        - beta2: Decay rate for moving average of second moment of gradient.</span><br><span class="line">        - epsilon: Small scalar used for smoothing to avoid dividing by zero.</span><br><span class="line">        - m: Moving average of gradient.</span><br><span class="line">        - v: Moving average of squared gradient.</span><br><span class="line">        - t: Iteration number.</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        if config is None: config = &#123;&#125;</span><br><span class="line">        config.setdefault(&apos;learning_rate&apos;, 1e-3)</span><br><span class="line">        config.setdefault(&apos;beta1&apos;, 0.9)</span><br><span class="line">        config.setdefault(&apos;beta2&apos;, 0.999)</span><br><span class="line">        config.setdefault(&apos;epsilon&apos;, 1e-8)</span><br><span class="line">        config.setdefault(&apos;m&apos;, np.zeros_like(w))</span><br><span class="line">        config.setdefault(&apos;v&apos;, np.zeros_like(w))</span><br><span class="line">        config.setdefault(&apos;t&apos;, 0)</span><br><span class="line"></span><br><span class="line">        t = config[&apos;t&apos;] + 1</span><br><span class="line">        m = config[&apos;beta1&apos;] * config[&apos;m&apos;] + (1 - config[&apos;beta1&apos;]) * dw</span><br><span class="line">        mt = m / (1 - config[&apos;beta1&apos;] ** t)</span><br><span class="line">        v = config[&apos;beta2&apos;] * config[&apos;v&apos;] + (1 - config[&apos;beta2&apos;]) * (dw ** 2)</span><br><span class="line">        vt = v / (1 - config[&apos;beta2&apos;] ** t)</span><br><span class="line"></span><br><span class="line">        next_w = w - config[&apos;learning_rate&apos;] * mt / (np.sqrt(vt) + config[&apos;epsilon&apos;])</span><br><span class="line"></span><br><span class="line">        config[&apos;t&apos;] = t</span><br><span class="line">        config[&apos;m&apos;] = m</span><br><span class="line">        config[&apos;v&apos;] = v</span><br><span class="line"></span><br><span class="line">        return next_w, config</span><br></pre></td></tr></table></figure><h2 id="数值稳定性"><a href="#数值稳定性" class="headerlink" title="数值稳定性"></a>数值稳定性</h2><p>预测计算结果时，有可能出现数值溢出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scores, caches = self.forward(X)</span><br><span class="line">exp_scores = np.exp(scores)</span><br><span class="line">probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)</span><br></pre></td></tr></table></figure><p>出错信息如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">RuntimeWarning: overflow encountered in exp</span><br><span class="line">exp_scores = np.exp(scores)</span><br><span class="line"></span><br><span class="line">RuntimeWarning: invalid value encountered in true_divide</span><br><span class="line">probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)</span><br></pre></td></tr></table></figure><p>参考<a href="https://zhujian.tech/posts/2626bec3.html" target="_blank" rel="noopener">指数计算 - 数值稳定性考虑</a>，修改代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scores, caches = self.forward(X)</span><br><span class="line">scores -= np.atleast_2d(np.max(scores, axis=1)).T</span><br><span class="line">exp_scores = np.exp(scores)</span><br><span class="line">probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>整体结构仍旧遵循<code>创建-&gt;训练-&gt;预测</code>模型</p><p>实现如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">classifier = Classifier([120, 60], input_dim=dim, num_classes=out_dim, learning_rate=lr, reg=reg)</span><br><span class="line"></span><br><span class="line">classifier.train(x_train, y_train, num_iters=10000, batch_size=100, verbose=True)</span><br><span class="line">y_train_pred = classifier.predict(x_train)</span><br><span class="line">y_val_pred = classifier.predict(x_val)</span><br><span class="line"></span><br><span class="line">train_acc = np.mean(y_train_pred == y_train)</span><br><span class="line">val_acc = np.mean(y_val_pred == y_val</span><br></pre></td></tr></table></figure><p>完整训练和实现代码参考：<a href="https://github.com/zjZSTU/cs231n/tree/master/coding" target="_blank" rel="noopener">cs231n/coding/</a></p></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.jpg" alt="zhujian 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="zhujian 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zhujian</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zhujian.tech/posts/81a57a7.html" title="神经网络分类器">https://www.zhujian.tech/posts/81a57a7.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/python/" rel="tag"># python</a> <a href="/tags/numpy/" rel="tag"># numpy</a> <a href="/tags/nerual-network/" rel="tag"># 神经网络</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/e043b7fb.html" rel="next" title="softmax分类器"><i class="fa fa-chevron-left"></i> softmax分类器</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/posts/c37e79f3.html" rel="prev" title="决策边界">决策边界<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#实现"><span class="nav-number">1.</span> <span class="nav-text">实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#数值稳定性"><span class="nav-number">2.</span> <span class="nav-text">数值稳定性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#小结"><span class="nav-number">3.</span> <span class="nav-text">小结</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zhujian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">zhujian</p><div class="site-description" itemprop="description">one bite at a time</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">169</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">129</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/./atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zjZSTU" title="Github &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;zjZSTU" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/u012005313" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;u012005313" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i> CSDN</a></span><span class="links-of-author-item"><a href="/mailto:zjzstu@gmail.com" title="Gmail &amp;rarr; mailto:zjzstu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> Gmail</a></span><span class="links-of-author-item"><a href="/mailto:505169307@gmail.com" title="QQmail &amp;rarr; mailto:505169307@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> QQmail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://cloud.tencent.com/" title="https:&#x2F;&#x2F;cloud.tencent.com" rel="noopener" target="_blank">腾讯云</a></li><li class="links-of-blogroll-item"> <a href="https://www.aliyun.com/" title="https:&#x2F;&#x2F;www.aliyun.com" rel="noopener" target="_blank">阿里云</a></li><li class="links-of-blogroll-item"> <a href="https://developer.android.com/" title="https:&#x2F;&#x2F;developer.android.com&#x2F;" rel="noopener" target="_blank">Android Dev</a></li><li class="links-of-blogroll-item"> <a href="https://jenkins.io/" title="https:&#x2F;&#x2F;jenkins.io&#x2F;" rel="noopener" target="_blank">Jenkins</a></li><li class="links-of-blogroll-item"> <a href="http://cs231n.github.io/" title="http:&#x2F;&#x2F;cs231n.github.io&#x2F;" rel="noopener" target="_blank">cs231n</a></li><li class="links-of-blogroll-item"> <a href="https://pytorch.org/docs/stable/index.html" title="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;index.html" rel="noopener" target="_blank">pytorch</a></li><li class="links-of-blogroll-item"> <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" title="https:&#x2F;&#x2F;docs.docker.com&#x2F;install&#x2F;linux&#x2F;docker-ce&#x2F;ubuntu&#x2F;" rel="noopener" target="_blank">docker</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">zhujian</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">948k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">26:20</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-bar-chart-o"></i></span> <a href="https://tongji.baidu.com/web/27249108/overview/index?siteId=13647183" rel="noopener" target="_blank">百度统计</a></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div><div class="beian"> <a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备 19026415号</a> <span class="post-meta-divider">|</span> <img src="/images/beian_icon.png" style="display:inline-block;text-decoration:none;height:13px"> <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001748" rel="noopener" target="_blank">浙公网安备 33011802001748号</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span> <span class="site-uv" title="总访客量">访客数：<span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="site-pv" title="总访问量">阅读量：<span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="17,63,61" opacity="1" zindex="-1" count="199" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '8f22595c6dd29c94467d',
      clientSecret: 'e66bd90ebb9aa66c562c753dec6c90ceecbd51f2',
      repo: 'guestbook',
      owner: 'zjZSTU',
      admin: ['zjZSTU'],
      id: '10482a4e2cc0d33ac64e4e2ef2368c8b',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script></body></html>