<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/./atom.xml" title="做一个幸福的人" type="application/atom+xml"><meta name="google-site-verification" content="Qr_3yqLtyErDFKHW7mE8PJz4qDUX-bf_fMLpSRckQe4"><meta name="baidu-site-verification" content="zOwIvKMV7f"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"搜索文章",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet"><meta name="description" content="In this tutorial, you will learn how to train a convolutional neural network for image classification using transfer learning. You can read more about the transfer learning at cs231n notes"><meta name="keywords" content="python,pytorch,torchvision"><meta property="og:type" content="article"><meta property="og:title" content="[译]Transfer Learning for Computer Vision Tutorial"><meta property="og:url" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;posts&#x2F;c8566254.html"><meta property="og:site_name" content="做一个幸福的人"><meta property="og:description" content="In this tutorial, you will learn how to train a convolutional neural network for image classification using transfer learning. You can read more about the transfer learning at cs231n notes"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;译-transfer-learning&#x2F;sphx_glr_transfer_learning_tutorial_001.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;译-transfer-learning&#x2F;sphx_glr_transfer_learning_tutorial_002.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;译-transfer-learning&#x2F;sphx_glr_transfer_learning_tutorial_003.png"><meta property="og:updated_time" content="2020-02-27T14:08:07.885Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;译-transfer-learning&#x2F;sphx_glr_transfer_learning_tutorial_001.png"><link rel="canonical" href="https://www.zhujian.tech/posts/c8566254.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>[译]Transfer Learning for Computer Vision Tutorial | 做一个幸福的人</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e677aac1ac69b8826b9cfecb4e72e107";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">做一个幸福的人</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">面朝大海，春暖花开</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">129</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">48</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">169</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header> <a href="https://github.com/zjZSTU" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zhujian.tech/posts/c8566254.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="zhujian"><meta itemprop="description" content="one bite at a time"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="做一个幸福的人"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> [译]Transfer Learning for Computer Vision Tutorial</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-02-26 19:53:35" itemprop="dateCreated datePublished" datetime="2020-02-26T19:53:35+00:00">2020-02-26</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-27 14:08:07" itemprop="dateModified" datetime="2020-02-27T14:08:07+00:00">2020-02-27</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">编程</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/programming-language/" itemprop="url" rel="index"><span itemprop="name">编程语言</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/data/" itemprop="url" rel="index"><span itemprop="name">数据</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/codebase/" itemprop="url" rel="index"><span itemprop="name">代码库</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/data/dataset/" itemprop="url" rel="index"><span itemprop="name">数据集</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/translation/" itemprop="url" rel="index"><span itemprop="name">翻译</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>10k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>17 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><blockquote><p>In this tutorial, you will learn how to train a convolutional neural network for image classification using transfer learning. You can read more about the transfer learning at cs231n notes</p></blockquote><a id="more"></a><p>在本教程中，您将学习如何使用迁移学习来训练用于图像分类的卷积神经网络。您可以在<a href="https://cs231n.github.io/transfer-learning/" target="_blank" rel="noopener">cs231n notes</a>上阅读更多关于迁移学习的信息</p><blockquote><p>Quoting these notes,</p><blockquote><p>In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest.</p></blockquote></blockquote><p>引用</p><p>实际上，很少有人从头开始训练整个卷积网络(随机初始化)，因为拥有足够大的数据集是相对罕见的。相反，通常在非常大的数据集(例如，包含120万张1000个类别的图像的ImageNet)上预处理ConvNet，然后将ConvNet用作感兴趣任务的初始化或固定特征提取器</p><blockquote><p>These two major transfer learning scenarios look as follows:</p><ul><li>Finetuning the convnet: Instead of random initializaion, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. Rest of the training looks as usual.</li><li>ConvNet as fixed feature extractor: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained.</li></ul></blockquote><p>这两个主要的迁移学习场景如下:</p><ul><li>微调网络：不使用随机初始化而是用一个预训练网络来初始化网络，就像在imagenet 1000数据集上训练的网络一样。剩下的训练看起来和往常一样</li><li>卷积网络作为固定的特征提取器：除了最后的全连接层外，将会冻结所有网络的权重。最后的全连接层将会被一个新的随机初始化的全连接层替代，并且仅训练该层</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># License: BSD</span><br><span class="line"># Author: Sasank Chilamkurthy</span><br><span class="line"></span><br><span class="line">from __future__ import print_function, division</span><br><span class="line"></span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.optim as optim</span><br><span class="line">from torch.optim import lr_scheduler</span><br><span class="line">import numpy as np</span><br><span class="line">import torchvision</span><br><span class="line">from torchvision import datasets, models, transforms</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import time</span><br><span class="line">import os</span><br><span class="line">import copy</span><br><span class="line"></span><br><span class="line">plt.ion()   # interactive mode</span><br></pre></td></tr></table></figure><h2 id="Load-Data"><a href="#Load-Data" class="headerlink" title="Load Data"></a>Load Data</h2><p>加载数据</p><blockquote><p>We will use torchvision and torch.utils.data packages for loading the data.</p></blockquote><p>我们使用torchvision和torch.util.data包来加载数据</p><blockquote><p>The problem we’re going to solve today is to train a model to classify ants and bees. We have about 120 training images each for ants and bees. There are 75 validation images for each class. Usually, this is a very small dataset to generalize upon, if trained from scratch. Since we are using transfer learning, we should be able to generalize reasonably well.</p></blockquote><p>要解决的问题是训练一个分类蚂蚁和蜜蜂的模型，分别有大约120张蚂蚁和蜜蜂的训练图片，并且每个类还有75张验证图像。通常来说这是一个非常小的数据集，如果从零开始训练的话会导致无法收敛。而使用迁移学习，应该能够得到相当好的泛化结果</p><blockquote><p>This dataset is a very small subset of imagenet.</p></blockquote><p>本数据集是imagenet的一个非常小的子集</p><blockquote><p>Note: Download the data from here and extract it to the current directory.</p></blockquote><p>注意：下载<a href="https://download.pytorch.org/tutorial/hymenoptera_data.zip" target="_blank" rel="noopener">数据</a>并提取到当前文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># Data augmentation and normalization for training</span><br><span class="line"># Just normalization for validation</span><br><span class="line">data_transforms = &#123;</span><br><span class="line">    &apos;train&apos;: transforms.Compose([</span><br><span class="line">        transforms.RandomResizedCrop(224),</span><br><span class="line">        transforms.RandomHorizontalFlip(),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])</span><br><span class="line">    ]),</span><br><span class="line">    &apos;val&apos;: transforms.Compose([</span><br><span class="line">        transforms.Resize(256),</span><br><span class="line">        transforms.CenterCrop(224),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])</span><br><span class="line">    ]),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data_dir = &apos;data/hymenoptera_data&apos;</span><br><span class="line">image_datasets = &#123;x: datasets.ImageFolder(os.path.join(data_dir, x),</span><br><span class="line">                                          data_transforms[x])</span><br><span class="line">                  for x in [&apos;train&apos;, &apos;val&apos;]&#125;</span><br><span class="line">dataloaders = &#123;x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,</span><br><span class="line">                                             shuffle=True, num_workers=4)</span><br><span class="line">              for x in [&apos;train&apos;, &apos;val&apos;]&#125;</span><br><span class="line">dataset_sizes = &#123;x: len(image_datasets[x]) for x in [&apos;train&apos;, &apos;val&apos;]&#125;</span><br><span class="line">class_names = image_datasets[&apos;train&apos;].classes</span><br><span class="line"></span><br><span class="line">device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br></pre></td></tr></table></figure><h2 id="Visualize-a-few-images"><a href="#Visualize-a-few-images" class="headerlink" title="Visualize a few images"></a>Visualize a few images</h2><p>可视化图像</p><blockquote><p>Let’s visualize a few training images so as to understand the data augmentations.</p></blockquote><p>可视化小部分训练图像，以便理解数据扩充</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">def imshow(inp, title=None):</span><br><span class="line">    &quot;&quot;&quot;Imshow for Tensor.&quot;&quot;&quot;</span><br><span class="line">    inp = inp.numpy().transpose((1, 2, 0))</span><br><span class="line">    mean = np.array([0.485, 0.456, 0.406])</span><br><span class="line">    std = np.array([0.229, 0.224, 0.225])</span><br><span class="line">    inp = std * inp + mean</span><br><span class="line">    inp = np.clip(inp, 0, 1)</span><br><span class="line">    plt.imshow(inp)</span><br><span class="line">    if title is not None:</span><br><span class="line">        plt.title(title)</span><br><span class="line">    plt.pause(0.001)  # pause a bit so that plots are updated</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Get a batch of training data</span><br><span class="line">inputs, classes = next(iter(dataloaders[&apos;train&apos;]))</span><br><span class="line"></span><br><span class="line"># Make a grid from batch</span><br><span class="line">out = torchvision.utils.make_grid(inputs)</span><br><span class="line"></span><br><span class="line">imshow(out, title=[class_names[x] for x in classes])</span><br></pre></td></tr></table></figure><p><img src="/imgs/译-transfer-learning/sphx_glr_transfer_learning_tutorial_001.png" alt></p><h2 id="Training-the-model"><a href="#Training-the-model" class="headerlink" title="Training the model"></a>Training the model</h2><p>训练模型</p><blockquote><p>Now, let’s write a general function to train a model. Here, we will illustrate:</p><ul><li>Scheduling the learning rate</li><li>Saving the best model</li></ul></blockquote><p>现在，让我们编写一个通用函数来训练一个模型。将实现以下功能：</p><ul><li>调度学习率</li><li>保存最佳模型</li></ul><blockquote><p>In the following, parameter scheduler is an LR scheduler object from torch.optim.lr_scheduler.</p></blockquote><p>下面代码中，参数scheduler是来自于包torch.optim.lr_scheduler的学习率调度器对象</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">def train_model(model, criterion, optimizer, scheduler, num_epochs=25):</span><br><span class="line">    since = time.time()</span><br><span class="line"></span><br><span class="line">    best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line">    best_acc = 0.0</span><br><span class="line"></span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        print(&apos;Epoch &#123;&#125;/&#123;&#125;&apos;.format(epoch, num_epochs - 1))</span><br><span class="line">        print(&apos;-&apos; * 10)</span><br><span class="line"></span><br><span class="line">        # Each epoch has a training and validation phase</span><br><span class="line">        for phase in [&apos;train&apos;, &apos;val&apos;]:</span><br><span class="line">            if phase == &apos;train&apos;:</span><br><span class="line">                model.train()  # Set model to training mode</span><br><span class="line">            else:</span><br><span class="line">                model.eval()   # Set model to evaluate mode</span><br><span class="line"></span><br><span class="line">            running_loss = 0.0</span><br><span class="line">            running_corrects = 0</span><br><span class="line"></span><br><span class="line">            # Iterate over data.</span><br><span class="line">            for inputs, labels in dataloaders[phase]:</span><br><span class="line">                inputs = inputs.to(device)</span><br><span class="line">                labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">                # zero the parameter gradients</span><br><span class="line">                optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">                # forward</span><br><span class="line">                # track history if only in train</span><br><span class="line">                with torch.set_grad_enabled(phase == &apos;train&apos;):</span><br><span class="line">                    outputs = model(inputs)</span><br><span class="line">                    _, preds = torch.max(outputs, 1)</span><br><span class="line">                    loss = criterion(outputs, labels)</span><br><span class="line"></span><br><span class="line">                    # backward + optimize only if in training phase</span><br><span class="line">                    if phase == &apos;train&apos;:</span><br><span class="line">                        loss.backward()</span><br><span class="line">                        optimizer.step()</span><br><span class="line"></span><br><span class="line">                # statistics</span><br><span class="line">                running_loss += loss.item() * inputs.size(0)</span><br><span class="line">                running_corrects += torch.sum(preds == labels.data)</span><br><span class="line">            if phase == &apos;train&apos;:</span><br><span class="line">                scheduler.step()</span><br><span class="line"></span><br><span class="line">            epoch_loss = running_loss / dataset_sizes[phase]</span><br><span class="line">            epoch_acc = running_corrects.double() / dataset_sizes[phase]</span><br><span class="line"></span><br><span class="line">            print(&apos;&#123;&#125; Loss: &#123;:.4f&#125; Acc: &#123;:.4f&#125;&apos;.format(</span><br><span class="line">                phase, epoch_loss, epoch_acc))</span><br><span class="line"></span><br><span class="line">            # deep copy the model</span><br><span class="line">            if phase == &apos;val&apos; and epoch_acc &gt; best_acc:</span><br><span class="line">                best_acc = epoch_acc</span><br><span class="line">                best_model_wts = copy.deepcopy(model.state_dict())</span><br><span class="line"></span><br><span class="line">        print()</span><br><span class="line"></span><br><span class="line">    time_elapsed = time.time() - since</span><br><span class="line">    print(&apos;Training complete in &#123;:.0f&#125;m &#123;:.0f&#125;s&apos;.format(</span><br><span class="line">        time_elapsed // 60, time_elapsed % 60))</span><br><span class="line">    print(&apos;Best val Acc: &#123;:4f&#125;&apos;.format(best_acc))</span><br><span class="line"></span><br><span class="line">    # load best model weights</span><br><span class="line">    model.load_state_dict(best_model_wts)</span><br><span class="line">    return model</span><br></pre></td></tr></table></figure><h2 id="Visualizing-the-model-predictions"><a href="#Visualizing-the-model-predictions" class="headerlink" title="Visualizing the model predictions"></a>Visualizing the model predictions</h2><p>可视化模型预测</p><blockquote><p>Generic function to display predictions for a few images</p></blockquote><p>显示图像预测的通用函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">def visualize_model(model, num_images=6):</span><br><span class="line">    was_training = model.training</span><br><span class="line">    model.eval()</span><br><span class="line">    images_so_far = 0</span><br><span class="line">    fig = plt.figure()</span><br><span class="line"></span><br><span class="line">    with torch.no_grad():</span><br><span class="line">        for i, (inputs, labels) in enumerate(dataloaders[&apos;val&apos;]):</span><br><span class="line">            inputs = inputs.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">            outputs = model(inputs)</span><br><span class="line">            _, preds = torch.max(outputs, 1)</span><br><span class="line"></span><br><span class="line">            for j in range(inputs.size()[0]):</span><br><span class="line">                images_so_far += 1</span><br><span class="line">                ax = plt.subplot(num_images//2, 2, images_so_far)</span><br><span class="line">                ax.axis(&apos;off&apos;)</span><br><span class="line">                ax.set_title(&apos;predicted: &#123;&#125;&apos;.format(class_names[preds[j]]))</span><br><span class="line">                imshow(inputs.cpu().data[j])</span><br><span class="line"></span><br><span class="line">                if images_so_far == num_images:</span><br><span class="line">                    model.train(mode=was_training)</span><br><span class="line">                    return</span><br><span class="line">        model.train(mode=was_training)</span><br></pre></td></tr></table></figure><h2 id="Finetuning-the-convnet"><a href="#Finetuning-the-convnet" class="headerlink" title="Finetuning the convnet"></a>Finetuning the convnet</h2><p>微调网络</p><blockquote><p>Load a pretrained model and reset final fully connected layer.</p></blockquote><p>加载预处理模型并重置最后的全连接层</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">model_ft = models.resnet18(pretrained=True)</span><br><span class="line">num_ftrs = model_ft.fc.in_features</span><br><span class="line"># Here the size of each output sample is set to 2.</span><br><span class="line"># Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).</span><br><span class="line">model_ft.fc = nn.Linear(num_ftrs, 2)</span><br><span class="line"></span><br><span class="line">model_ft = model_ft.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"># Observe that all parameters are being optimized</span><br><span class="line">optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)</span><br><span class="line"></span><br><span class="line"># Decay LR by a factor of 0.1 every 7 epochs</span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)</span><br></pre></td></tr></table></figure><h2 id="Train-and-evaluate"><a href="#Train-and-evaluate" class="headerlink" title="Train and evaluate"></a>Train and evaluate</h2><p>训练和评估</p><blockquote><p>It should take around 15-25 min on CPU. On GPU though, it takes less than a minute.</p></blockquote><p>在CPU上大约需要15-25分钟。但是在GPU上只需要不到一分钟的时间</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,</span><br><span class="line">                       num_epochs=25)</span><br></pre></td></tr></table></figure><blockquote><p>Out</p></blockquote><p>输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0/24</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.5952 Acc: 0.7172</span><br><span class="line">val Loss: 0.2687 Acc: 0.9150</span><br><span class="line"></span><br><span class="line">Epoch 1/24</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.7011 Acc: 0.7172</span><br><span class="line">val Loss: 0.2945 Acc: 0.9216</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">Epoch 23/24</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.2068 Acc: 0.9139</span><br><span class="line">val Loss: 0.2128 Acc: 0.9150</span><br><span class="line"></span><br><span class="line">Epoch 24/24</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.3255 Acc: 0.8443</span><br><span class="line">val Loss: 0.2181 Acc: 0.9085</span><br><span class="line"></span><br><span class="line">Training complete in 1m 7s</span><br><span class="line">Best val Acc: 0.921569</span><br></pre></td></tr></table></figure><p>可视化模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">visualize_model(model_ft)</span><br></pre></td></tr></table></figure><p><img src="/imgs/译-transfer-learning/sphx_glr_transfer_learning_tutorial_002.png" alt></p><h2 id="ConvNet-as-fixed-feature-extractor"><a href="#ConvNet-as-fixed-feature-extractor" class="headerlink" title="ConvNet as fixed feature extractor"></a>ConvNet as fixed feature extractor</h2><p>使用卷积网络作为固定的特征提取器</p><blockquote><p>Here, we need to freeze all the network except the final layer. We need to set requires_grad == False to freeze the parameters so that the gradients are not computed in backward().</p></blockquote><p>下面操作中将冻结除最后一层以外的所有网络。通过设置requires_grad == False来冻结参数，这样梯度就不会向后计算</p><blockquote><p>Here, we need to freeze all the network except the final layer. We need to set requires_grad == False to freeze the parameters so that the gradients are not computed in backward().</p><p>You can read more about this in the documentation here.</p></blockquote><p>更多关于梯度计算的可参考<a href="https://pytorch.org/docs/master/notes/autograd.html" target="_blank" rel="noopener">Autograd mechanics</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">model_conv = torchvision.models.resnet18(pretrained=True)</span><br><span class="line">for param in model_conv.parameters():</span><br><span class="line">    param.requires_grad = False</span><br><span class="line"></span><br><span class="line"># Parameters of newly constructed modules have requires_grad=True by default</span><br><span class="line">num_ftrs = model_conv.fc.in_features</span><br><span class="line">model_conv.fc = nn.Linear(num_ftrs, 2)</span><br><span class="line"></span><br><span class="line">model_conv = model_conv.to(device)</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"># Observe that only parameters of final layer are being optimized as</span><br><span class="line"># opposed to before.</span><br><span class="line">optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)</span><br><span class="line"></span><br><span class="line"># Decay LR by a factor of 0.1 every 7 epochs</span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)</span><br></pre></td></tr></table></figure><h2 id="Train-and-evaluate-1"><a href="#Train-and-evaluate-1" class="headerlink" title="Train and evaluate"></a>Train and evaluate</h2><p>训练和评估</p><blockquote><p>On CPU this will take about half the time compared to previous scenario. This is expected as gradients don’t need to be computed for most of the network. However, forward does need to be computed.</p></blockquote><p>与之前的场景相比，在CPU上这将花费大约一半的时间。这是可预期的，虽然大多数网络不需要计算梯度，但是向前操作确实需要计算</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model_conv = train_model(model_conv, criterion, optimizer_conv,</span><br><span class="line">                         exp_lr_scheduler, num_epochs=25)</span><br></pre></td></tr></table></figure><blockquote><p>Out:</p></blockquote><p>输出：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0/24</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.5803 Acc: 0.6680</span><br><span class="line">val Loss: 0.3703 Acc: 0.8235</span><br><span class="line"></span><br><span class="line">Epoch 1/24</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.4522 Acc: 0.7869</span><br><span class="line">val Loss: 0.1773 Acc: 0.9346</span><br><span class="line"></span><br><span class="line">Epoch 2/24</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.4594 Acc: 0.8197</span><br><span class="line">val Loss: 0.2089 Acc: 0.9216</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">Epoch 23/24</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.2957 Acc: 0.8525</span><br><span class="line">val Loss: 0.2206 Acc: 0.9281</span><br><span class="line"></span><br><span class="line">Epoch 24/24</span><br><span class="line">----------</span><br><span class="line">train Loss: 0.3527 Acc: 0.8443</span><br><span class="line">val Loss: 0.2230 Acc: 0.9477</span><br><span class="line"></span><br><span class="line">Training complete in 0m 34s</span><br><span class="line">Best val Acc: 0.954248</span><br></pre></td></tr></table></figure><p>可视化模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">visualize_model(model_conv)</span><br><span class="line"></span><br><span class="line">plt.ioff()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="/imgs/译-transfer-learning/sphx_glr_transfer_learning_tutorial_003.png" alt></p><h2 id="Further-Learning"><a href="#Further-Learning" class="headerlink" title="Further Learning"></a>Further Learning</h2><p>进一步学习</p><blockquote><p>If you would like to learn more about the applications of transfer learning, checkout our Quantized Transfer Learning for Computer Vision Tutorial.</p></blockquote><p>如果想要学习更多关于迁移学习，参考<a href="https://pytorch.org/tutorials/intermediate/quantized_transfer_learning_tutorial.html" target="_blank" rel="noopener">Quantized Transfer Learning for Computer Vision Tutorial</a></p></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.jpg" alt="zhujian 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="zhujian 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zhujian</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zhujian.tech/posts/c8566254.html" title="[译]Transfer Learning for Computer Vision Tutorial">https://www.zhujian.tech/posts/c8566254.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/python/" rel="tag"># python</a> <a href="/tags/pytorch/" rel="tag"># pytorch</a> <a href="/tags/torchvision/" rel="tag"># torchvision</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/6c61a203.html" rel="next" title="[数据集]Penn-Fudan"><i class="fa fa-chevron-left"></i> [数据集]Penn-Fudan</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/posts/c7511b44.html" rel="prev" title="迁移学习">迁移学习<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Load-Data"><span class="nav-number">1.</span> <span class="nav-text">Load Data</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Visualize-a-few-images"><span class="nav-number">2.</span> <span class="nav-text">Visualize a few images</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-the-model"><span class="nav-number">3.</span> <span class="nav-text">Training the model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Visualizing-the-model-predictions"><span class="nav-number">4.</span> <span class="nav-text">Visualizing the model predictions</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Finetuning-the-convnet"><span class="nav-number">5.</span> <span class="nav-text">Finetuning the convnet</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Train-and-evaluate"><span class="nav-number">6.</span> <span class="nav-text">Train and evaluate</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ConvNet-as-fixed-feature-extractor"><span class="nav-number">7.</span> <span class="nav-text">ConvNet as fixed feature extractor</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Train-and-evaluate-1"><span class="nav-number">8.</span> <span class="nav-text">Train and evaluate</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Further-Learning"><span class="nav-number">9.</span> <span class="nav-text">Further Learning</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zhujian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">zhujian</p><div class="site-description" itemprop="description">one bite at a time</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">169</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">129</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/./atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zjZSTU" title="Github &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;zjZSTU" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/u012005313" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;u012005313" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i> CSDN</a></span><span class="links-of-author-item"><a href="/mailto:zjzstu@gmail.com" title="Gmail &amp;rarr; mailto:zjzstu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> Gmail</a></span><span class="links-of-author-item"><a href="/mailto:505169307@gmail.com" title="QQmail &amp;rarr; mailto:505169307@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> QQmail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://cloud.tencent.com/" title="https:&#x2F;&#x2F;cloud.tencent.com" rel="noopener" target="_blank">腾讯云</a></li><li class="links-of-blogroll-item"> <a href="https://www.aliyun.com/" title="https:&#x2F;&#x2F;www.aliyun.com" rel="noopener" target="_blank">阿里云</a></li><li class="links-of-blogroll-item"> <a href="https://developer.android.com/" title="https:&#x2F;&#x2F;developer.android.com&#x2F;" rel="noopener" target="_blank">Android Dev</a></li><li class="links-of-blogroll-item"> <a href="https://jenkins.io/" title="https:&#x2F;&#x2F;jenkins.io&#x2F;" rel="noopener" target="_blank">Jenkins</a></li><li class="links-of-blogroll-item"> <a href="http://cs231n.github.io/" title="http:&#x2F;&#x2F;cs231n.github.io&#x2F;" rel="noopener" target="_blank">cs231n</a></li><li class="links-of-blogroll-item"> <a href="https://pytorch.org/docs/stable/index.html" title="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;index.html" rel="noopener" target="_blank">pytorch</a></li><li class="links-of-blogroll-item"> <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" title="https:&#x2F;&#x2F;docs.docker.com&#x2F;install&#x2F;linux&#x2F;docker-ce&#x2F;ubuntu&#x2F;" rel="noopener" target="_blank">docker</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">zhujian</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">948k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">26:20</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-bar-chart-o"></i></span> <a href="https://tongji.baidu.com/web/27249108/overview/index?siteId=13647183" rel="noopener" target="_blank">百度统计</a></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div><div class="beian"> <a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备 19026415号</a> <span class="post-meta-divider">|</span> <img src="/images/beian_icon.png" style="display:inline-block;text-decoration:none;height:13px"> <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001748" rel="noopener" target="_blank">浙公网安备 33011802001748号</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span> <span class="site-uv" title="总访客量">访客数：<span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="site-pv" title="总访问量">阅读量：<span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="17,63,61" opacity="1" zindex="-1" count="199" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '8f22595c6dd29c94467d',
      clientSecret: 'e66bd90ebb9aa66c562c753dec6c90ceecbd51f2',
      repo: 'guestbook',
      owner: 'zjZSTU',
      admin: ['zjZSTU'],
      id: 'df971fbcee8dbc2df1cc014f976ec52c',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script></body></html>