<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/./atom.xml" title="做一个幸福的人" type="application/atom+xml"><meta name="google-site-verification" content="Qr_3yqLtyErDFKHW7mE8PJz4qDUX-bf_fMLpSRckQe4"><meta name="baidu-site-verification" content="zOwIvKMV7f"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"搜索文章",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet"><meta name="description" content="参考：神经网络概述神经网络推导-批量数据神经网络推导-矩阵计算使用numpy实现神经网络模型 使用单层神经网络OneNet实现逻辑或、逻辑与和逻辑非分类 使用2层神经网络TwoNet实现逻辑异或分类 使用3层神经网络ThreeNet实现iris数据集和mnist数据集分类"><meta name="keywords" content="python,numpy,matplotlib,sklearn,pandas,神经网络"><meta property="og:type" content="article"><meta property="og:title" content="神经网络实现-numpy"><meta property="og:url" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;posts&#x2F;ba2ca878.html"><meta property="og:site_name" content="做一个幸福的人"><meta property="og:description" content="参考：神经网络概述神经网络推导-批量数据神经网络推导-矩阵计算使用numpy实现神经网络模型 使用单层神经网络OneNet实现逻辑或、逻辑与和逻辑非分类 使用2层神经网络TwoNet实现逻辑异或分类 使用3层神经网络ThreeNet实现iris数据集和mnist数据集分类"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;神经网络实现-numpy&#x2F;1-layer-network.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;神经网络实现-numpy&#x2F;logical_and.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;神经网络实现-numpy&#x2F;logical_or.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;神经网络实现-numpy&#x2F;logical_non.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;神经网络实现-numpy&#x2F;two_layer_network.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;神经网络实现-numpy&#x2F;logical_xor.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;神经网络实现-numpy&#x2F;three_layer_net.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;神经网络实现-numpy&#x2F;iris_loss.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;神经网络实现-numpy&#x2F;iris_accuracy.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;神经网络实现-numpy&#x2F;mnist_loss.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;神经网络实现-numpy&#x2F;mnist_accuracy.png"><meta property="og:updated_time" content="2020-02-15T05:36:35.875Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;神经网络实现-numpy&#x2F;1-layer-network.png"><link rel="canonical" href="https://www.zhujian.tech/posts/ba2ca878.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>神经网络实现-numpy | 做一个幸福的人</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e677aac1ac69b8826b9cfecb4e72e107";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">做一个幸福的人</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">面朝大海，春暖花开</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">129</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">48</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">169</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header> <a href="https://github.com/zjZSTU" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zhujian.tech/posts/ba2ca878.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="zhujian"><meta itemprop="description" content="one bite at a time"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="做一个幸福的人"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 神经网络实现-numpy</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-05-17 13:24:43" itemprop="dateCreated datePublished" datetime="2019-05-17T13:24:43+00:00">2019-05-17</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-15 05:36:35" itemprop="dateModified" datetime="2020-02-15T05:36:35+00:00">2020-02-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/" itemprop="url" rel="index"><span itemprop="name">算法</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">编程</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/deep-learning/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/programming-language/" itemprop="url" rel="index"><span itemprop="name">编程语言</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/data-learning/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/codebase/" itemprop="url" rel="index"><span itemprop="name">代码库</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>24k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>40 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>参考：</p><p><a href="https://www.zhujian.tech/posts/7ca31f7.html#more">神经网络概述</a></p><p><a href="https://www.zhujian.tech/posts/66015d4d.html#more">神经网络推导-批量数据</a></p><p><a href="https://www.zhujian.tech/posts/1dd3ebad.html#more">神经网络推导-矩阵计算</a></p><p>使用<code>numpy</code>实现神经网络模型</p><ul><li>使用单层神经网络<code>OneNet</code>实现逻辑或、逻辑与和逻辑非分类</li><li>使用<code>2</code>层神经网络<code>TwoNet</code>实现逻辑异或分类</li><li>使用<code>3</code>层神经网络<code>ThreeNet</code>实现<code>iris</code>数据集和<code>mnist</code>数据集分类</li></ul><a id="more"></a><h2 id="使用单层神经网络OneNet实现逻辑或、逻辑与和逻辑非分类"><a href="#使用单层神经网络OneNet实现逻辑或、逻辑与和逻辑非分类" class="headerlink" title="使用单层神经网络OneNet实现逻辑或、逻辑与和逻辑非分类"></a>使用单层神经网络<code>OneNet</code>实现逻辑或、逻辑与和逻辑非分类</h2><p>使用单层神经网络<code>OneNet</code></p><ul><li>输入层有<code>2</code>个神经元</li><li>输出层有<code>1</code>个神经元</li><li>评分函数是<code>sigmoid</code></li><li>损失函数是交叉熵损失</li></ul><p><em><code>OneNet</code>就是逻辑回归模型</em></p><p><img src="/imgs/神经网络实现-numpy/1-layer-network.png" alt></p><ul><li>$L=1$</li><li>$a^{(0)}\in R^{m\times 2}$</li><li>$W^{(1)}\in R^{2\times 1}$</li><li>$b^{(1)}\in R^{1\times 2}$</li><li>$y\in R^{m\times 1}$，每行数值表示正确类别（0或者1）</li></ul><p><strong>前向传播过程</strong></p><script type="math/tex;mode=display">
z^{(1)}=a^{(0)}\cdot W^{(1)} +b^{(1)} \\
h(z^{(1)})=p(y=1)=sigmoid(z^{(1)})=\frac {1}{1+e^{-z^{(1)}}} \\</script><p>所以分类概率是</p><script type="math/tex;mode=display">
probs=[p(y=0), p(y=1)]=[1-h(z^{(1)}), h(z^{(1)})]\\
=[\frac {e^{-z^{(1)}}}{1+e^{-z^{(1)}}}， \frac {1}{1+e^{-z^{(1)}}}] \in R^{m\times 2}</script><p>损失值是</p><script type="math/tex;mode=display">
J(z^{(1)})=-\frac {1}{m} 1^{T}\cdot (y* \ln h(z^{(1)})+(1-y)* \ln (1-h(z^{(1)})))</script><p>因为OneNet很特殊（类别不是0就是1），所以损失值可以用下式计算</p><script type="math/tex;mode=display">
J(z^{(1)})=-\frac {1}{m} (y\cdot \ln h(z^{(1)})+(1-y)\cdot \ln (1-h(z^{(1)})))</script><p><strong>反向传播过程</strong></p><p>计算最终残差$\delta^{(L)}$</p><script type="math/tex;mode=display">
dJ=d(-\frac {1}{m} 1^{T}\cdot (y* \ln h(z^{(1)})+(1-y)* \ln (1-h(z^{(1)}))))\\
=d(-\frac {1}{m} 1^{T}\cdot (y* \ln h(z^{(1)})))+d(-\frac {1}{m} 1^{T}\cdot (1-y)* \ln (1-h(z^{(1)})))</script><p>因为</p><script type="math/tex;mode=display">
d(-\frac {1}{m} 1^{T}\cdot (y* \ln h(z^{(1)})))=
d(-\frac {1}{m} 1^{T}\cdot (y* (h(z^{(1)})^{-1}\cdot dh(z^{(1)})))\\
=d(-\frac {1}{m} 1^{T}\cdot (y* (h(z^{(1)})^{-1}\cdot h(z^{(1)})\cdot (1-h(z^{(1)})* dz^{(1)}))))\\
=d(-\frac {1}{m} 1^{T}\cdot (y* ((1-h(z^{(1)})* dz^{(1)}))))\\
=d(-\frac {1}{m} y^{T}\cdot ((1-h(z^{(1)})* dz^{(1)})))\\
=d(-\frac {1}{m} y^{T} * (1-h(z^{(1)})^{T}\cdot dz^{(1)}))</script><script type="math/tex;mode=display">
d(-\frac {1}{m} 1^{T}\cdot (1-y)* \ln (1-h(z^{(1)})))=d(-\frac {1}{m} 1^{T}\cdot (1-y)* ((1-h(z^{(1)}))^{-1}\cdot d(1-h(z^{(1)}))))\\
=d(-\frac {1}{m} 1^{T}\cdot (1-y)* ((1-h(z^{(1)}))^{-1}\cdot (-1)\cdot (1-h(z^{(1)}))\cdot h(z^{(1)})* dz^{(1)}))\\
=d(-\frac {1}{m} 1^{T}\cdot (1-y)* ((-1)\cdot h(z^{(1)})* dz^{(1)}))\\
=d(\frac {1}{m} 1^{T}\cdot (1-y)* (h(z^{(1)})* dz^{(1)}))\\
=d(\frac {1}{m} (1-y)^{T}\cdot (h(z^{(1)})* dz^{(1)}))\\
=d(\frac {1}{m} (1-y)^{T}* h(z^{(1)})^{T}\cdot dz^{(1)})</script><p>所以</p><script type="math/tex;mode=display">
dJ=d(-\frac {1}{m} y^{T} * (1-h(z^{(1)})^{T}\cdot dz^{(1)}))+
d(\frac {1}{m} (1-y)^{T}* h(z^{(1)})^{T}\cdot dz^{(1)})\\
=d(\frac {1}{m} ((1-y)^{T}* h(z^{(1)})^{T} - y^{T} * (1-h(z^{(1)})^{T})\cdot dz^{(1)}))\\
=d(\frac {1}{m} (h(z^{(1)})^{T}-y^{T}* h(z^{(1)})^{T} - y^{T} + y^{T}* h(z^{(1)})^{T})\cdot dz^{(1)}))\\
=d(\frac {1}{m} (h(z^{(1)})^{T}- y^{T})\cdot dz^{(1)}))</script><script type="math/tex;mode=display">
D_{z^{(1)}}f(z^{(1)})=\frac {1}{m}\cdot (h(z^{(1)})^{T}- y^{T})\\
\bigtriangledown_{z^{(1)}}f(z^{(1)})=\frac {1}{m}\cdot (h(z^{(1)})- y)</script><p>因为<code>OneNet</code>是单层神经网络，所以仅有一个权重矩阵和偏置值</p><script type="math/tex;mode=display">
z^{(1)}=a^{(0)}\cdot W^{(1)} +b^{(1)}\\
dz^{(1)}=a^{(0)}\cdot dW^{(1)} + db^{(1)}\\
dJ=d(\frac {1}{m} (h(z^{(1)})^{T}- y^{T})\cdot dz^{(1)}))\\
=d(\frac {1}{m} (h(z^{(1)})^{T}- y^{T})\cdot (a^{(0)}\cdot dW^{(1)} + db^{(1)})))\\
=d(\frac {1}{m} (h(z^{(1)})^{T}- y^{T})\cdot a^{(0)}\cdot dW^{(1)})+d(\frac {1}{m} (h(z^{(1)})^{T}- y^{T})\cdot db^{(1)})\\</script><script type="math/tex;mode=display">
D_{W^{(1)}}f(W^{(1)})=\frac {1}{m}\cdot (h(z^{(1)})^{T}- y^{T})\cdot a^{(0)}\\
\bigtriangledown_{W^{(1)}}f(W^{(1)})=\frac {1}{m}\cdot (a^{(0)})^{T}\cdot (h(z^{(1)})- y)</script><script type="math/tex;mode=display">
D_{b^{(1)}}f(b^{(1)})=\frac {1}{m}\cdot \sum_{i=1}^{m} (h(z_{i}^{(1)})^{T}- y^{T}_{i})\\
\bigtriangledown_{b^{(1)}}f(b^{(1)})=\frac {1}{m}\cdot \sum_{i=1}^{m} (h(z_{i}^{(1)})- y_{i})</script><p><strong>偏置向量需要考虑维数</strong></p><p>进行权重更新时添加正则化项</p><script type="math/tex;mode=display">
W^{(1)} = W^{(1)} - \alpha\cdot (\nabla_{W^{(1)}} J(W, b)+\lambda \sum W^{(1)})</script><h3 id="numpy实现"><a href="#numpy实现" class="headerlink" title="numpy实现"></a>numpy实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># @Time    : 19-5-17 下午2:54</span><br><span class="line"># @Author  : zj</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># 批量大小</span><br><span class="line">N = 4</span><br><span class="line"># 输入维数</span><br><span class="line">D = 2</span><br><span class="line"># 输出类别</span><br><span class="line">K = 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def init_weights(inputs, outputs):</span><br><span class="line">    return 0.01 * np.random.normal(loc=0.0, scale=1.0, size=(inputs, outputs))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class OneNet(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, w, b):</span><br><span class="line">        self.w = w</span><br><span class="line">        self.b = b</span><br><span class="line"></span><br><span class="line">    def forward(self, inputs):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        前向计算，计算评分函数值</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.m = inputs.shape[0]</span><br><span class="line">        self.a0 = inputs</span><br><span class="line"></span><br><span class="line">        self.z1 = inputs.dot(self.w) + self.b</span><br><span class="line">        self.h = self.sigmoid(self.z1)</span><br><span class="line">        return self.h</span><br><span class="line"></span><br><span class="line">    def backward(self, output):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        反向传播，计算梯度</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        delta = (self.h - output) / self.m</span><br><span class="line">        self.dw = self.a0.T.dot(delta)</span><br><span class="line">        self.db = np.sum(delta, axis=0)</span><br><span class="line"></span><br><span class="line">    def update(self, alpha=1e-3, la=1e-3):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        更新梯度</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.w = self.w - alpha * (self.dw + la * np.sum(self.w))</span><br><span class="line">        self.b = self.b - alpha * self.db</span><br><span class="line"></span><br><span class="line">    def sigmoid(self, inputs):</span><br><span class="line">        return 1.0 / (1 + np.exp(-1 * inputs))</span><br><span class="line"></span><br><span class="line">    def get_parameters(self):</span><br><span class="line">        return self.w, self.b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_loss(score, y, beta=0.0001):</span><br><span class="line">    loss = -1.0 / score.shape[0] * (y.T.dot(np.log(score + beta)) + (1 - y).T.dot(np.log(1 - score + beta)))</span><br><span class="line">    return loss[0][0]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def draw(loss_list, title=&apos;损失图&apos;):</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.ylabel(&apos;损失值&apos;)</span><br><span class="line">    plt.xlabel(&apos;迭代/500次&apos;)</span><br><span class="line">    plt.plot(loss_list)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    w = init_weights(D, K)</span><br><span class="line">    b = init_weights(K, D)</span><br><span class="line">    net = OneNet(w, b)</span><br><span class="line"></span><br><span class="line">    input_array = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])</span><br><span class="line">    or_array = np.array([[0, 0, 0, 1]]).T</span><br><span class="line">    loss_list = []</span><br><span class="line">    total_loss = 0</span><br><span class="line">    for i in range(200000):</span><br><span class="line">        score = net.forward(input_array)</span><br><span class="line">        total_loss += compute_loss(score, or_array)</span><br><span class="line">        net.backward(or_array)</span><br><span class="line">        net.update()</span><br><span class="line">        if (i % 500) == 499:</span><br><span class="line">            print(&apos;epoch: %d loss: %.4f&apos; % (i + 1, total_loss / 500))</span><br><span class="line">            loss_list.append(total_loss / 500)</span><br><span class="line">            total_loss = 0</span><br><span class="line">    w, b = net.get_parameters()</span><br><span class="line">    print(&apos;weight: &#123;&#125;&apos;.format(w))</span><br><span class="line">    print(&apos;bias: &#123;&#125;&apos;.format(b))</span><br><span class="line"></span><br><span class="line">    print(&apos;输入  输出  预测成绩&apos;)</span><br><span class="line">    score = net.forward(input_array)</span><br><span class="line">    for item in zip(input_array, or_array, score):</span><br><span class="line">        print(item[0], item[1][0], item[2][0])</span><br><span class="line">    draw(loss_list, &apos;逻辑与&apos;)</span><br></pre></td></tr></table></figure><h3 id="逻辑与"><a href="#逻辑与" class="headerlink" title="逻辑与"></a>逻辑与</h3><p>输入值与输出值</p><ul><li>(0,0) - 0</li><li>(0,1) - 0</li><li>(1,0) - 0</li><li>(1,1) - 1</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_array = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])</span><br><span class="line">or_array = np.array([[0, 0, 0, 1]]).T</span><br></pre></td></tr></table></figure><p>参数如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 批量大小</span><br><span class="line">N = 4</span><br><span class="line"># 输入维数</span><br><span class="line">D = 2</span><br><span class="line"># 输出类别</span><br><span class="line">K = 1</span><br><span class="line"># 学习率</span><br><span class="line">alpha=1e-3</span><br><span class="line"># 正则化强度</span><br><span class="line">la=1e-3</span><br></pre></td></tr></table></figure><p>进行<code>2</code>万轮迭代后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">weight: [[3.58627447 3.58664415]</span><br><span class="line"> [3.58627445 3.58664413]]</span><br><span class="line">bias: [[-5.69865399 -5.69921649]]</span><br><span class="line">输入  输出  预测成绩</span><br><span class="line">[0 0] 0 0.003339284003451103</span><br><span class="line">[0 1] 0 0.1078994052700841</span><br><span class="line">[1 0] 0 0.1078994065762308</span><br><span class="line">[1 1] 1 0.8136486739472225</span><br></pre></td></tr></table></figure><p><img src="/imgs/神经网络实现-numpy/logical_and.png" alt></p><h3 id="逻辑或"><a href="#逻辑或" class="headerlink" title="逻辑或"></a>逻辑或</h3><p>输入值与输出值</p><ul><li>(0,0) - 0</li><li>(0,1) - 1</li><li>(1,0) - 1</li><li>(1,1) - 1</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_array = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])</span><br><span class="line">or_array = np.array([[0, 1, 1, 1]]).T</span><br></pre></td></tr></table></figure><p>参数如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 批量大小</span><br><span class="line">N = 4</span><br><span class="line"># 输入维数</span><br><span class="line">D = 2</span><br><span class="line"># 输出类别</span><br><span class="line">K = 1</span><br><span class="line"># 学习率</span><br><span class="line">alpha=1e-3</span><br><span class="line"># 正则化强度</span><br><span class="line">la=1e-3</span><br></pre></td></tr></table></figure><p>进行<code>2</code>万轮迭代后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">weight: [[4.63629047]</span><br><span class="line"> [4.63636187]]</span><br><span class="line">bias: [[-1.87568775]]</span><br><span class="line">输入  输出  预测成绩</span><br><span class="line">[0 0] 0 0.1328849727549866</span><br><span class="line">[0 1] 1 0.9405133604748699</span><br><span class="line">[1 0] 1 0.9405093656227651</span><br><span class="line">[1 1] 1 0.9993872646847757</span><br></pre></td></tr></table></figure><p><img src="/imgs/神经网络实现-numpy/logical_or.png" alt></p><h3 id="逻辑非"><a href="#逻辑非" class="headerlink" title="逻辑非"></a>逻辑非</h3><p>输入值与输出值</p><ul><li>(1) - 0</li><li>(0) - 1</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_array = np.array([[1], [0]])</span><br><span class="line">or_array = np.array([[0, 1]]).T</span><br></pre></td></tr></table></figure><p>参数如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># 批量大小</span><br><span class="line">N = 2</span><br><span class="line"># 输入维数</span><br><span class="line">D = 1</span><br><span class="line"># 输出类别</span><br><span class="line">K = 1</span><br><span class="line"># 学习率</span><br><span class="line">alpha=1e-3</span><br><span class="line"># 正则化强度</span><br><span class="line">la=1e-3</span><br></pre></td></tr></table></figure><p>进行<code>2</code>万轮迭代后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">weight: [[-6.79010254]]</span><br><span class="line">bias: [[3.26280938]]</span><br><span class="line">输入  输出  预测成绩</span><br><span class="line">[1] 0 0.02854555463398659</span><br><span class="line">[0] 1 0.9631306816639573</span><br></pre></td></tr></table></figure><p><img src="/imgs/神经网络实现-numpy/logical_non.png" alt></p><h2 id="使用2层神经网络TwoNet实现逻辑异或分类"><a href="#使用2层神经网络TwoNet实现逻辑异或分类" class="headerlink" title="使用2层神经网络TwoNet实现逻辑异或分类"></a>使用2层神经网络TwoNet实现逻辑异或分类</h2><p>使用<code>2</code>层神经网络<code>TwoNet</code></p><ul><li>网络层数$L=2$</li><li>批量数据$N$</li><li>输入层神经元个数$D$</li><li>隐藏层神经元个数$H$</li><li>输出层神经元个数$K$</li><li>激活函数是<code>relu</code></li><li>评分函数是<code>softmax</code>评分</li><li>损失函数是交叉熵损失平凡</li></ul><p><img src="/imgs/神经网络实现-numpy/two_layer_network.png" alt></p><ul><li>$a^{(0)}\in R^{N\times D}$</li><li>$W^{(1)}\in R^{D\times H}$</li><li>$b^{(1)}\in R^{1\times H}$</li><li>$W^{(2)}\in R^{H\times K}$</li><li>$b^{(2)}\in R^{1\times K}$</li><li>$Y\in R^{N\times K}$，每行仅有正确类别为1，其余为0</li></ul><p><strong>前向传播过程</strong></p><script type="math/tex;mode=display">
z^{(1)}=a^{(0)}\cdot W^{(1)}+b^{(1)} \\
a^{(1)}=relu(z^{(1)}) \\
z^{(2)}=a^{(1)}\cdot W^{(2)}+b^{(2)}</script><p>所以分类概率是</p><script type="math/tex;mode=display">
probs=h(z^{(2)})=\frac {exp(z^{(2)})}{exp(z^{(2)})\cdot A\cdot B^{T}}</script><p>其中$A\in R^{K\times 1}，B\in R^{K\times 1}$都是全$1$向量，</p><p>损失值是</p><script type="math/tex;mode=display">
dataLoss = -\frac {1}{N} 1^{T}\cdot \ln \frac {exp(z^{(2)}* Y\cdot A)}{exp(z^{2})\cdot A}</script><script type="math/tex;mode=display">
regLoss = 0.5\cdot reg\cdot ||W^{(1)}||^{2} + 0.5\cdot reg\cdot ||W^{(2)}||^{2}</script><script type="math/tex;mode=display">
J(z^{(2)})=dataLoss + regLoss</script><p><strong>反向传播过程</strong></p><p><strong>输出层输入向量梯度</strong></p><script type="math/tex;mode=display">
d(dataloss) = d(-\frac {1}{N} 1^{T}\cdot \ln \frac {exp(z^{(2)}* Y\cdot A)}{exp(z^{2})\cdot A})\\
=tr(\frac {1}{N} (probs^{T} - Y^{T})\cdot dz^{(2)})</script><p>所以</p><script type="math/tex;mode=display">
D_{z^{(2)}}f(z^{(2)})=\frac {1}{N} (probs^{T} - Y^{T})\\
\bigtriangledown_{z^{(2)}}f(z^{(2)})=\frac {1}{N} (probs - Y)</script><p><strong>对于输出层权重矩阵、偏置向量以及隐藏层输出向量</strong></p><script type="math/tex;mode=display">
z^{(2)}=a^{(1)}\cdot W^{(2)}+b^{(2)}\\
dz^{(2)}=da^{(1)}\cdot W^{(2)} + a^{(1)}\cdot dW^{(2)} + db^{(2)}</script><script type="math/tex;mode=display">
d(dataloss)
=tr(\frac {1}{N} (probs^{T} - Y^{T})\cdot dz^{(2)})\\
=tr(\frac {1}{N} (probs^{T} - Y^{T})\cdot (da^{(1)}\cdot W^{(2)} + a^{(1)}\cdot dW^{(2)} + db^{(2)}))\\
tr(\frac {1}{N} (probs^{T} - Y^{T})\cdot da^{(1)}\cdot W^{(2)}) + tr(\frac {1}{N} (probs^{T} - Y^{T})\cdot a^{(1)}\cdot dW^{(2)}) + tr(\frac {1}{N} (probs^{T} - Y^{T})\cdot db^{(2)}))</script><p>输出层权重矩阵</p><script type="math/tex;mode=display">
d(dataloss)=tr(\frac {1}{N} (probs^{T} - Y^{T})\cdot a^{(1)}\cdot dW^{(2)})</script><script type="math/tex;mode=display">
D_{W^{(2)}}f(W^{(2)})=\frac {1}{N} (probs^{T} - Y^{T})\cdot a^{(1)}\\
\bigtriangledown_{W^{(2)}}f(W^{(2)})=\frac {1}{N} (a^{(1)})^{T}\cdot (probs - Y)</script><p>输出层偏置向量</p><script type="math/tex;mode=display">
d(dataloss)=tr(\frac {1}{N} (probs^{T} - Y^{T})\cdot db^{(2)}))</script><script type="math/tex;mode=display">
D_{b^{(2)}}f(b^{(2)})=\frac {1}{N} \sum_{i=1}^{N}(probs_{i}^{T} - Y_{i}^{T})\\
\bigtriangledown_{b^{(2)}}f(b^{(2)})=\frac {1}{N} \sum_{i=1}^{N}(probs_{i} - Y_{i})</script><p>隐藏层输出向量</p><script type="math/tex;mode=display">
d(dataloss)=tr(\frac {1}{N} (probs^{T} - Y^{T})\cdot da^{(1)}\cdot W^{(2)})
=tr(\frac {1}{N} W^{(2)}\cdot (probs^{T} - Y^{T})\cdot da^{(1)})</script><script type="math/tex;mode=display">
D_{a^{(1)}}f(a^{(1)})=\frac {1}{N} W^{(2)}\cdot (probs^{T} - Y^{T})\\
\bigtriangledown_{a^{(1)}}f(a^{(1)})=\frac {1}{N} (probs - Y)\cdot (W^{(2)})^{T}</script><p><strong>对于隐藏层输入向量</strong></p><script type="math/tex;mode=display">
a^{(1)}=relu(z^{(1)})\\
da^{(1)}=1(z^{(1)}\geq 0)* dz^{(1)}</script><script type="math/tex;mode=display">
d(dataloss)
=tr(\frac {1}{N} W^{(2)}\cdot (probs^{T} - Y^{T})\cdot da^{(1)})\\
=tr(\frac {1}{N} (W^{(2)}\cdot (probs^{T} - Y^{T}))\cdot 1(z^{(1)}\geq 0)* dz^{(1)})\\
=tr(\frac {1}{N} (W^{(2)}\cdot (probs^{T} - Y^{T}))* 1(z^{(1)}\geq 0)^{T}\cdot dz^{(1)})</script><script type="math/tex;mode=display">
D_{z^{(1)}}f(z^{(1)})=\frac {1}{N} (W^{(2)}\cdot (probs^{T} - Y^{T}))* 1(z^{(1)}\geq 0)^{T}\\
\bigtriangledown_{z^{(1)}}f(z^{(1)})=\frac {1}{N} ((probs - Y)\cdot (W^{(2)})^{T})* 1(z^{(1)}\geq 0)</script><p><strong>对于隐藏层权重矩阵和偏置值</strong></p><script type="math/tex;mode=display">
z^{(1)}=a^{(0)}\cdot W^{(1)}+b^{(1)}\\
dz^{(1)}=da^{(0)}\cdot W^{(1)} + a^{(0)}\cdot dW^{(1)} + db^{(1)}</script><script type="math/tex;mode=display">
d(dataloss)
=tr(\frac {1}{N} (W^{(2)}\cdot (probs^{T} - Y^{T}))* 1(z^{(1)}\geq 0)^{T}\cdot dz^{(1)})\\
=tr(\frac {1}{N} (W^{(2)}\cdot (probs^{T} - Y^{T}))* 1(z^{(1)}\geq 0)^{T}\cdot (da^{(0)}\cdot W^{(1)} + a^{(0)}\cdot dW^{(1)} + db^{(1)}))\\
=tr(\frac {1}{N} (W^{(2)}\cdot (probs^{T} - Y^{T}))* 1(z^{(1)}\geq 0)^{T}\cdot (da^{(0)}\cdot W^{(1)})\\
+tr(\frac {1}{N} (W^{(2)}\cdot (probs^{T} - Y^{T}))* 1(z^{(1)}\geq 0)^{T}\cdot a^{(0)}\cdot dW^{(1)})
+tr(\frac {1}{N} (W^{(2)}\cdot (probs^{T} - Y^{T}))* 1(z^{(1)}\geq 0)^{T}\cdot db^{(1)})</script><p>输出层权重矩阵</p><script type="math/tex;mode=display">
d(dataloss)=tr(\frac {1}{N} (W^{(2)}\cdot (probs^{T} - Y^{T}))* 1(z^{(1)}\geq 0)^{T}\cdot a^{(0)}\cdot dW^{(1)})</script><script type="math/tex;mode=display">
D_{W^{(1)}}f(W^{(1)})=\frac {1}{N} (W^{(2)}\cdot (probs^{T} - Y^{T}))* 1(z^{(1)}\geq 0)^{T}\cdot a^{(0)}\\
\bigtriangledown_{W^{(1)}}f(W^{(1)})=\frac {1}{N} (a^{(0)})^{T}\cdot (W^{(2)}\cdot (probs^{T} - Y^{T}))* 1(z^{(1)}\geq 0)</script><p>输出层偏置向量</p><script type="math/tex;mode=display">
d(dataloss)=tr(\frac {1}{N} (W^{(2)}\cdot (probs^{T} - Y^{T}))* 1(z^{(1)}\geq 0)^{T}\cdot db^{(1)})</script><script type="math/tex;mode=display">
D_{b^{(1)}}f(b^{(1)})=\frac {1}{N} \sum_{i=1}^{N}(W^{(2)}\cdot (probs^{T} - Y^{T}))* 1(z^{(1)}\geq 0)^{T}\\
\bigtriangledown_{b^{(1)}}f(b^{(1)})=\frac {1}{N} \sum_{i=1}^{N}((probs - Y)\cdot (W^{(2)})^{T})* 1(z^{(1)}\geq 0)</script><h3 id="numpy实现-1"><a href="#numpy实现-1" class="headerlink" title="numpy实现"></a>numpy实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># @Time    : 19-5-17 下午6:45</span><br><span class="line"># @Author  : zj</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># 批量大小</span><br><span class="line">N = 4</span><br><span class="line"># 输入维数</span><br><span class="line">D = 2</span><br><span class="line"># 隐藏层大小</span><br><span class="line">H = 30</span><br><span class="line"># 输出类别</span><br><span class="line">K = 2</span><br><span class="line"></span><br><span class="line"># 学习率</span><br><span class="line">learning_rate = 1e-1</span><br><span class="line"># 正则化强度</span><br><span class="line">lambda_rate = 1e-3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def init_weights(inputs, outputs):</span><br><span class="line">    return 0.01 * np.random.normal(loc=0.0, scale=1.0, size=(inputs, outputs))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class TwoNet(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, w, b, w2, b2):</span><br><span class="line">        self.w = w</span><br><span class="line">        self.b = b</span><br><span class="line">        self.w2 = w2</span><br><span class="line">        self.b2 = b2</span><br><span class="line"></span><br><span class="line">    def forward(self, inputs):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        前向计算，计算评分函数值</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.N = inputs.shape[0]</span><br><span class="line">        self.a0 = inputs</span><br><span class="line"></span><br><span class="line">        self.z1 = inputs.dot(self.w) + self.b</span><br><span class="line">        self.a1 = np.maximum(0, self.z1)</span><br><span class="line">        self.z2 = self.a1.dot(self.w2) + self.b2</span><br><span class="line">        expscores = np.exp(self.z2)</span><br><span class="line">        self.h = expscores / np.sum(expscores, axis=1, keepdims=True)</span><br><span class="line">        return self.h</span><br><span class="line"></span><br><span class="line">    def backward(self, output):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        反向传播，计算梯度</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        delta = self.h</span><br><span class="line">        delta[range(self.N), output] -= 1</span><br><span class="line">        delta /= self.N</span><br><span class="line">        self.dw2 = self.a1.T.dot(delta)</span><br><span class="line">        self.db2 = np.sum(delta, axis=0, keepdims=True)</span><br><span class="line"></span><br><span class="line">        da1 = delta.dot(self.w2.T)</span><br><span class="line">        dz1 = da1</span><br><span class="line">        dz1[self.z1 &lt; 0] = 0</span><br><span class="line"></span><br><span class="line">        self.dw = self.a0.T.dot(dz1)</span><br><span class="line">        self.db = np.sum(dz1, axis=0, keepdims=True)</span><br><span class="line"></span><br><span class="line">    def update(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        更新梯度</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.dw2 += lambda_rate * self.w2</span><br><span class="line">        self.dw += lambda_rate * self.w</span><br><span class="line"></span><br><span class="line">        self.w2 = self.w2 - learning_rate * self.dw2</span><br><span class="line">        self.b2 = self.b2 - learning_rate * self.b2</span><br><span class="line"></span><br><span class="line">        self.w = self.w - learning_rate * self.dw</span><br><span class="line">        self.b = self.b - learning_rate * self.db</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_loss(score, y):</span><br><span class="line">    num = score.shape[0]</span><br><span class="line">    data_loss = -1.0 / num * np.sum(np.log(score[range(num), y]))</span><br><span class="line">    # reg_loss = 0.5 * lambda_rate * (np.sum(w ** 2) + np.sum(w2 ** 2))</span><br><span class="line">    return data_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_accuracy(y, score):</span><br><span class="line">    predict = np.argmax(score, axis=1)</span><br><span class="line">    return np.mean(predict == y), predict</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def draw(loss_list, title=&apos;损失图&apos;):</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.ylabel(&apos;损失值&apos;)</span><br><span class="line">    plt.xlabel(&apos;迭代/500次&apos;)</span><br><span class="line">    plt.plot(loss_list)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    w = init_weights(D, H)</span><br><span class="line">    b = init_weights(1, H)</span><br><span class="line">    w2 = init_weights(H, K)</span><br><span class="line">    b2 = init_weights(1, K)</span><br><span class="line">    net = TwoNet(w, b, w2, b2)</span><br><span class="line"></span><br><span class="line">    input_array = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])</span><br><span class="line">    xor_array = np.array([0, 1, 1, 0])</span><br><span class="line">    loss_list = []</span><br><span class="line">    total_loss = 0</span><br><span class="line">    for i in range(10000):</span><br><span class="line">        score = net.forward(input_array)</span><br><span class="line">        total_loss += compute_loss(score, xor_array)</span><br><span class="line">        net.backward(xor_array)</span><br><span class="line">        net.update()</span><br><span class="line">        if (i % 500) == 499:</span><br><span class="line">            print(&apos;epoch: %d loss: %.4f&apos; % (i + 1, total_loss / 500))</span><br><span class="line">            loss_list.append(total_loss / 500)</span><br><span class="line">            total_loss = 0</span><br><span class="line">    draw(loss_list, &apos;逻辑异或&apos;)</span><br><span class="line"></span><br><span class="line">    w, b = net.get_parameters()</span><br><span class="line">    print(&apos;weight: &#123;&#125;&apos;.format(w))</span><br><span class="line">    print(&apos;bias: &#123;&#125;&apos;.format(b))</span><br><span class="line"></span><br><span class="line">    score = net.forward(input_array)</span><br><span class="line">    res, predict = compute_accuracy(xor_array, score)</span><br><span class="line">    print(&apos;labels: &apos; + str(xor_array))</span><br><span class="line">    print(&apos;predict: &apos; + str(predict))</span><br><span class="line">    print(&apos;training accuracy: %.2f %%&apos; % (res * 100))</span><br></pre></td></tr></table></figure><h3 id="逻辑异或"><a href="#逻辑异或" class="headerlink" title="逻辑异或"></a>逻辑异或</h3><p>输入值与输出值</p><ul><li>(0,0) - 0</li><li>(0,1) - 1</li><li>(1,0) - 1</li><li>(1,1) - 0</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input_array = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])</span><br><span class="line">xor_array = np.array([0, 1, 1, 0])</span><br></pre></td></tr></table></figure><p>参数如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># 批量大小</span><br><span class="line">N = 4</span><br><span class="line"># 输入维数</span><br><span class="line">D = 2</span><br><span class="line"># 隐藏层大小</span><br><span class="line">H = 6</span><br><span class="line"># 输出类别</span><br><span class="line">K = 2</span><br><span class="line"></span><br><span class="line"># 学习率</span><br><span class="line">learning_rate = 1e-1</span><br><span class="line"># 正则化强度</span><br><span class="line">lambda_rate = 1e-3</span><br></pre></td></tr></table></figure><p>进行<code>1</code>万轮迭代后</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">weight: [[-1.39091559  0.26154732 -0.90273461  1.66258303  1.63181952 -1.61815551]</span><br><span class="line"> [ 1.39121663  0.26156278  0.90284832 -1.66206662  1.63189271 -1.61824032]]</span><br><span class="line">bias: [[ 4.92481313e-05 -2.61570825e-01  8.55319233e-06  8.17393648e-05</span><br><span class="line">  -1.63169400e+00  1.61798312e+00]]</span><br><span class="line">labels: [0 1 1 0]</span><br><span class="line">predict: [0 1 1 0]</span><br><span class="line">training accuracy: 100.00 %</span><br></pre></td></tr></table></figure><p><img src="/imgs/神经网络实现-numpy/logical_xor.png" alt></p><h2 id="使用3层神经网络ThreeNet实现iris数据集和mnist数据集分类"><a href="#使用3层神经网络ThreeNet实现iris数据集和mnist数据集分类" class="headerlink" title="使用3层神经网络ThreeNet实现iris数据集和mnist数据集分类"></a>使用3层神经网络ThreeNet实现iris数据集和mnist数据集分类</h2><p>使用<code>3</code>层神经网络<code>ThreeNet</code></p><ul><li>网络层数$L=3$</li><li>批量数据$N$</li><li>输入层神经元个数$D$</li><li>第一个隐藏层神经元个数$H1$</li><li>第二个隐藏层神经元个数$H2$</li><li>输出层神经元个数$K$</li><li>激活函数是<code>relu</code></li><li>评分函数是<code>softmax</code>评分</li><li>损失函数是交叉熵损失平凡</li></ul><p><img src="/imgs/神经网络实现-numpy/three_layer_net.png" alt></p><ul><li>$a^{(0)}\in R^{N\times D}$</li><li>$W^{(1)}\in R^{D\times H1}$</li><li>$b^{(1)}\in R^{1\times H1}$</li><li>$W^{(2)}\in R^{H1\times H2}$</li><li>$b^{(2)}\in R^{1\times H2}$</li><li>$W^{(3)}\in R^{H2\times K}$</li><li>$b^{(3)}\in R^{1\times K}$</li><li>$Y\in R^{N\times K}$，每行仅有正确类别为1，其余为0</li></ul><p><strong>前向传播过程</strong></p><script type="math/tex;mode=display">
z^{(1)}=a^{(0)}\cdot W^{(1)}+b^{(1)} \\
a^{(1)}=relu(z^{(1)}) \\
z^{(2)}=a^{(1)}\cdot W^{(2)}+b^{(2)}\\
a^{(2)}=relu(z^{(2)}) \\
z^{(3)}=a^{(2)}\cdot W^{(3)}+b^{(3)}\\</script><p>所以分类概率是</p><script type="math/tex;mode=display">
probs=h(z^{(3)})=\frac {exp(z^{(3)})}{exp(z^{(3)})\cdot A\cdot B^{T}}</script><p>其中$A\in R^{K\times 1}，B\in R^{K\times 1}$都是全$1$向量，</p><p>损失值是</p><script type="math/tex;mode=display">
dataLoss = -\frac {1}{N} 1^{T}\cdot \ln \frac {exp(z^{(3)}* Y\cdot A)}{exp(z^{3})\cdot A}</script><script type="math/tex;mode=display">
regLoss = 0.5\cdot reg\cdot ||W^{(1)}||^{2} + 0.5\cdot reg\cdot ||W^{(2)}||^{2} + 0.5\cdot reg\cdot ||W^{(3)}||^{2}</script><script type="math/tex;mode=display">
J(z^{(2)})=dataLoss + regLoss</script><p><strong>反向传播过程</strong></p><p><strong>输出层输入向量梯度</strong></p><script type="math/tex;mode=display">
\bigtriangledown_{z^{(3)}}f(z^{(3)})=\frac {1}{N} (probs - Y)</script><p><strong>对于输出层权重矩阵、偏置向量以及第二个隐藏层输出向量</strong></p><script type="math/tex;mode=display">
\bigtriangledown_{W^{(3)}}f(W^{(3)})=\frac {1}{N} (a^{(2)})^{T}\cdot (probs - Y)</script><script type="math/tex;mode=display">
\bigtriangledown_{b^{(3)}}f(b^{(3)})=\frac {1}{N} \sum_{i=1}^{N}(probs_{i} - Y_{i})</script><script type="math/tex;mode=display">
\bigtriangledown_{a^{(2)}}f(a^{(2)})=\frac {1}{N} (probs - Y)\cdot (W^{(3)})^{T}</script><p><strong>对于第二个隐藏层输入向量</strong></p><script type="math/tex;mode=display">
\bigtriangledown_{z^{(2)}}f(z^{(2)})=\frac {1}{N} ((probs - Y)\cdot (W^{(3)})^{T})* 1(z^{(2)}\geq 0)</script><p><strong>对于第二个隐藏层权重矩阵、偏置向量和第一个隐藏层输出向量</strong></p><script type="math/tex;mode=display">
\bigtriangledown_{W^{(2)}}f(W^{(2)})=\frac {1}{N} (a^{(1)})^{T}\cdot ((W^{(3)}\cdot (probs^{T} - Y^{T}))* 1(z^{(2)}\geq 0))</script><script type="math/tex;mode=display">
\bigtriangledown_{b^{(2)}}f(b^{(1)})=\frac {1}{N} \sum_{i=1}^{N}((probs - Y)\cdot (W^{(3)})^{T})* 1(z^{(2)}\geq 0)</script><script type="math/tex;mode=display">
\bigtriangledown_{a^{(1)}}f(a^{(1)})=\frac {1}{N} (((probs - Y)\cdot (W^{(3)})^{T})* 1(z^{(2)}\geq 0))\cdot (W^{(2)})^{T}</script><p><strong>对于第一个隐藏层输入向量</strong></p><script type="math/tex;mode=display">
\bigtriangledown_{z^{(2)}}f(z^{(2)})=\frac {1}{N} (((probs - Y)\cdot (W^{(3)})^{T})* 1(z^{(2)}\geq 0)\cdot (W^{(2)})^{T})* 1(z^{(1)}\geq 0)</script><p><strong>对于第一个隐藏层权重矩阵和偏置向量</strong></p><script type="math/tex;mode=display">
\bigtriangledown_{W^{(2)}}f(W^{(2)})=\frac {1}{N} (a^{(1)})^{T}\cdot ((((probs - Y)\cdot (W^{(3)})^{T})* 1(z^{(2)}\geq 0)\cdot (W^{(2)})^{T})* 1(z^{(1)}\geq 0))</script><script type="math/tex;mode=display">
\bigtriangledown_{b^{(2)}}f(b^{(1)})=\frac {1}{N} \sum_{i=1}^{N}(((probs - Y)\cdot (W^{(3)})^{T})* 1(z^{(2)}\geq 0)\cdot (W^{(2)})^{T})* 1(z^{(1)}\geq 0)</script><h3 id="numpy实现-2"><a href="#numpy实现-2" class="headerlink" title="numpy实现"></a>numpy实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">class ThreeNet(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, w, b, w2, b2, w3, b3):</span><br><span class="line">        self.w = w</span><br><span class="line">        self.b = b</span><br><span class="line">        self.w2 = w2</span><br><span class="line">        self.b2 = b2</span><br><span class="line">        self.w3 = w3</span><br><span class="line">        self.b3 = b3</span><br><span class="line"></span><br><span class="line">    def forward(self, inputs):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        前向计算，计算评分函数值</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.N = inputs.shape[0]</span><br><span class="line">        self.a0 = inputs</span><br><span class="line"></span><br><span class="line">        self.z1 = inputs.dot(self.w) + self.b</span><br><span class="line">        self.a1 = np.maximum(0, self.z1)</span><br><span class="line"></span><br><span class="line">        self.z2 = self.a1.dot(self.w2) + self.b2</span><br><span class="line">        self.a2 = np.maximum(0, self.z2)</span><br><span class="line"></span><br><span class="line">        self.z3 = self.a2.dot(self.w3) + self.b3</span><br><span class="line">        expscores = np.exp(self.z3)</span><br><span class="line">        self.h = expscores / np.sum(expscores, axis=1, keepdims=True)</span><br><span class="line">        return self.h</span><br><span class="line"></span><br><span class="line">    def backward(self, output):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        反向传播，计算梯度</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        delta = self.h</span><br><span class="line">        delta[range(self.N), output] -= 1</span><br><span class="line">        delta /= self.N</span><br><span class="line"></span><br><span class="line">        self.dw3 = self.a2.T.dot(delta)</span><br><span class="line">        self.db3 = np.sum(delta, axis=0, keepdims=True)</span><br><span class="line"></span><br><span class="line">        da2 = delta.dot(self.w3.T)</span><br><span class="line">        dz2 = da2</span><br><span class="line">        dz2[self.z2 &lt; 0] = 0</span><br><span class="line"></span><br><span class="line">        self.dw2 = self.a1.T.dot(dz2)</span><br><span class="line">        self.db2 = np.sum(dz2, axis=0, keepdims=True)</span><br><span class="line"></span><br><span class="line">        da1 = dz2.dot(self.w2.T)</span><br><span class="line">        dz1 = da1</span><br><span class="line">        dz1[self.z1 &lt; 0] = 0</span><br><span class="line"></span><br><span class="line">        self.dw = self.a0.T.dot(dz1)</span><br><span class="line">        self.db = np.sum(dz1, axis=0, keepdims=True)</span><br><span class="line"></span><br><span class="line">    def update(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        更新梯度</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.dw3 += lambda_rate * self.w3</span><br><span class="line">        self.dw2 += lambda_rate * self.w2</span><br><span class="line">        self.dw += lambda_rate * self.w</span><br><span class="line"></span><br><span class="line">        self.w3 = self.w3 - learning_rate * self.dw3</span><br><span class="line">        self.b3 = self.b3 - learning_rate * self.b3</span><br><span class="line"></span><br><span class="line">        self.w2 = self.w2 - learning_rate * self.dw2</span><br><span class="line">        self.b2 = self.b2 - learning_rate * self.b2</span><br><span class="line"></span><br><span class="line">        self.w = self.w - learning_rate * self.dw</span><br><span class="line">        self.b = self.b - learning_rate * self.db</span><br><span class="line"></span><br><span class="line">    def __copy__(self):</span><br><span class="line">        w = copy.deepcopy(self.w)</span><br><span class="line">        b = copy.deepcopy(self.b)</span><br><span class="line">        w2 = copy.deepcopy(self.w2)</span><br><span class="line">        b2 = copy.deepcopy(self.b2)</span><br><span class="line">        w3 = copy.deepcopy(self.w3)</span><br><span class="line">        b3 = copy.deepcopy(self.b3)</span><br><span class="line"></span><br><span class="line">        net = ThreeNet(w, b, w2, b2, w3, b3)</span><br><span class="line">        return net</span><br></pre></td></tr></table></figure><h3 id="iris数据集"><a href="#iris数据集" class="headerlink" title="iris数据集"></a>iris数据集</h3><p>参考：<a href="https://www.zhujian.tech/posts/2626bec3.html#more">softmax回归</a></p><p>分类鸢尾（iris）数据集，下载地址：<a href="https://www.kaggle.com/uciml/iris" target="_blank" rel="noopener">iris</a></p><p>共<code>4</code>个变量：</p><ul><li><code>SepalLengthCm</code> - 花萼长度</li><li><code>SepalWidthCm</code> - 花萼宽度</li><li><code>PetalLengthCm</code> - 花瓣长度</li><li><code>PetalWidthCm</code> - 花瓣宽度</li></ul><p>以及<code>3</code>个类别：</p><ul><li><code>Iris-setosa</code></li><li><code>Iris-versicolor</code></li><li><code>Iris-virginica</code></li></ul><p>网络和训练参数如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 批量大小</span><br><span class="line">N = 120</span><br><span class="line"># 输入维数</span><br><span class="line">D = 4</span><br><span class="line"># 隐藏层大小</span><br><span class="line">H1 = 20</span><br><span class="line">H2 = 20</span><br><span class="line"># 输出类别</span><br><span class="line">K = 3</span><br><span class="line"></span><br><span class="line"># 学习率</span><br><span class="line">learning_rate = 5e-2</span><br><span class="line"># 正则化强度</span><br><span class="line">lambda_rate = 1e-3</span><br></pre></td></tr></table></figure><p>完整代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># @Time    : 19-5-18 下午2:23</span><br><span class="line"># @Author  : zj</span><br><span class="line"></span><br><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># @Time    : 19-5-18 上午11:48</span><br><span class="line"># @Author  : zj</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import copy</span><br><span class="line">from sklearn import utils</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line"># 批量大小</span><br><span class="line">N = 120</span><br><span class="line"># 输入维数</span><br><span class="line">D = 4</span><br><span class="line"># 隐藏层大小</span><br><span class="line">H1 = 20</span><br><span class="line">H2 = 20</span><br><span class="line"># 输出类别</span><br><span class="line">K = 3</span><br><span class="line"></span><br><span class="line"># 学习率</span><br><span class="line">learning_rate = 5e-2</span><br><span class="line"># 正则化强度</span><br><span class="line">lambda_rate = 1e-3</span><br><span class="line"></span><br><span class="line">data_path = &apos;../data/iris-species/Iris.csv&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_data(shuffle=True, tsize=0.8):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    加载iris数据</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    data = pd.read_csv(data_path, header=0, delimiter=&apos;,&apos;)</span><br><span class="line"></span><br><span class="line">    if shuffle:</span><br><span class="line">        data = utils.shuffle(data)</span><br><span class="line"></span><br><span class="line">    species_dict = &#123;</span><br><span class="line">        &apos;Iris-setosa&apos;: 0,</span><br><span class="line">        &apos;Iris-versicolor&apos;: 1,</span><br><span class="line">        &apos;Iris-virginica&apos;: 2</span><br><span class="line">    &#125;</span><br><span class="line">    data[&apos;Species&apos;] = data[&apos;Species&apos;].map(species_dict)</span><br><span class="line"></span><br><span class="line">    data_x = np.array(</span><br><span class="line">        [data[&apos;SepalLengthCm&apos;], data[&apos;SepalWidthCm&apos;], data[&apos;PetalLengthCm&apos;], data[&apos;PetalWidthCm&apos;]]).T</span><br><span class="line">    data_y = data[&apos;Species&apos;]</span><br><span class="line"></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, train_size=tsize, test_size=(1 - tsize),</span><br><span class="line">                                                        shuffle=False)</span><br><span class="line"></span><br><span class="line">    return x_train, x_test, y_train, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def init_weights(inputs, outputs):</span><br><span class="line">    return 0.01 * np.random.normal(loc=0.0, scale=1.0, size=(inputs, outputs))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ThreeNet(object):</span><br><span class="line">    ...</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_loss(score, y):</span><br><span class="line">    num = score.shape[0]</span><br><span class="line">    data_loss = -1.0 / num * np.sum(np.log(score[range(num), y]))</span><br><span class="line">    # reg_loss = 0.5 * lambda_rate * (np.sum(w ** 2) + np.sum(w2 ** 2))</span><br><span class="line">    return data_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_accuracy(score, y):</span><br><span class="line">    predict = np.argmax(score, axis=1)</span><br><span class="line">    return np.mean(predict == y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def draw(loss_list, title=&apos;损失图&apos;, ylabel=&apos;损失值&apos;, xlabel=&apos;迭代/100次&apos;):</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.ylabel(ylabel)</span><br><span class="line">    plt.xlabel(xlabel)</span><br><span class="line">    plt.plot(loss_list)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    x_train, x_test, y_train, y_test = load_data(shuffle=True, tsize=0.8)</span><br><span class="line"></span><br><span class="line">    w = init_weights(D, H1)</span><br><span class="line">    b = init_weights(1, H1)</span><br><span class="line">    w2 = init_weights(H1, H2)</span><br><span class="line">    b2 = init_weights(1, H2)</span><br><span class="line">    w3 = init_weights(H2, K)</span><br><span class="line">    b3 = init_weights(1, K)</span><br><span class="line">    net = ThreeNet(w, b, w2, b2, w3, b3)</span><br><span class="line"></span><br><span class="line">    loss_list = []</span><br><span class="line">    total_loss = 0</span><br><span class="line">    accuracy_list = []</span><br><span class="line">    bestA = 0</span><br><span class="line">    best_net = None</span><br><span class="line">    for i in range(10000):</span><br><span class="line">        score = net.forward(x_train)</span><br><span class="line">        total_loss += compute_loss(score, y_train)</span><br><span class="line">        net.backward(y_train)</span><br><span class="line">        net.update()</span><br><span class="line"></span><br><span class="line">        if i % 100 == 99:</span><br><span class="line">            avg_loss = total_loss / 100</span><br><span class="line">            print(&apos;epoch: %d loss: %.4f&apos; % (i + 1, avg_loss))</span><br><span class="line">            loss_list.append(avg_loss)</span><br><span class="line">            total_loss = 0</span><br><span class="line"></span><br><span class="line">            # 计算训练数据集检测精度</span><br><span class="line">            accuracy = compute_accuracy(net.forward(x_train), y_train)</span><br><span class="line">            accuracy_list.append(accuracy)</span><br><span class="line">            if accuracy &gt;= bestA:</span><br><span class="line">                bestA = accuracy</span><br><span class="line">                best_net = net.__copy__()</span><br><span class="line"></span><br><span class="line">    draw(loss_list, &apos;iris数据集&apos;)</span><br><span class="line">    draw(accuracy_list, &apos;训练精度&apos;, &apos;检测精度&apos;)</span><br><span class="line"></span><br><span class="line">    test_score = best_net.forward(x_test)</span><br><span class="line">    res = compute_accuracy(test_score, y_test)</span><br><span class="line">    print(&apos;best train accuracy: %.2f %%&apos; % (bestA * 100))</span><br><span class="line">    print(&apos;test accuracy: %.2f %%&apos; % (res * 100))</span><br></pre></td></tr></table></figure><p>训练<code>1</code>万次结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">best train accuracy: 98.33 %</span><br><span class="line">test accuracy: 100.00 %</span><br></pre></td></tr></table></figure><p><img src="/imgs/神经网络实现-numpy/iris_loss.png" alt></p><p><img src="/imgs/神经网络实现-numpy/iris_accuracy.png" alt></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">softmax回归</th><th style="text-align:center">神经网络</th></tr></thead><tbody><tr><td style="text-align:center">iris</td><td style="text-align:center">96.67%</td><td style="text-align:center">98.33%</td></tr></tbody></table></div><h3 id="mnist数据集"><a href="#mnist数据集" class="headerlink" title="mnist数据集"></a>mnist数据集</h3><p>参考：<a href="https://www.zhujian.tech/posts/dd673751.html#more">使用softmax回归进行mnist分类</a></p><p><code>mnist</code>数据集是手写数字数据集，共有共有<code>60000</code>张训练图像和<code>10000</code>张测试图像，分别表示数字<code>0-9</code></p><p>数据集的下载和解压参考：<a href="https://blog.csdn.net/u012005313/article/details/84453316" target="_blank" rel="noopener">Python MNIST解压</a></p><p>网络和训练参数如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 批量大小</span><br><span class="line">batch_size = 256</span><br><span class="line"># 输入维数</span><br><span class="line">D = 784</span><br><span class="line"># 隐藏层大小</span><br><span class="line">H1 = 200</span><br><span class="line">H2 = 80</span><br><span class="line"># 输出类别</span><br><span class="line">K = 10</span><br><span class="line"></span><br><span class="line"># 学习率</span><br><span class="line">learning_rate = 1e-3</span><br><span class="line"># 正则化强度</span><br><span class="line">lambda_rate = 1e-3</span><br></pre></td></tr></table></figure><p>完整代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># @Time    : 19-5-18 上午11:48</span><br><span class="line"># @Author  : zj</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">import copy</span><br><span class="line">import cv2</span><br><span class="line">import os</span><br><span class="line">import warnings</span><br><span class="line"></span><br><span class="line">warnings.filterwarnings(&apos;ignore&apos;)</span><br><span class="line"></span><br><span class="line"># 批量大小</span><br><span class="line">batch_size = 256</span><br><span class="line"># 输入维数</span><br><span class="line">D = 784</span><br><span class="line"># 隐藏层大小</span><br><span class="line">H1 = 200</span><br><span class="line">H2 = 80</span><br><span class="line"># 输出类别</span><br><span class="line">K = 10</span><br><span class="line"></span><br><span class="line"># 学习率</span><br><span class="line">learning_rate = 1e-3</span><br><span class="line"># 正则化强度</span><br><span class="line">lambda_rate = 1e-3</span><br><span class="line"></span><br><span class="line">data_path = &apos;../data/mnist/&apos;</span><br><span class="line"></span><br><span class="line">cate_list = list(range(10))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_data(shuffle=True):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    加载mnist数据</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    train_dir = os.path.join(data_path, &apos;train&apos;)</span><br><span class="line">    test_dir = os.path.join(data_path, &apos;test&apos;)</span><br><span class="line"></span><br><span class="line">    x_train = []</span><br><span class="line">    x_test = []</span><br><span class="line">    y_train = []</span><br><span class="line">    y_test = []</span><br><span class="line">    train_file_list = []</span><br><span class="line">    for i in cate_list:</span><br><span class="line">        data_dir = os.path.join(train_dir, str(i))</span><br><span class="line">        file_list = os.listdir(data_dir)</span><br><span class="line">        for filename in file_list:</span><br><span class="line">            file_path = os.path.join(data_dir, filename)</span><br><span class="line">            train_file_list.append(file_path)</span><br><span class="line"></span><br><span class="line">        data_dir = os.path.join(test_dir, str(i))</span><br><span class="line">        file_list = os.listdir(data_dir)</span><br><span class="line">        for filename in file_list:</span><br><span class="line">            file_path = os.path.join(data_dir, filename)</span><br><span class="line">            img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)</span><br><span class="line">            if img is not None:</span><br><span class="line">                h, w = img.shape[:2]</span><br><span class="line">                x_test.append(img.reshape(h * w))</span><br><span class="line">                y_test.append(i)</span><br><span class="line"></span><br><span class="line">    train_file_list = np.array(train_file_list)</span><br><span class="line">    if shuffle:</span><br><span class="line">        np.random.shuffle(train_file_list)</span><br><span class="line"></span><br><span class="line">    for file_path in train_file_list:</span><br><span class="line">        img = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)</span><br><span class="line">        if img is not None:</span><br><span class="line">            h, w = img.shape[:2]</span><br><span class="line">            x_train.append(img.reshape(h * w))</span><br><span class="line">            y_train.append(int(os.path.split(file_path)[0].split(&apos;/&apos;)[-1]))</span><br><span class="line"></span><br><span class="line">    return np.array(x_train), np.array(x_test), np.array(y_train), np.array(y_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def init_weights(inputs, outputs):</span><br><span class="line">    return 0.01 * np.random.normal(loc=0.0, scale=1.0, size=(inputs, outputs))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ThreeNet(object):</span><br><span class="line">    ...</span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_loss(score, y):</span><br><span class="line">    num = score.shape[0]</span><br><span class="line">    data_loss = -1.0 / num * np.sum(np.log(score[range(num), y]))</span><br><span class="line">    # reg_loss = 0.5 * lambda_rate * (np.sum(w ** 2) + np.sum(w2 ** 2))</span><br><span class="line">    return data_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_accuracy(score, y):</span><br><span class="line">    predict = np.argmax(score, axis=1)</span><br><span class="line">    return np.mean(predict == y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def draw(loss_list, title=&apos;损失图&apos;, ylabel=&apos;损失值&apos;, xlabel=&apos;迭代/100次&apos;):</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.ylabel(ylabel)</span><br><span class="line">    plt.xlabel(xlabel)</span><br><span class="line">    plt.plot(loss_list)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    x_train, x_test, y_train, y_test = load_data(shuffle=True)</span><br><span class="line"></span><br><span class="line">    w = init_weights(D, H1)</span><br><span class="line">    b = init_weights(1, H1)</span><br><span class="line">    w2 = init_weights(H1, H2)</span><br><span class="line">    b2 = init_weights(1, H2)</span><br><span class="line">    w3 = init_weights(H2, K)</span><br><span class="line">    b3 = init_weights(1, K)</span><br><span class="line">    net = ThreeNet(w, b, w2, b2, w3, b3)</span><br><span class="line"></span><br><span class="line">    loss_list = []</span><br><span class="line">    total_loss = 0</span><br><span class="line">    accuracy_list = []</span><br><span class="line">    bestA = 0</span><br><span class="line">    best_net = None</span><br><span class="line">    range_list = np.arange(0, x_train.shape[0] - batch_size, step=batch_size)</span><br><span class="line">    for i in range(200):</span><br><span class="line">        for j in range_list:</span><br><span class="line">            data = x_train[j:j + batch_size]</span><br><span class="line">            labels = y_train[j:j + batch_size]</span><br><span class="line"></span><br><span class="line">            score = net.forward(data)</span><br><span class="line">            total_loss += compute_loss(score, labels)</span><br><span class="line">            net.backward(labels)</span><br><span class="line">            net.update()</span><br><span class="line"></span><br><span class="line">            if j == range_list[-1]:</span><br><span class="line">                avg_loss = total_loss / len(range_list)</span><br><span class="line">                print(&apos;epoch: %d loss: %.4f&apos; % (i + 1, avg_loss))</span><br><span class="line">                loss_list.append(avg_loss)</span><br><span class="line">                total_loss = 0</span><br><span class="line"></span><br><span class="line">                # 计算训练数据集检测精度</span><br><span class="line">                score = net.forward(x_train[j:j + batch_size])</span><br><span class="line">                accuracy = compute_accuracy(score, labels)</span><br><span class="line">                accuracy_list.append(accuracy)</span><br><span class="line">                if accuracy &gt;= bestA:</span><br><span class="line">                    bestA = accuracy</span><br><span class="line">                    best_net = net.__copy__()</span><br><span class="line">                break</span><br><span class="line"></span><br><span class="line">    draw(loss_list, title=&apos;mnist&apos;, xlabel=&apos;迭代/次&apos;)</span><br><span class="line">    draw(accuracy_list, title=&apos;训练精度&apos;, ylabel=&apos;检测精度&apos;, xlabel=&apos;迭代/次&apos;)</span><br><span class="line"></span><br><span class="line">    test_score = best_net.forward(x_test)</span><br><span class="line">    res = compute_accuracy(test_score, y_test)</span><br><span class="line">    print(&apos;best train accuracy: %.2f %%&apos; % (bestA * 100))</span><br><span class="line">    print(&apos;test accuracy: %.2f %%&apos; % (res * 100))</span><br></pre></td></tr></table></figure><p>训练<code>200</code>次结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">best train accuracy: 100.00 %</span><br><span class="line">test accuracy: 97.92 %</span><br></pre></td></tr></table></figure><p><img src="/imgs/神经网络实现-numpy/mnist_loss.png" alt></p><p><img src="/imgs/神经网络实现-numpy/mnist_accuracy.png" alt></p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">softmax回归</th><th style="text-align:center">神经网络</th></tr></thead><tbody><tr><td style="text-align:center">mnist</td><td style="text-align:center">92.15%</td><td style="text-align:center">97.92%</td></tr></tbody></table></div></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.jpg" alt="zhujian 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="zhujian 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zhujian</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zhujian.tech/posts/ba2ca878.html" title="神经网络实现-numpy">https://www.zhujian.tech/posts/ba2ca878.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/python/" rel="tag"># python</a> <a href="/tags/numpy/" rel="tag"># numpy</a> <a href="/tags/matplotlib/" rel="tag"># matplotlib</a> <a href="/tags/sklearn/" rel="tag"># sklearn</a> <a href="/tags/pandas/" rel="tag"># pandas</a> <a href="/tags/nerual-network/" rel="tag"># 神经网络</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/1dd3ebad.html" rel="next" title="神经网络推导-矩阵计算"><i class="fa fa-chevron-left"></i> 神经网络推导-矩阵计算</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/posts/5a77dbca.html" rel="prev" title="神经网络实现-pytorch">神经网络实现-pytorch<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#使用单层神经网络OneNet实现逻辑或、逻辑与和逻辑非分类"><span class="nav-number">1.</span> <span class="nav-text">使用单层神经网络OneNet实现逻辑或、逻辑与和逻辑非分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#numpy实现"><span class="nav-number">1.1.</span> <span class="nav-text">numpy实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#逻辑与"><span class="nav-number">1.2.</span> <span class="nav-text">逻辑与</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#逻辑或"><span class="nav-number">1.3.</span> <span class="nav-text">逻辑或</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#逻辑非"><span class="nav-number">1.4.</span> <span class="nav-text">逻辑非</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用2层神经网络TwoNet实现逻辑异或分类"><span class="nav-number">2.</span> <span class="nav-text">使用2层神经网络TwoNet实现逻辑异或分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#numpy实现-1"><span class="nav-number">2.1.</span> <span class="nav-text">numpy实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#逻辑异或"><span class="nav-number">2.2.</span> <span class="nav-text">逻辑异或</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#使用3层神经网络ThreeNet实现iris数据集和mnist数据集分类"><span class="nav-number">3.</span> <span class="nav-text">使用3层神经网络ThreeNet实现iris数据集和mnist数据集分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#numpy实现-2"><span class="nav-number">3.1.</span> <span class="nav-text">numpy实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#iris数据集"><span class="nav-number">3.2.</span> <span class="nav-text">iris数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#mnist数据集"><span class="nav-number">3.3.</span> <span class="nav-text">mnist数据集</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zhujian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">zhujian</p><div class="site-description" itemprop="description">one bite at a time</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">169</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">129</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/./atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zjZSTU" title="Github &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;zjZSTU" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/u012005313" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;u012005313" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i> CSDN</a></span><span class="links-of-author-item"><a href="/mailto:zjzstu@gmail.com" title="Gmail &amp;rarr; mailto:zjzstu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> Gmail</a></span><span class="links-of-author-item"><a href="/mailto:505169307@gmail.com" title="QQmail &amp;rarr; mailto:505169307@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> QQmail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://cloud.tencent.com/" title="https:&#x2F;&#x2F;cloud.tencent.com" rel="noopener" target="_blank">腾讯云</a></li><li class="links-of-blogroll-item"> <a href="https://www.aliyun.com/" title="https:&#x2F;&#x2F;www.aliyun.com" rel="noopener" target="_blank">阿里云</a></li><li class="links-of-blogroll-item"> <a href="https://developer.android.com/" title="https:&#x2F;&#x2F;developer.android.com&#x2F;" rel="noopener" target="_blank">Android Dev</a></li><li class="links-of-blogroll-item"> <a href="https://jenkins.io/" title="https:&#x2F;&#x2F;jenkins.io&#x2F;" rel="noopener" target="_blank">Jenkins</a></li><li class="links-of-blogroll-item"> <a href="http://cs231n.github.io/" title="http:&#x2F;&#x2F;cs231n.github.io&#x2F;" rel="noopener" target="_blank">cs231n</a></li><li class="links-of-blogroll-item"> <a href="https://pytorch.org/docs/stable/index.html" title="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;index.html" rel="noopener" target="_blank">pytorch</a></li><li class="links-of-blogroll-item"> <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" title="https:&#x2F;&#x2F;docs.docker.com&#x2F;install&#x2F;linux&#x2F;docker-ce&#x2F;ubuntu&#x2F;" rel="noopener" target="_blank">docker</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">zhujian</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">948k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">26:20</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-bar-chart-o"></i></span> <a href="https://tongji.baidu.com/web/27249108/overview/index?siteId=13647183" rel="noopener" target="_blank">百度统计</a></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div><div class="beian"> <a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备 19026415号</a> <span class="post-meta-divider">|</span> <img src="/images/beian_icon.png" style="display:inline-block;text-decoration:none;height:13px"> <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001748" rel="noopener" target="_blank">浙公网安备 33011802001748号</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span> <span class="site-uv" title="总访客量">访客数：<span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="site-pv" title="总访问量">阅读量：<span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="17,63,61" opacity="1" zindex="-1" count="199" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '8f22595c6dd29c94467d',
      clientSecret: 'e66bd90ebb9aa66c562c753dec6c90ceecbd51f2',
      repo: 'guestbook',
      owner: 'zjZSTU',
      admin: ['zjZSTU'],
      id: '919b091575d9918e84abdf2298d6348d',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script></body></html>