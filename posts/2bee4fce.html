<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/./atom.xml" title="做一个幸福的人" type="application/atom+xml"><meta name="google-site-verification" content="Qr_3yqLtyErDFKHW7mE8PJz4qDUX-bf_fMLpSRckQe4"><meta name="baidu-site-verification" content="zOwIvKMV7f"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"搜索文章",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet"><meta name="description" content="pytorch提供了多种失活函数实现 torch.nn.Dropout torch.nn.Dropout2d torch.nn.Dropout3d torch.nn.AlphaDropout"><meta name="keywords" content="python,pytorch,torchvision,matplotlib,随机失活"><meta property="og:type" content="article"><meta property="og:title" content="随机失活-pytorch"><meta property="og:url" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;posts&#x2F;2bee4fce.html"><meta property="og:site_name" content="做一个幸福的人"><meta property="og:description" content="pytorch提供了多种失活函数实现 torch.nn.Dropout torch.nn.Dropout2d torch.nn.Dropout3d torch.nn.AlphaDropout"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;随机失活-pytorch&#x2F;lenet_5_loss.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;随机失活-pytorch&#x2F;lenet_5_accuracy.png"><meta property="og:updated_time" content="2020-02-15T05:36:35.879Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;随机失活-pytorch&#x2F;lenet_5_loss.png"><link rel="canonical" href="https://www.zhujian.tech/posts/2bee4fce.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>随机失活-pytorch | 做一个幸福的人</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e677aac1ac69b8826b9cfecb4e72e107";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">做一个幸福的人</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">面朝大海，春暖花开</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">129</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">48</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">169</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header> <a href="https://github.com/zjZSTU" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zhujian.tech/posts/2bee4fce.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="zhujian"><meta itemprop="description" content="one bite at a time"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="做一个幸福的人"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 随机失活-pytorch</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-06-08 10:49:36" itemprop="dateCreated datePublished" datetime="2019-06-08T10:49:36+00:00">2019-06-08</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-15 05:36:35" itemprop="dateModified" datetime="2020-02-15T05:36:35+00:00">2020-02-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/" itemprop="url" rel="index"><span itemprop="name">算法</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">编程</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/optimization/" itemprop="url" rel="index"><span itemprop="name">最优化</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/programming-language/" itemprop="url" rel="index"><span itemprop="name">编程语言</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/codebase/" itemprop="url" rel="index"><span itemprop="name">代码库</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>7.4k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>12 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p><code>pytorch</code>提供了多种失活函数实现</p><ol><li><a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout" target="_blank" rel="noopener">torch.nn.Dropout</a></li><li><a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout2d" target="_blank" rel="noopener">torch.nn.Dropout2d</a></li><li><a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Dropout3d" target="_blank" rel="noopener">torch.nn.Dropout3d</a></li><li><a href="https://pytorch.org/docs/stable/nn.html#torch.nn.AlphaDropout" target="_blank" rel="noopener">torch.nn.AlphaDropout</a></li></ol><a id="more"></a><p>下面首先介绍<code>Dropout</code>和<code>Dropout2d</code>的使用，然后通过<code>LeNet-5</code>模型进行<code>cifar-10</code>的训练</p><h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>对每个神经元进行随机失活</p><blockquote><p>CLASS torch.nn.Dropout(p=0.5, inplace=False)</p></blockquote><p>默认失活概率为$p=0.5$</p><p>输入数组可以是任意大小，输出数组大小和输出数组一致</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; dropout = nn.Dropout()</span><br><span class="line">&gt;&gt;&gt; inputs = torch.randn(2,4)</span><br><span class="line">&gt;&gt;&gt; dropout(inputs)</span><br><span class="line">tensor([[ 3.5830,  5.0388, -0.0000,  0.0000],</span><br><span class="line">        [ 2.4098, -2.1856, -0.7015,  2.0616]])</span><br><span class="line">&gt;&gt;&gt; dropout(inputs)</span><br><span class="line">tensor([[ 3.5830,  5.0388, -0.0000,  0.0000],</span><br><span class="line">        [ 0.0000, -2.1856, -0.0000,  0.0000]])</span><br><span class="line">&gt;&gt;&gt; dropout(inputs)</span><br><span class="line">tensor([[0.0000, 0.0000, -0.0000, 1.7565],</span><br><span class="line">        [0.0000, -0.0000, -0.0000, 2.0616]])</span><br></pre></td></tr></table></figure><p><strong>注意：参数$p$表示失活概率，$p=1$表示全部置为$0$，$p=0$表示不执行失活操作</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; dropout = nn.Dropout(p=0)</span><br><span class="line">&gt;&gt;&gt; inputs = torch.randn(2,4)</span><br><span class="line">&gt;&gt;&gt; dropout(inputs)</span><br><span class="line">tensor([[ 1.2098,  0.3409,  1.4093,  0.6397],</span><br><span class="line">        [ 1.2380, -0.8287,  0.6893,  0.9666]])</span><br><span class="line">&gt;&gt;&gt; dropout(inputs)</span><br><span class="line">tensor([[ 1.2098,  0.3409,  1.4093,  0.6397],</span><br><span class="line">        [ 1.2380, -0.8287,  0.6893,  0.9666]])</span><br><span class="line">&gt;&gt;&gt; dropout = nn.Dropout(p=1)</span><br><span class="line">&gt;&gt;&gt; dropout(inputs)</span><br><span class="line">tensor([[0., 0., 0., 0.],</span><br><span class="line">        [0., -0., 0., 0.]])</span><br><span class="line">&gt;&gt;&gt; dropout(inputs)</span><br><span class="line">tensor([[0., 0., 0., 0.],</span><br><span class="line">        [0., -0., 0., 0.]])</span><br></pre></td></tr></table></figure><h2 id="Dropout2d"><a href="#Dropout2d" class="headerlink" title="Dropout2d"></a>Dropout2d</h2><p>对每个通道（一个通道表示一个激活图）进行随机失活</p><blockquote><p>CLASS torch.nn.Dropout2d(p=0.5, inplace=False)</p></blockquote><p>默认失活概率为$p=0.5$</p><p>输入数组大小至少为<code>2</code>维，默认为$[N, C, H, W]$，输出数组大小和输出数组一致</p><blockquote><p>RuntimeError: Feature dropout requires at least 2 dimensions in the input</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; dropout = nn.Dropout2d()</span><br><span class="line">&gt;&gt;&gt; inputs = torch.randn(2,3,2,2)</span><br><span class="line">&gt;&gt;&gt; dropout(inputs)</span><br><span class="line">tensor([[[[ 2.0601,  0.0035],</span><br><span class="line">          [-0.7429,  1.2160]],</span><br><span class="line"></span><br><span class="line">         [[-0.0000,  0.0000],</span><br><span class="line">          [-0.0000,  0.0000]],</span><br><span class="line"></span><br><span class="line">         [[-1.3138, -1.9364],</span><br><span class="line">          [-1.1147,  0.6847]]],</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        [[[ 0.0000, -0.0000],</span><br><span class="line">          [-0.0000, -0.0000]],</span><br><span class="line"></span><br><span class="line">         [[-0.0000, -0.0000],</span><br><span class="line">          [-0.0000, -0.0000]],</span><br><span class="line"></span><br><span class="line">         [[-0.0000,  0.0000],</span><br><span class="line">          [-0.0000,  0.0000]]]])</span><br></pre></td></tr></table></figure><p><strong>注意：参数$p$表示失活概率，$p=1$表示全部置为$0$，$p=0$表示不执行失活操作</strong></p><h2 id="训练-测试阶段实现"><a href="#训练-测试阶段实现" class="headerlink" title="训练/测试阶段实现"></a>训练/测试阶段实现</h2><p><code>Pytorch</code>实现采用<strong>反向失活</strong>方式，在训练阶段，除了进行随机失活操作外，还将结果乘以缩放因子$\frac {1}{1-p}$，这样在测试阶段直接计算全部神经元即可</p><p>所以需要区分训练阶段和测试阶段，有两种方式</p><ol><li>设置标志位</li><li>添加测试函数</li></ol><h3 id="设置标志位"><a href="#设置标志位" class="headerlink" title="设置标志位"></a>设置标志位</h3><p>参考：</p><p><a href="https://discuss.pytorch.org/t/model-train-and-model-eval-vs-model-and-model-eval/5744" target="_blank" rel="noopener">Model.train() and model.eval() vs model and model.eval()</a></p><p><a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module.eval" target="_blank" rel="noopener">torch.nn.Module.eval</a></p><p><a href="https://pytorch.org/docs/stable/nn.html#torch.nn.Module.train" target="_blank" rel="noopener">torch.nn.Module.train</a></p><p><code>Pytorch</code>采用设置标志位的方式判断训练和测试阶段</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def train(self, mode=True):</span><br><span class="line">        self.training = mode</span><br><span class="line">        for module in self.children():</span><br><span class="line">                module.train(mode)</span><br><span class="line">        return self</span><br><span class="line">def eval(self):</span><br><span class="line">        return self.train(False)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net.train()  # 训练模式</span><br><span class="line">net.eval()  # 测试模式</span><br></pre></td></tr></table></figure><h3 id="添加测试函数"><a href="#添加测试函数" class="headerlink" title="添加测试函数"></a>添加测试函数</h3><p>另一种方式是重写测试函数，将训练和测试实现分开即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">def forward(self, inputs): # 训练实现</span><br><span class="line">        a1 = F.relu(self.conv1(inputs))</span><br><span class="line">        a1 = self.dropout2d(a1)</span><br><span class="line">        z2 = self.pool(a1)</span><br><span class="line"></span><br><span class="line">        a3 = F.relu(self.conv2(z2))</span><br><span class="line">        a3 = self.dropout2d(a3)</span><br><span class="line">        z4 = self.pool(a3)</span><br><span class="line"></span><br><span class="line">        a5 = F.relu(self.conv3(z4))</span><br><span class="line">        a5 = self.dropout2d(a5)</span><br><span class="line"></span><br><span class="line">        x = a5.view(-1, self.num_flat_features(a5))</span><br><span class="line"></span><br><span class="line">        a6 = F.relu(self.fc1(x))</span><br><span class="line">        a6 = self.dropout(a6)</span><br><span class="line">        return self.fc2(a6)</span><br><span class="line"></span><br><span class="line">def predict(self, inputs): # 测试实现</span><br><span class="line">        a1 = F.relu(self.conv1(inputs))</span><br><span class="line">        z2 = self.pool(a1)</span><br><span class="line"></span><br><span class="line">        a3 = F.relu(self.conv2(z2))</span><br><span class="line">        z4 = self.pool(a3)</span><br><span class="line"></span><br><span class="line">        a5 = F.relu(self.conv3(z4))</span><br><span class="line"></span><br><span class="line">        x = a5.view(-1, self.num_flat_features(a5))</span><br><span class="line"></span><br><span class="line">        a6 = F.relu(self.fc1(x))</span><br><span class="line">        return self.fc2(a6)</span><br></pre></td></tr></table></figure><h2 id="LeNet-5测试"><a href="#LeNet-5测试" class="headerlink" title="LeNet-5测试"></a>LeNet-5测试</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">class LeNet5(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, in_channels, p=0.0):</span><br><span class="line">        super(LeNet5, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=6, kernel_size=5, stride=1, padding=0, bias=True)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True)</span><br><span class="line">        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1, padding=0, bias=True)</span><br><span class="line"></span><br><span class="line">        self.pool = nn.MaxPool2d((2, 2), stride=2)</span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(in_features=120, out_features=84, bias=True)</span><br><span class="line">        self.fc2 = nn.Linear(84, 10, bias=True)</span><br><span class="line"></span><br><span class="line">        self.p = p</span><br><span class="line">        self.dropout2d = nn.Dropout2d(p=p)</span><br><span class="line">        self.dropout = nn.Dropout(p=p)</span><br><span class="line"></span><br><span class="line">    def forward(self, inputs):</span><br><span class="line">        a1 = F.relu(self.conv1(inputs))</span><br><span class="line">        a1 = self.dropout2d(a1)</span><br><span class="line">        z2 = self.pool(a1)</span><br><span class="line"></span><br><span class="line">        a3 = F.relu(self.conv2(z2))</span><br><span class="line">        a3 = self.dropout2d(a3)</span><br><span class="line">        z4 = self.pool(a3)</span><br><span class="line"></span><br><span class="line">        a5 = F.relu(self.conv3(z4))</span><br><span class="line">        a5 = self.dropout2d(a5)</span><br><span class="line"></span><br><span class="line">        x = a5.view(-1, self.num_flat_features(a5))</span><br><span class="line"></span><br><span class="line">        a6 = F.relu(self.fc1(x))</span><br><span class="line">        a6 = self.dropout(a6)</span><br><span class="line">        return self.fc2(a6)</span><br><span class="line"></span><br><span class="line">    def predict(self, inputs):</span><br><span class="line">        a1 = F.relu(self.conv1(inputs))</span><br><span class="line">        z2 = self.pool(a1)</span><br><span class="line"></span><br><span class="line">        a3 = F.relu(self.conv2(z2))</span><br><span class="line">        z4 = self.pool(a3)</span><br><span class="line"></span><br><span class="line">        a5 = F.relu(self.conv3(z4))</span><br><span class="line"></span><br><span class="line">        x = a5.view(-1, self.num_flat_features(a5))</span><br><span class="line"></span><br><span class="line">        a6 = F.relu(self.fc1(x))</span><br><span class="line">        return self.fc2(a6)</span><br><span class="line"></span><br><span class="line">    def num_flat_features(self, x):</span><br><span class="line">        size = x.size()[1:]  # all dimensions except the batch dimension</span><br><span class="line">        num_features = 1</span><br><span class="line">        for s in size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        return num_features</span><br></pre></td></tr></table></figure><p>共测试<code>4</code>个网络</p><ul><li>网络$A$：标准神经网络</li><li>网络$B$：对全连接层进行失活操作</li><li>网络$C$：对卷积层进行失活操作</li><li>网络$D$：对所有隐藏层进行失活操作</li></ul><p>参考细节如下：</p><ul><li>批量大小<code>batch_size=256</code></li><li>迭代次数<code>epochs=1000</code></li><li>学习率<code>lr=1e-2</code></li><li>失活率<code>p=0.5</code></li><li>动量因子<code>momentum=0.9</code></li><li>每隔<code>150</code>轮迭代衰减一半学习率</li></ul><p>每隔<code>20</code>轮进行一次精度检测，实现如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># @Time    : 19-6-7 下午3:09</span><br><span class="line"># @Author  : zj</span><br><span class="line"></span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">import torch.optim as optim</span><br><span class="line">import torch.optim.lr_scheduler as lr_scheduler</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">import torchvision.transforms as transforms</span><br><span class="line">import torchvision.datasets as datasets</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line"># 批量大小</span><br><span class="line">batch_size = 256</span><br><span class="line"># 迭代次数</span><br><span class="line">epochs = 1000</span><br><span class="line"></span><br><span class="line"># 学习率</span><br><span class="line">lr = 1e-2</span><br><span class="line"># 失活率</span><br><span class="line">p_h = 0.5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_cifar_10_data(batch_size=128, shuffle=False):</span><br><span class="line">    data_dir = &apos;/home/lab305/Documents/data/cifar_10/&apos;</span><br><span class="line"></span><br><span class="line">    transform = transforms.Compose([</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    train_data_set = datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)</span><br><span class="line">    test_data_set = datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)</span><br><span class="line"></span><br><span class="line">    train_loader = DataLoader(train_data_set, batch_size=batch_size, shuffle=shuffle)</span><br><span class="line">    test_loader = DataLoader(test_data_set, batch_size=batch_size, shuffle=shuffle)</span><br><span class="line"></span><br><span class="line">    return train_loader, test_loader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class LeNet5(nn.Module):</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">def compute_accuracy(loader, net, device):</span><br><span class="line">    total = 0</span><br><span class="line">    correct = 0</span><br><span class="line">    for item in loader:</span><br><span class="line">        data, labels = item</span><br><span class="line">        data = data.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">        scores = net.predict(data)</span><br><span class="line">        predicted = torch.argmax(scores, dim=1)</span><br><span class="line">        total += labels.size(0)</span><br><span class="line">        correct += (predicted == labels).sum().item()</span><br><span class="line"></span><br><span class="line">    return correct / total</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    train_loader, test_loader = load_cifar_10_data(batch_size=batch_size, shuffle=True)</span><br><span class="line"></span><br><span class="line">    device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="line"></span><br><span class="line">    net = LeNet5(3, p=p_h).to(device)</span><br><span class="line">    criterion = nn.CrossEntropyLoss().to(device)</span><br><span class="line">    optimer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)</span><br><span class="line">    stepLR = lr_scheduler.StepLR(optimer, step_size=150, gamma=0.5)</span><br><span class="line"></span><br><span class="line">    best_train_accuracy = 0.99</span><br><span class="line">    best_test_accuracy = 0</span><br><span class="line"></span><br><span class="line">    loss_list = []</span><br><span class="line">    train_list = []</span><br><span class="line">    for i in range(epochs):</span><br><span class="line">        num = 0</span><br><span class="line">        total_loss = 0</span><br><span class="line">        start = time.time()</span><br><span class="line">        net.train()  # 训练模式</span><br><span class="line">        for j, item in enumerate(train_loader, 0):</span><br><span class="line">            data, labels = item</span><br><span class="line">            data = data.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">            scores = net.forward(data)</span><br><span class="line">            loss = criterion.forward(scores, labels)</span><br><span class="line"></span><br><span class="line">            optimer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimer.step()</span><br><span class="line"></span><br><span class="line">            total_loss += loss.item()</span><br><span class="line">            num += 1</span><br><span class="line">        end = time.time()</span><br><span class="line">        stepLR.step()</span><br><span class="line"></span><br><span class="line">        avg_loss = total_loss / num</span><br><span class="line">        loss_list.append(float(&apos;%.4f&apos; % avg_loss))</span><br><span class="line">        print(&apos;epoch: %d time: %.2f loss: %.4f&apos; % (i + 1, end - start, avg_loss))</span><br><span class="line"></span><br><span class="line">        if i % 20 == 19:</span><br><span class="line">            # 计算训练数据集检测精度</span><br><span class="line">            net.eval()  # 测试模式</span><br><span class="line">            train_accuracy = compute_accuracy(train_loader, net, device)</span><br><span class="line">            train_list.append(float(&apos;%.4f&apos; % train_accuracy))</span><br><span class="line">            if best_train_accuracy &lt; train_accuracy:</span><br><span class="line">                best_train_accuracy = train_accuracy</span><br><span class="line"></span><br><span class="line">                test_accuracy = compute_accuracy(test_loader, net, device)</span><br><span class="line">                if best_test_accuracy &lt; test_accuracy:</span><br><span class="line">                    best_test_accuracy = test_accuracy</span><br><span class="line"></span><br><span class="line">            print(&apos;best train accuracy: %.2f %%   best test accuracy: %.2f %%&apos; % (</span><br><span class="line">                best_train_accuracy * 100, best_test_accuracy * 100))</span><br><span class="line">            print(loss_list)</span><br><span class="line">            print(train_list)</span><br></pre></td></tr></table></figure><p><code>1000</code>轮迭代后的测试精度如下：</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">最好训练集精度</th><th style="text-align:center">最好测试集精度</th></tr></thead><tbody><tr><td style="text-align:center">A</td><td style="text-align:center">100%</td><td style="text-align:center">60.45 %</td></tr><tr><td style="text-align:center">B</td><td style="text-align:center">99.84%</td><td style="text-align:center">61.47%</td></tr><tr><td style="text-align:center">C</td><td style="text-align:center">57.04%</td><td style="text-align:center">/</td></tr><tr><td style="text-align:center">D</td><td style="text-align:center">50.93%</td><td style="text-align:center">/</td></tr></tbody></table></div><p>其损失值和训练集精度值变化如下：</p><p><img src="/imgs/随机失活-pytorch/lenet_5_loss.png" alt></p><p><img src="/imgs/随机失活-pytorch/lenet_5_accuracy.png" alt></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>从训练结果看出</p><ol><li>失活网络需要更多的时间训练才能收敛</li><li>失活操作能够提高泛化能力</li><li>对卷积层进行失活操作会导致损失值过早收敛</li></ol></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.jpg" alt="zhujian 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="zhujian 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zhujian</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zhujian.tech/posts/2bee4fce.html" title="随机失活-pytorch">https://www.zhujian.tech/posts/2bee4fce.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/python/" rel="tag"># python</a> <a href="/tags/pytorch/" rel="tag"># pytorch</a> <a href="/tags/torchvision/" rel="tag"># torchvision</a> <a href="/tags/matplotlib/" rel="tag"># matplotlib</a> <a href="/tags/dropout/" rel="tag"># 随机失活</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/20cc7a49.html" rel="next" title="随机失活"><i class="fa fa-chevron-left"></i> 随机失活</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/posts/ca9994d1.html" rel="prev" title="AlexNet">AlexNet<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Dropout"><span class="nav-number">1.</span> <span class="nav-text">Dropout</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Dropout2d"><span class="nav-number">2.</span> <span class="nav-text">Dropout2d</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练-测试阶段实现"><span class="nav-number">3.</span> <span class="nav-text">训练/测试阶段实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#设置标志位"><span class="nav-number">3.1.</span> <span class="nav-text">设置标志位</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#添加测试函数"><span class="nav-number">3.2.</span> <span class="nav-text">添加测试函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LeNet-5测试"><span class="nav-number">4.</span> <span class="nav-text">LeNet-5测试</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#小结"><span class="nav-number">5.</span> <span class="nav-text">小结</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zhujian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">zhujian</p><div class="site-description" itemprop="description">one bite at a time</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">169</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">129</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/./atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zjZSTU" title="Github &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;zjZSTU" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/u012005313" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;u012005313" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i> CSDN</a></span><span class="links-of-author-item"><a href="/mailto:zjzstu@gmail.com" title="Gmail &amp;rarr; mailto:zjzstu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> Gmail</a></span><span class="links-of-author-item"><a href="/mailto:505169307@gmail.com" title="QQmail &amp;rarr; mailto:505169307@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> QQmail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://cloud.tencent.com/" title="https:&#x2F;&#x2F;cloud.tencent.com" rel="noopener" target="_blank">腾讯云</a></li><li class="links-of-blogroll-item"> <a href="https://www.aliyun.com/" title="https:&#x2F;&#x2F;www.aliyun.com" rel="noopener" target="_blank">阿里云</a></li><li class="links-of-blogroll-item"> <a href="https://developer.android.com/" title="https:&#x2F;&#x2F;developer.android.com&#x2F;" rel="noopener" target="_blank">Android Dev</a></li><li class="links-of-blogroll-item"> <a href="https://jenkins.io/" title="https:&#x2F;&#x2F;jenkins.io&#x2F;" rel="noopener" target="_blank">Jenkins</a></li><li class="links-of-blogroll-item"> <a href="http://cs231n.github.io/" title="http:&#x2F;&#x2F;cs231n.github.io&#x2F;" rel="noopener" target="_blank">cs231n</a></li><li class="links-of-blogroll-item"> <a href="https://pytorch.org/docs/stable/index.html" title="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;index.html" rel="noopener" target="_blank">pytorch</a></li><li class="links-of-blogroll-item"> <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" title="https:&#x2F;&#x2F;docs.docker.com&#x2F;install&#x2F;linux&#x2F;docker-ce&#x2F;ubuntu&#x2F;" rel="noopener" target="_blank">docker</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">zhujian</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">948k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">26:20</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-bar-chart-o"></i></span> <a href="https://tongji.baidu.com/web/27249108/overview/index?siteId=13647183" rel="noopener" target="_blank">百度统计</a></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div><div class="beian"> <a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备 19026415号</a> <span class="post-meta-divider">|</span> <img src="/images/beian_icon.png" style="display:inline-block;text-decoration:none;height:13px"> <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001748" rel="noopener" target="_blank">浙公网安备 33011802001748号</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span> <span class="site-uv" title="总访客量">访客数：<span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="site-pv" title="总访问量">阅读量：<span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="17,63,61" opacity="1" zindex="-1" count="199" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '8f22595c6dd29c94467d',
      clientSecret: 'e66bd90ebb9aa66c562c753dec6c90ceecbd51f2',
      repo: 'guestbook',
      owner: 'zjZSTU',
      admin: ['zjZSTU'],
      id: '9ddcff91f7a38d8d468feef299c28747',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script></body></html>