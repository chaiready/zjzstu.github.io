<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/./atom.xml" title="做一个幸福的人" type="application/atom+xml"><meta name="google-site-verification" content="Qr_3yqLtyErDFKHW7mE8PJz4qDUX-bf_fMLpSRckQe4"><meta name="baidu-site-verification" content="zOwIvKMV7f"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"搜索文章",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet"><meta name="description" content="参考：Linear Regression and Gradient Descent from scratch in PyTorch"><meta name="keywords" content="python,pytorch,numpy,matplotlib,线性回归"><meta property="og:type" content="article"><meta property="og:title" content="从numpy到pytorch实现线性回归"><meta property="og:url" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;posts&#x2F;ca2079f0.html"><meta property="og:site_name" content="做一个幸福的人"><meta property="og:description" content="参考：Linear Regression and Gradient Descent from scratch in PyTorch"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;从numpy到pytorch实现线性回归&#x2F;numpy_sgd.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;从numpy到pytorch实现线性回归&#x2F;pytorch_batch.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;从numpy到pytorch实现线性回归&#x2F;pytorch_stochastic.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;从numpy到pytorch实现线性回归&#x2F;pytorch_small_batch.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;从numpy到pytorch实现线性回归&#x2F;torch-class.png"><meta property="og:updated_time" content="2020-02-15T05:36:35.871Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;从numpy到pytorch实现线性回归&#x2F;numpy_sgd.png"><link rel="canonical" href="https://www.zhujian.tech/posts/ca2079f0.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>从numpy到pytorch实现线性回归 | 做一个幸福的人</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e677aac1ac69b8826b9cfecb4e72e107";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">做一个幸福的人</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">面朝大海，春暖花开</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">129</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">48</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">169</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header> <a href="https://github.com/zjZSTU" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zhujian.tech/posts/ca2079f0.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="zhujian"><meta itemprop="description" content="one bite at a time"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="做一个幸福的人"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 从numpy到pytorch实现线性回归</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-04-16 20:13:01" itemprop="dateCreated datePublished" datetime="2019-04-16T20:13:01+00:00">2019-04-16</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-15 05:36:35" itemprop="dateModified" datetime="2020-02-15T05:36:35+00:00">2020-02-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/" itemprop="url" rel="index"><span itemprop="name">算法</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">编程</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/programming-language/" itemprop="url" rel="index"><span itemprop="name">编程语言</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/data-learning/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/codebase/" itemprop="url" rel="index"><span itemprop="name">代码库</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>8.1k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>13 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>参考：</p><p><a href="https://medium.com/jovian-io/linear-regression-with-pytorch-3dde91d60b50" target="_blank" rel="noopener">Linear Regression and Gradient Descent from scratch in PyTorch</a></p><a id="more"></a><p><a href="https://www.jiqizhixin.com/articles/2019-03-15-5" target="_blank" rel="noopener">PyTorch进阶之路（二）：如何实现线性回归</a></p><p><a href="https://www.zhujian.tech/posts/ec419bd2.html#more">线性回归</a></p><p><a href="https://www.zhujian.tech/posts/dea583b1.html#more">特征缩放</a></p><p>首先利用<code>numpy</code>实现梯度下降解决多变量线性回归问题，然后逐步将操作转换成<code>pytorch</code></p><p>实现步骤如下：</p><ol><li>加载训练数据</li><li>初始化权重</li><li>计算预测结果</li><li>计算损失函数</li><li>梯度更新</li><li>重复<code>3-5</code>步，直到完成迭代次数</li><li>绘制损失图</li></ol><p>多变量线性回归测试数据参考<a href="https://github.com/peedeep/Coursera/blob/master/ex1/ex1data2.txt" target="_blank" rel="noopener">ex1data2.txt</a></p><h2 id="numpy实现随机梯度下降"><a href="#numpy实现随机梯度下降" class="headerlink" title="numpy实现随机梯度下降"></a><code>numpy</code>实现随机梯度下降</h2><p>参考：<a href="https://www.zhujian.tech/posts/3c50d4b7.html#more">梯度下降</a></p><p><strong>随机梯度下降</strong>实现如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># @Author  : zj</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">梯度下降法计算线性回归问题</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_ex1_multi_data():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    加载多变量数据</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    path = &apos;../data/coursera2.txt&apos;</span><br><span class="line">    datas = []</span><br><span class="line">    with open(path, &apos;r&apos;) as f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">        for line in lines:</span><br><span class="line">            datas.append(line.strip().split(&apos;,&apos;))</span><br><span class="line">    data_arr = np.array(datas)</span><br><span class="line">    data_arr = data_arr.astype(np.float)</span><br><span class="line"></span><br><span class="line">    X = data_arr[:, :2]</span><br><span class="line">    Y = data_arr[:, 2]</span><br><span class="line">    return X, Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def draw_loss(loss_list):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    绘制损失函数值</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    plt.plot(loss_list)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def init_weight(size):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    初始化权重，使用均值为0,方差为1的标准正态分布</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return np.random.normal(loc=0.0, scale=1.0, size=size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_loss(w, x, y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    计算损失值</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    n = y.shape[0]</span><br><span class="line">    return (x.dot(w) - y).T.dot(x.dot(w) - y) / n</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def using_stochastic_gradient_descent():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    随机梯度下降</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x, y = load_ex1_multi_data()</span><br><span class="line">    extend_x = np.insert(x, 0, values=np.ones(x.shape[0]), axis=1)</span><br><span class="line">    w = init_weight(extend_x.shape[1])</span><br><span class="line">    # print(w)</span><br><span class="line">    print(w.shape)</span><br><span class="line"></span><br><span class="line">    # 打乱数据</span><br><span class="line">    np.random.shuffle(extend_x)</span><br><span class="line">    print(extend_x.shape)</span><br><span class="line">    print(y.shape)</span><br><span class="line"></span><br><span class="line">    n = y.shape[0]</span><br><span class="line">    epoches = 10</span><br><span class="line">    alpha = 1e-8</span><br><span class="line">    loss_list = []</span><br><span class="line">    for i in range(epoches):</span><br><span class="line">        for j in range(n):</span><br><span class="line">            temp = w - alpha * (extend_x[j].dot(w) - y[j]) * extend_x[j].T / 2</span><br><span class="line">            w = temp</span><br><span class="line">            loss_list.append(compute_loss(w, extend_x, y))</span><br><span class="line">    draw_loss(loss_list)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    using_stochastic_gradient_descent()</span><br></pre></td></tr></table></figure><p><img src="/imgs/从numpy到pytorch实现线性回归/numpy_sgd.png" alt></p><h2 id="pytorch实现批量梯度下降"><a href="#pytorch实现批量梯度下降" class="headerlink" title="pytorch实现批量梯度下降"></a><code>pytorch</code>实现批量梯度下降</h2><p><code>pytorch</code>使用<code>tensor</code>作为数据保存结构，使用函数<code>from_numpy</code>可以将<code>numpy array</code>数组转换成<code>tensor</code>类型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.from_numpy(X), torch.from_numpy(Y)</span><br></pre></td></tr></table></figure><p>使用<code>torch.randn</code>可以生成符合标准正态分布的随机数组，用于生成权重和偏置值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.randn(h, 1, requires_grad=True, dtype=torch.double), torch.randn(1, requires_grad=True,                                                                                  dtype=torch.double)</span><br></pre></td></tr></table></figure><p><code>pytorch</code>内置了<code>autograd</code>包，计算预测结果和损失函数后，调用函数<code>backward()</code>就能够自动计算出梯度</p><p>首先需要开启权重和偏置值的梯度开关，然后在调用函数后进行梯度更新</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">with torch.no_grad():</span><br><span class="line">      w -= w.grad * lr</span><br><span class="line">      b -= b.grad * lr</span><br><span class="line">      w.grad.zero_()</span><br><span class="line">      b.grad.zero_()</span><br></pre></td></tr></table></figure><p>使用<code>torch.no_grad</code>能够保证梯度更新过程中不再计算梯度值，计算完成后需要将梯度归零，避免下次叠加</p><p>使用<code>pytorch</code>实现<strong>批量梯度下降</strong>计算多变量线性回归问题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># @Author  : zj</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">梯度下降法计算线性回归问题</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_ex1_multi_data():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    加载多变量数据</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    path = &apos;../data/coursera2.txt&apos;</span><br><span class="line">    datas = []</span><br><span class="line">    with open(path, &apos;r&apos;) as f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">        for line in lines:</span><br><span class="line">            datas.append(line.strip().split(&apos;,&apos;))</span><br><span class="line">    data_arr = np.array(datas)</span><br><span class="line">    data_arr = data_arr.astype(np.float)</span><br><span class="line"></span><br><span class="line">    X = data_arr[:, :2]</span><br><span class="line">    Y = data_arr[:, 2]</span><br><span class="line"></span><br><span class="line">    return torch.from_numpy(X), torch.from_numpy(Y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def init_weight(h):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    初始化权重，使用均值为0,方差为1的标准正态分布</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return torch.randn(h, 1, requires_grad=True, dtype=torch.double), torch.randn(1, requires_grad=True,</span><br><span class="line">                                                                                  dtype=torch.double)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def predict_result(w, b, x):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    预测结果</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return x.mm(w) + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_loss(w, b, x, y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    计算损失值 MSE</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    diff = y - predict_result(w, b, x)</span><br><span class="line">    return torch.sum(diff * diff) / diff.numel()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def draw_loss(loss_list):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    绘制损失函数值</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    plt.plot(loss_list)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def using_batch_gradient_descent():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    批量梯度下降</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x, y = load_ex1_multi_data()</span><br><span class="line">    w, b = init_weight(x.shape[1])</span><br><span class="line"></span><br><span class="line">    epoches = 20</span><br><span class="line">    lr = 1e-7</span><br><span class="line">    loss_list = []</span><br><span class="line">    for i in range(epoches):</span><br><span class="line">        # 计算损失值</span><br><span class="line">        loss = compute_loss(w, b, x, y)</span><br><span class="line">        # 保存损失值</span><br><span class="line">        loss_list.append(loss)</span><br><span class="line">        # 反向更新</span><br><span class="line">        loss.backward()</span><br><span class="line">        # 梯度更新</span><br><span class="line">        with torch.no_grad():</span><br><span class="line">            w -= w.grad * lr</span><br><span class="line">            b -= b.grad * lr</span><br><span class="line">            w.grad.zero_()</span><br><span class="line">            b.grad.zero_()</span><br><span class="line">    draw_loss(loss_list)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    using_batch_gradient_descent()</span><br></pre></td></tr></table></figure><p><img src="/imgs/从numpy到pytorch实现线性回归/pytorch_batch.png" alt></p><h2 id="pytorch实现随机梯度下降"><a href="#pytorch实现随机梯度下降" class="headerlink" title="pytorch实现随机梯度下降"></a><code>pytorch</code>实现随机梯度下降</h2><p><code>pytorch</code>提供了许多类和函数用于计算，下面实现<strong>随机梯度下降</strong>解决多变量线性回归</p><p>首先在<code>numpy</code>数组转换成<code>pytorch tensor</code>类型前先打乱数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 打乱数据</span><br><span class="line">indexs = np.arange(X.shape[0])</span><br><span class="line">np.random.shuffle(indexs)</span><br><span class="line">X = X[indexs]</span><br><span class="line">Y = Y[indexs]</span><br></pre></td></tr></table></figure><p><code>pytorch.nn</code>包提供了类<code>Linear</code>用于线性计算</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 定义线性模型</span><br><span class="line">model = nn.Linear(x.size()[1], 1)</span><br><span class="line"># 获取初始权重和偏置值</span><br><span class="line">w = model.weight</span><br><span class="line">b = model.bias</span><br><span class="line"># 计算预测结果，计算损失值</span><br><span class="line">diff = y - model(x)</span><br></pre></td></tr></table></figure><p><code>pytorch.nn.function</code>包提供了函数<code>mse_loss</code>用于计算均方误差</p><p>也可以使用包装类<code>nn.MSELoss</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># 损失函数</span><br><span class="line">loss_fn = F.mse_loss</span><br><span class="line"># 计算损失值</span><br><span class="line">loss = loss_fn(model(x), y)</span><br><span class="line"># 或者</span><br><span class="line"># 损失函数</span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line"># 计算损失值</span><br><span class="line">loss = criterion(model(x), y)</span><br></pre></td></tr></table></figure><p><code>pytorch.optim</code>提供了类<code>SGD</code>用于计算随机梯度下降</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 定义优化器</span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=2e-7, momentum=0.9)</span><br><span class="line"># 清空梯度</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line"># 计算梯度</span><br><span class="line">loss.backward()</span><br><span class="line"># 更新</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><p>实现如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># @Author  : zj</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">梯度下降法计算线性回归问题</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">import torch.optim as optim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_ex1_multi_data():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    加载多变量数据</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    path = &apos;../data/coursera2.txt&apos;</span><br><span class="line">    datas = []</span><br><span class="line">    with open(path, &apos;r&apos;) as f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">        for line in lines:</span><br><span class="line">            datas.append(line.strip().split(&apos;,&apos;))</span><br><span class="line">    data_arr = np.array(datas)</span><br><span class="line">    data_arr = data_arr.astype(np.float)</span><br><span class="line"></span><br><span class="line">    X = data_arr[:, :2]</span><br><span class="line">    Y = data_arr[:, 2]</span><br><span class="line">    </span><br><span class="line">    # 打乱数据</span><br><span class="line">    indexs = np.arange(X.shape[0])</span><br><span class="line">    np.random.shuffle(indexs)</span><br><span class="line">    X = X[indexs]</span><br><span class="line">    Y = Y[indexs]</span><br><span class="line"></span><br><span class="line">    return torch.from_numpy(X).float(), torch.from_numpy(Y).float()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def draw_loss(loss_list):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    绘制损失函数值</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    plt.plot(loss_list)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def using_stochastic_gradient_descent():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    随机梯度下降</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x, y = load_ex1_multi_data()</span><br><span class="line"></span><br><span class="line">    # 定义线性模型</span><br><span class="line">    model = nn.Linear(x.size()[1], 1)</span><br><span class="line">    # 获取初始权重和偏置值</span><br><span class="line">    w = model.weight</span><br><span class="line">    b = model.bias</span><br><span class="line"></span><br><span class="line">    # 损失函数</span><br><span class="line">    criterion = nn.MSELoss()</span><br><span class="line">    # 定义优化器</span><br><span class="line">    optimizer = optim.SGD(model.parameters(), lr=1e-10, momentum=0.9)</span><br><span class="line"></span><br><span class="line">    epoches = 10</span><br><span class="line">    loss_list = []</span><br><span class="line">    for i in range(epoches):</span><br><span class="line">        for j, item in enumerate(x, 0):</span><br><span class="line">            # 计算损失值</span><br><span class="line">            loss = criterion(model(item), y[j])</span><br><span class="line">            # 清空梯度</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            # 计算梯度</span><br><span class="line">            loss.backward()</span><br><span class="line">            # 更新</span><br><span class="line">            optimizer.step()</span><br><span class="line">            # 保存损失值</span><br><span class="line">            loss_list.append(loss)</span><br><span class="line">    draw_loss(loss_list)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    using_stochastic_gradient_descent()</span><br></pre></td></tr></table></figure><p><img src="/imgs/从numpy到pytorch实现线性回归/pytorch_stochastic.png" alt></p><h2 id="pytorch实现小批量梯度下降"><a href="#pytorch实现小批量梯度下降" class="headerlink" title="pytorch实现小批量梯度下降"></a><code>pytorch</code>实现小批量梯度下降</h2><p>实际训练过程中最常使用的梯度下降方法是小批量梯度下降，</p><p><code>pytorch</code>提供了类<code>torch.utils.data.TensorDataset</code>以及<code>torch.utils.data.DataLoader</code>来实现数据的加载、打乱和批量化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">batch_size = 8</span><br><span class="line">data_ts = TensorDataset(x, y)</span><br><span class="line">data_loader = DataLoader(data_ts, batch_size=batch_size, shuffle=True)</span><br><span class="line">for j, item in enumerate(data_loader, 0):</span><br><span class="line">    inputs, targets = item</span><br><span class="line">    # 计算损失值</span><br><span class="line">    loss = criterion(model(inputs), targets)</span><br></pre></td></tr></table></figure><p>实现如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># @Author  : zj</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">梯度下降法计算线性回归问题</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">import torch.optim as optim</span><br><span class="line">from torch.utils.data import TensorDataset</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_ex1_multi_data():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    加载多变量数据</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    path = &apos;../data/coursera2.txt&apos;</span><br><span class="line">    datas = []</span><br><span class="line">    with open(path, &apos;r&apos;) as f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">        for line in lines:</span><br><span class="line">            datas.append(line.strip().split(&apos;,&apos;))</span><br><span class="line">    data_arr = np.array(datas)</span><br><span class="line">    data_arr = data_arr.astype(np.float)</span><br><span class="line"></span><br><span class="line">    X = data_arr[:, :2]</span><br><span class="line">    Y = data_arr[:, 2]</span><br><span class="line"></span><br><span class="line">    return torch.from_numpy(X).float(), torch.from_numpy(Y).float()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def draw_loss(loss_list):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    绘制损失函数值</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    plt.plot(loss_list)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def using_small_batch_gradient_descent():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    小批量梯度下降</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x, y = load_ex1_multi_data()</span><br><span class="line"></span><br><span class="line">    batch_size = 8</span><br><span class="line">    data_ts = TensorDataset(x, y)</span><br><span class="line">    data_loader = DataLoader(data_ts, batch_size=batch_size, shuffle=True)</span><br><span class="line"></span><br><span class="line">    # 定义线性模型</span><br><span class="line">    model = nn.Linear(x.size()[1], 1)</span><br><span class="line">    # 获取初始权重和偏置值</span><br><span class="line">    w = model.weight</span><br><span class="line">    b = model.bias</span><br><span class="line"></span><br><span class="line">    # 损失函数</span><br><span class="line">    criterion = nn.MSELoss()</span><br><span class="line">    # 定义优化器</span><br><span class="line">    optimizer = optim.SGD(model.parameters(), lr=1e-10, momentum=0.9)</span><br><span class="line"></span><br><span class="line">    epoches = 200</span><br><span class="line">    loss_list = []</span><br><span class="line">    for i in range(epoches):</span><br><span class="line">        for j, item in enumerate(data_loader, 0):</span><br><span class="line">            # print(item)</span><br><span class="line">            inputs, targets = item</span><br><span class="line">            # 计算损失值</span><br><span class="line">            loss = criterion(model(inputs), targets)</span><br><span class="line">            # 清空梯度</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            # 计算梯度</span><br><span class="line">            loss.backward()</span><br><span class="line">            # 更新</span><br><span class="line">            optimizer.step()</span><br><span class="line">            # 保存损失值</span><br><span class="line">            loss_list.append(loss)</span><br><span class="line">    draw_loss(loss_list)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    using_small_batch_gradient_descent()</span><br></pre></td></tr></table></figure><p><img src="/imgs/从numpy到pytorch实现线性回归/pytorch_small_batch.png" alt></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><code>pytorch</code>使用到的类库如下所示</p><p><img src="/imgs/从numpy到pytorch实现线性回归/torch-class.png" alt></p></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.jpg" alt="zhujian 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="zhujian 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zhujian</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zhujian.tech/posts/ca2079f0.html" title="从numpy到pytorch实现线性回归">https://www.zhujian.tech/posts/ca2079f0.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/python/" rel="tag"># python</a> <a href="/tags/pytorch/" rel="tag"># pytorch</a> <a href="/tags/numpy/" rel="tag"># numpy</a> <a href="/tags/matplotlib/" rel="tag"># matplotlib</a> <a href="/tags/linear-regression/" rel="tag"># 线性回归</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/3c50d4b7.html" rel="next" title="梯度下降"><i class="fa fa-chevron-left"></i> 梯度下降</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/posts/9f2d3388.html" rel="prev" title="逻辑回归">逻辑回归<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#numpy实现随机梯度下降"><span class="nav-number">1.</span> <span class="nav-text">numpy实现随机梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pytorch实现批量梯度下降"><span class="nav-number">2.</span> <span class="nav-text">pytorch实现批量梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pytorch实现随机梯度下降"><span class="nav-number">3.</span> <span class="nav-text">pytorch实现随机梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pytorch实现小批量梯度下降"><span class="nav-number">4.</span> <span class="nav-text">pytorch实现小批量梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#小结"><span class="nav-number">5.</span> <span class="nav-text">小结</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zhujian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">zhujian</p><div class="site-description" itemprop="description">one bite at a time</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">169</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">129</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/./atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zjZSTU" title="Github &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;zjZSTU" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/u012005313" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;u012005313" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i> CSDN</a></span><span class="links-of-author-item"><a href="/mailto:zjzstu@gmail.com" title="Gmail &amp;rarr; mailto:zjzstu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> Gmail</a></span><span class="links-of-author-item"><a href="/mailto:505169307@gmail.com" title="QQmail &amp;rarr; mailto:505169307@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> QQmail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://cloud.tencent.com/" title="https:&#x2F;&#x2F;cloud.tencent.com" rel="noopener" target="_blank">腾讯云</a></li><li class="links-of-blogroll-item"> <a href="https://www.aliyun.com/" title="https:&#x2F;&#x2F;www.aliyun.com" rel="noopener" target="_blank">阿里云</a></li><li class="links-of-blogroll-item"> <a href="https://developer.android.com/" title="https:&#x2F;&#x2F;developer.android.com&#x2F;" rel="noopener" target="_blank">Android Dev</a></li><li class="links-of-blogroll-item"> <a href="https://jenkins.io/" title="https:&#x2F;&#x2F;jenkins.io&#x2F;" rel="noopener" target="_blank">Jenkins</a></li><li class="links-of-blogroll-item"> <a href="http://cs231n.github.io/" title="http:&#x2F;&#x2F;cs231n.github.io&#x2F;" rel="noopener" target="_blank">cs231n</a></li><li class="links-of-blogroll-item"> <a href="https://pytorch.org/docs/stable/index.html" title="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;index.html" rel="noopener" target="_blank">pytorch</a></li><li class="links-of-blogroll-item"> <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" title="https:&#x2F;&#x2F;docs.docker.com&#x2F;install&#x2F;linux&#x2F;docker-ce&#x2F;ubuntu&#x2F;" rel="noopener" target="_blank">docker</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">zhujian</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">948k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">26:20</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-bar-chart-o"></i></span> <a href="https://tongji.baidu.com/web/27249108/overview/index?siteId=13647183" rel="noopener" target="_blank">百度统计</a></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div><div class="beian"> <a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备 19026415号</a> <span class="post-meta-divider">|</span> <img src="/images/beian_icon.png" style="display:inline-block;text-decoration:none;height:13px"> <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001748" rel="noopener" target="_blank">浙公网安备 33011802001748号</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span> <span class="site-uv" title="总访客量">访客数：<span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="site-pv" title="总访问量">阅读量：<span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="17,63,61" opacity="1" zindex="-1" count="199" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '8f22595c6dd29c94467d',
      clientSecret: 'e66bd90ebb9aa66c562c753dec6c90ceecbd51f2',
      repo: 'guestbook',
      owner: 'zjZSTU',
      admin: ['zjZSTU'],
      id: '5437328f62a0500fbcff9740a7f0b816',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script></body></html>