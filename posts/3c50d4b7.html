<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/./atom.xml" title="做一个幸福的人" type="application/atom+xml"><meta name="google-site-verification" content="Qr_3yqLtyErDFKHW7mE8PJz4qDUX-bf_fMLpSRckQe4"><meta name="baidu-site-verification" content="zOwIvKMV7f"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"搜索文章",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet"><meta name="description" content="参考：梯度下降梯度下降是求解函数最小值的算法，也称为最速下降法，它通过梯度更新不断的逼近最优解常用的比喻是下山问题，通过计算梯度能够找到函数值变化最快的地方，通过步长决定收敛的速度"><meta name="keywords" content="python,numpy,matplotlib,梯度下降"><meta property="og:type" content="article"><meta property="og:title" content="梯度下降"><meta property="og:url" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;posts&#x2F;3c50d4b7.html"><meta property="og:site_name" content="做一个幸福的人"><meta property="og:description" content="参考：梯度下降梯度下降是求解函数最小值的算法，也称为最速下降法，它通过梯度更新不断的逼近最优解常用的比喻是下山问题，通过计算梯度能够找到函数值变化最快的地方，通过步长决定收敛的速度"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;梯度下降&#x2F;batch.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;梯度下降&#x2F;stochastic.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;梯度下降&#x2F;small_batch.png"><meta property="og:updated_time" content="2020-02-15T05:36:35.875Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;梯度下降&#x2F;batch.png"><link rel="canonical" href="https://www.zhujian.tech/posts/3c50d4b7.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>梯度下降 | 做一个幸福的人</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e677aac1ac69b8826b9cfecb4e72e107";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">做一个幸福的人</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">面朝大海，春暖花开</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">129</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">48</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">169</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header> <a href="https://github.com/zjZSTU" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zhujian.tech/posts/3c50d4b7.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="zhujian"><meta itemprop="description" content="one bite at a time"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="做一个幸福的人"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 梯度下降</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-04-16 14:46:59" itemprop="dateCreated datePublished" datetime="2019-04-16T14:46:59+00:00">2019-04-16</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-15 05:36:35" itemprop="dateModified" datetime="2020-02-15T05:36:35+00:00">2020-02-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/" itemprop="url" rel="index"><span itemprop="name">算法</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">编程</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/optimization/" itemprop="url" rel="index"><span itemprop="name">最优化</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/programming-language/" itemprop="url" rel="index"><span itemprop="name">编程语言</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>5.3k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>9 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>参考：<a href="https://www.zhihu.com/topic/19650497/hot" target="_blank" rel="noopener">梯度下降</a></p><p>梯度下降是求解函数最小值的算法，也称为最速下降法，它通过梯度更新不断的逼近最优解</p><p>常用的比喻是下山问题，通过计算梯度能够找到函数值变化最快的地方，通过步长决定收敛的速度</p><a id="more"></a><p>梯度下降方法包括批量梯度下降、随机梯度下降和小批量梯度下降，下面通过梯度下降计算多变量线性回归问题</p><h2 id="多变量线性回归"><a href="#多变量线性回归" class="headerlink" title="多变量线性回归"></a>多变量线性回归</h2><p>参考<a href="https://www.zhujian.tech/posts/ec419bd2.html#more">线性回归</a></p><script type="math/tex;mode=display">
Y=X\cdot W</script><p>其中</p><script type="math/tex;mode=display">
Y_{m\times 1}=\begin{bmatrix}
y_{1}\\ 
y_{2}\\ 
...\\ 
y_{m}
\end{bmatrix}</script><script type="math/tex;mode=display">
X_{m\times (n+1)}=\begin{bmatrix}
x_{10} & x_{11} & ... & x_{1n}\\ 
x_{20} & x_{21} & ... & x_{2n}\\ 
... & ... & ... & ...\\ 
x_{m0} & x_{m1} & ... & x_{mn}
\end{bmatrix}
=\begin{bmatrix}
1 & x_{11} & ... & x_{1n}\\ 
1 & x_{21} & ... & x_{2n}\\ 
... & ... & ... & ...\\ 
1 & x_{m1} & ... & x_{mn}
\end{bmatrix}</script><script type="math/tex;mode=display">
W_{(n+1)\times 1}=
\begin{bmatrix}
w_{0} \\ w_{1} \\ ... \\ w_{n}
\end{bmatrix}</script><h2 id="多变量测试数据"><a href="#多变量测试数据" class="headerlink" title="多变量测试数据"></a>多变量测试数据</h2><p>使用<code>UCI</code>提供的计算机硬件数据集<a href="http://archive.ics.uci.edu/ml/datasets/Computer+Hardware" target="_blank" rel="noopener">machine-data</a>，其包含<code>10</code>类数据</p><ol><li>vendor name: 厂商名</li><li>Model Name: 型号名</li><li>MYCT: 机器周期(/ns)</li><li>MMIN: 主存最小值(/KB)</li><li>MMAX: 主存最大值(/KB)</li><li>CACH: 缓存大小(/KB)</li><li>CHMIN: 通道最小值</li><li>CHMAX: 通道最大值</li><li>PRP: 相对性能</li><li>ERP: cpu相对性能</li></ol><p>使用第<code>3-8</code>项作为训练数据，使用第<code>9</code>项作为真实数据进行训练</p><h2 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h2><p>批量梯度下降(<code>batch gradient descent</code>)每次迭代将所有样本数据都加入计算，其损失函数<code>MSE</code>如下：</p><script type="math/tex;mode=display">
J(\theta) = \frac{1}{N}\cdot \sum_{j=1}^{N}(h(x_{j};\theta)-y_{j})^{2}</script><script type="math/tex;mode=display">
\Rightarrow J(\theta) = \frac{1}{N} (X\cdot W-Y)^T\cdot(X\cdot W -Y)</script><p>批量求导如下</p><script type="math/tex;mode=display">
\frac{\varphi }{\varphi w_{i}}J(\theta) = \frac{1}{N} \sum_{j=1}^{N}(h(x_{j};\theta)-y_{j})\cdot x_{j,i})</script><script type="math/tex;mode=display">
\Rightarrow \frac{\varphi }{\varphi w_{i}}J(\theta) = \frac{1}{N} X[i]^T\cdot (X\cdot W - Y)</script><script type="math/tex;mode=display">
\Rightarrow \frac{\varphi }{\varphi w}J(\theta) = \frac{1}{N} X^T\cdot (X\cdot W - Y)</script><p>其中$x_{j,i}$表示第$j$行第$i$列，$X[i]$表示第$i$列</p><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><p>随机梯度下降(<code>stochastic gradient descent</code>)每次更新仅使用一条数据，其损失函数<code>MSE</code>如下：</p><script type="math/tex;mode=display">
J(\theta,j) = (h(x_{j};\theta)-y_{j})^{2}</script><p>批量求导如下</p><script type="math/tex;mode=display">
\frac{\varphi }{\varphi w_{i}}J(\theta) = \frac{1}{2} (h(x_{j};\theta)-y_{j})\cdot x_{j,i}</script><script type="math/tex;mode=display">
\Rightarrow \frac{\varphi }{\varphi w}J(\theta) = \frac{1}{2}(h(x_{j};\theta)-y_{j})\cdot X[j]^T</script><p>其中$x_{j,i}$表示第$j$行第$i$列，$X[j]$表示第$j$行</p><p><strong>在训练之前应该打乱训练集数据，以避免数据顺序对算法结果造成影响</strong></p><h2 id="小批量随机梯度下降"><a href="#小批量随机梯度下降" class="headerlink" title="小批量随机梯度下降"></a>小批量随机梯度下降</h2><p>小批量梯度下降(<code>small batch gradient descent</code>)介于批量和随机梯度下降之间，每次更新使用给定数量的训练数据来更新参数，其损失函数<code>MSE</code>如下：</p><script type="math/tex;mode=display">
J(\theta,i:j) = \frac{1}{j-i} \sum_{k=1}^{j-i}(h(x_{k};\theta)-y_{k})^{2}</script><p>小批量求导如下</p><script type="math/tex;mode=display">
\frac{\varphi }{\varphi w_{l}}J(\theta) = \frac{1}{j-i}\sum_{k=1}^{j-i} (h(x_{k};\theta)-y_{k})\cdot x_{k,l}</script><script type="math/tex;mode=display">
\Rightarrow \frac{\varphi }{\varphi w}J(\theta) = \frac{1}{j-i}X[i:j]^T\cdot (X[i:j]\cdot W-Y[i:j])</script><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>使用<code>numpy</code>实现了批量梯度下降和随机梯度下降方法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># @Time    : 19-4-16 下午3:38</span><br><span class="line"># @Author  : zj</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">梯度下降法计算线性回归问题</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_ex1_multi_data():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    加载多变量数据</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    path = &apos;../data/coursera2.txt&apos;</span><br><span class="line">    datas = []</span><br><span class="line">    with open(path, &apos;r&apos;) as f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">        for line in lines:</span><br><span class="line">            datas.append(line.strip().split(&apos;,&apos;))</span><br><span class="line">    data_arr = np.array(datas)</span><br><span class="line">    data_arr = data_arr.astype(np.float)</span><br><span class="line"></span><br><span class="line">    X = data_arr[:, :2]</span><br><span class="line">    Y = data_arr[:, 2]</span><br><span class="line">    return X, Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_machine_data():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    加载计算机硬件数据</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    data = np.loadtxt(&apos;../data/machine.data&apos;, delimiter=&apos;,&apos;, dtype=np.str)</span><br><span class="line">    # print(data)</span><br><span class="line"></span><br><span class="line">    x = data[:, 2:8].astype(np.float)</span><br><span class="line">    y = data[:, 8].astype(np.float)</span><br><span class="line">    # print(x)</span><br><span class="line">    # print(y)</span><br><span class="line"></span><br><span class="line">    return x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def draw_loss(loss_list):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    绘制损失函数值</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    plt.plot(loss_list)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def init_weight(size):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    初始化权重，使用均值为0,方差为1的标准正态分布</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return np.random.normal(loc=0.0, scale=1.0, size=size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_loss(w, x, y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    计算损失值</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    n = y.shape[0]</span><br><span class="line">    return (x.dot(w) - y).T.dot(x.dot(w) - y) / n</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def using_batch_gradient_descent():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    批量梯度下降</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x, y = load_machine_data()</span><br><span class="line">    extend_x = np.insert(x, 0, values=np.ones(x.shape[0]), axis=1)</span><br><span class="line">    w = init_weight(extend_x.shape[1])</span><br><span class="line">    # print(w)</span><br><span class="line"></span><br><span class="line">    n = y.shape[0]</span><br><span class="line">    epoches = 50</span><br><span class="line">    alpha = 1e-9</span><br><span class="line">    loss_list = []</span><br><span class="line">    for i in range(epoches):</span><br><span class="line">        temp = w - alpha * extend_x.T.dot(extend_x.dot(w) - y) / n</span><br><span class="line">        w = temp</span><br><span class="line">        loss_list.append(compute_loss(w, extend_x, y))</span><br><span class="line">    draw_loss(loss_list)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def using_stochastic_gradient_descent():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    随机梯度下降</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x, y = load_machine_data()</span><br><span class="line">    extend_x = np.insert(x, 0, values=np.ones(x.shape[0]), axis=1)</span><br><span class="line">    w = init_weight(extend_x.shape[1])</span><br><span class="line">    # print(w)</span><br><span class="line">    print(w.shape)</span><br><span class="line"></span><br><span class="line">    # 打乱数据</span><br><span class="line">    np.random.shuffle(extend_x)</span><br><span class="line">    print(extend_x.shape)</span><br><span class="line">    print(y.shape)</span><br><span class="line"></span><br><span class="line">    n = y.shape[0]</span><br><span class="line">    epoches = 20</span><br><span class="line">    alpha = 1e-9</span><br><span class="line">    loss_list = []</span><br><span class="line">    for i in range(epoches):</span><br><span class="line">        for j in range(n):</span><br><span class="line">            temp = w - alpha * (extend_x[j].dot(w) - y[j]) * extend_x[j].T / 2</span><br><span class="line">            w = temp</span><br><span class="line">            loss_list.append(compute_loss(w, extend_x, y))</span><br><span class="line">    draw_loss(loss_list)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def using_small_batch_gradient_descent():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    小批量梯度下降</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x, y = load_machine_data()</span><br><span class="line">    extend_x = np.insert(x, 0, values=np.ones(x.shape[0]), axis=1)</span><br><span class="line">    w = init_weight(extend_x.shape[1])</span><br><span class="line">    # print(w)</span><br><span class="line">    print(w.shape)</span><br><span class="line"></span><br><span class="line">    # 打乱数据</span><br><span class="line">    np.random.shuffle(extend_x)</span><br><span class="line">    print(extend_x.shape)</span><br><span class="line">    print(y.shape)</span><br><span class="line"></span><br><span class="line">    # 批量大小</span><br><span class="line">    batch_size = 16</span><br><span class="line"></span><br><span class="line">    n = y.shape[0]</span><br><span class="line">    epoches = 20</span><br><span class="line">    alpha = 5e-9</span><br><span class="line">    loss_list = []</span><br><span class="line">    for i in range(epoches):</span><br><span class="line">        for j in list(range(0, n, batch_size)):</span><br><span class="line">            temp = w - alpha * extend_x[j:j + batch_size].T.dot(</span><br><span class="line">                extend_x[j:j + batch_size].dot(w) - y[j:j + batch_size]) / batch_size</span><br><span class="line">            w = temp</span><br><span class="line">            loss_list.append(compute_loss(w, extend_x, y))</span><br><span class="line">    draw_loss(loss_list)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    # using_batch_gradient_descent()</span><br><span class="line">    using_stochastic_gradient_descent()</span><br><span class="line">    # using_small_batch_gradient_descent()</span><br></pre></td></tr></table></figure><p>批量梯度下降损失图</p><p><img src="/imgs/梯度下降/batch.png" alt></p><p>随机梯度下降损失图</p><p><img src="/imgs/梯度下降/stochastic.png" alt></p><p>小批量随机梯度下降损失图</p><p><img src="/imgs/梯度下降/small_batch.png" alt></p><h2 id="3种梯度下降比较"><a href="#3种梯度下降比较" class="headerlink" title="3种梯度下降比较"></a>3种梯度下降比较</h2><p>参考：</p><p><a href="https://www.cnblogs.com/lliuye/p/9451903.html" target="_blank" rel="noopener">批量梯度下降(BGD)、随机梯度下降(SGD)以及小批量梯度下降(MBGD)的理解</a></p><p><a href="https://blog.csdn.net/sofuzi/article/details/80882521" target="_blank" rel="noopener">三种梯度下降的方式：批量梯度下降、小批量梯度下降、随机梯度下降</a></p><ul><li>批量梯度下降<ul><li>优点：每次更新需要计算所有样本，从而得到的梯度更具有代表性，所以损失值收敛速度最快</li><li>缺点：由于每次梯度更新都需要计算所有样本，对于大样本数据训练需要更多的训练时间和训练资源</li></ul></li><li>随机梯度下降<ul><li>优点：每次更新仅需单个样本数据参与，权重更新速度快</li><li>缺点：计算的梯度不一定符合整体最优路径，需要更多的迭代才能完成收敛</li></ul></li><li>小批量梯度下降：结合批量梯度下降和随机梯度下降的优点，小批量样本计算得到的梯度更接近全局样本，同时提高梯度计算和权重更新速度</li></ul></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.jpg" alt="zhujian 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="zhujian 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zhujian</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zhujian.tech/posts/3c50d4b7.html" title="梯度下降">https://www.zhujian.tech/posts/3c50d4b7.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/python/" rel="tag"># python</a> <a href="/tags/numpy/" rel="tag"># numpy</a> <a href="/tags/matplotlib/" rel="tag"># matplotlib</a> <a href="/tags/gradient-descent/" rel="tag"># 梯度下降</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/5213c80b.html" rel="next" title="标签页测试"><i class="fa fa-chevron-left"></i> 标签页测试</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/posts/ca2079f0.html" rel="prev" title="从numpy到pytorch实现线性回归">从numpy到pytorch实现线性回归<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#多变量线性回归"><span class="nav-number">1.</span> <span class="nav-text">多变量线性回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#多变量测试数据"><span class="nav-number">2.</span> <span class="nav-text">多变量测试数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#批量梯度下降"><span class="nav-number">3.</span> <span class="nav-text">批量梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#随机梯度下降"><span class="nav-number">4.</span> <span class="nav-text">随机梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#小批量随机梯度下降"><span class="nav-number">5.</span> <span class="nav-text">小批量随机梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#示例"><span class="nav-number">6.</span> <span class="nav-text">示例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3种梯度下降比较"><span class="nav-number">7.</span> <span class="nav-text">3种梯度下降比较</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zhujian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">zhujian</p><div class="site-description" itemprop="description">one bite at a time</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">169</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">129</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/./atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zjZSTU" title="Github &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;zjZSTU" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/u012005313" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;u012005313" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i> CSDN</a></span><span class="links-of-author-item"><a href="/mailto:zjzstu@gmail.com" title="Gmail &amp;rarr; mailto:zjzstu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> Gmail</a></span><span class="links-of-author-item"><a href="/mailto:505169307@gmail.com" title="QQmail &amp;rarr; mailto:505169307@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> QQmail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://cloud.tencent.com/" title="https:&#x2F;&#x2F;cloud.tencent.com" rel="noopener" target="_blank">腾讯云</a></li><li class="links-of-blogroll-item"> <a href="https://www.aliyun.com/" title="https:&#x2F;&#x2F;www.aliyun.com" rel="noopener" target="_blank">阿里云</a></li><li class="links-of-blogroll-item"> <a href="https://developer.android.com/" title="https:&#x2F;&#x2F;developer.android.com&#x2F;" rel="noopener" target="_blank">Android Dev</a></li><li class="links-of-blogroll-item"> <a href="https://jenkins.io/" title="https:&#x2F;&#x2F;jenkins.io&#x2F;" rel="noopener" target="_blank">Jenkins</a></li><li class="links-of-blogroll-item"> <a href="http://cs231n.github.io/" title="http:&#x2F;&#x2F;cs231n.github.io&#x2F;" rel="noopener" target="_blank">cs231n</a></li><li class="links-of-blogroll-item"> <a href="https://pytorch.org/docs/stable/index.html" title="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;index.html" rel="noopener" target="_blank">pytorch</a></li><li class="links-of-blogroll-item"> <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" title="https:&#x2F;&#x2F;docs.docker.com&#x2F;install&#x2F;linux&#x2F;docker-ce&#x2F;ubuntu&#x2F;" rel="noopener" target="_blank">docker</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">zhujian</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">948k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">26:20</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-bar-chart-o"></i></span> <a href="https://tongji.baidu.com/web/27249108/overview/index?siteId=13647183" rel="noopener" target="_blank">百度统计</a></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div><div class="beian"> <a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备 19026415号</a> <span class="post-meta-divider">|</span> <img src="/images/beian_icon.png" style="display:inline-block;text-decoration:none;height:13px"> <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001748" rel="noopener" target="_blank">浙公网安备 33011802001748号</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span> <span class="site-uv" title="总访客量">访客数：<span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="site-pv" title="总访问量">阅读量：<span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="17,63,61" opacity="1" zindex="-1" count="199" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '8f22595c6dd29c94467d',
      clientSecret: 'e66bd90ebb9aa66c562c753dec6c90ceecbd51f2',
      repo: 'guestbook',
      owner: 'zjZSTU',
      admin: ['zjZSTU'],
      id: '20baee043f723ac5559cf9deb0286360',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script></body></html>