<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/./atom.xml" title="做一个幸福的人" type="application/atom+xml"><meta name="google-site-verification" content="Qr_3yqLtyErDFKHW7mE8PJz4qDUX-bf_fMLpSRckQe4"><meta name="baidu-site-verification" content="zOwIvKMV7f"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"搜索文章",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet"><meta name="description" content="参考：Improving neural networks by preventing co-adaptation of feature detectors"><meta name="keywords" content="python,pytorch,numpy,概率论,随机失活"><meta property="og:type" content="article"><meta property="og:title" content="随机失活"><meta property="og:url" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;posts&#x2F;20cc7a49.html"><meta property="og:site_name" content="做一个幸福的人"><meta property="og:description" content="参考：Improving neural networks by preventing co-adaptation of feature detectors"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;随机失活&#x2F;dropout_net.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;随机失活&#x2F;dropout_net_2.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;随机失活&#x2F;dropout_net_3.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;随机失活&#x2F;dropout_loss.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;随机失活&#x2F;dropout_accuracy.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;随机失活&#x2F;nn_loss.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;随机失活&#x2F;nn_accuracy.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;随机失活&#x2F;dropout_nn_loss.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;随机失活&#x2F;dropout_nn_accuracy.png"><meta property="og:updated_time" content="2020-02-15T05:36:35.879Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;随机失活&#x2F;dropout_net.png"><link rel="canonical" href="https://www.zhujian.tech/posts/20cc7a49.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>随机失活 | 做一个幸福的人</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e677aac1ac69b8826b9cfecb4e72e107";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">做一个幸福的人</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">面朝大海，春暖花开</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">129</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">48</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">169</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header> <a href="https://github.com/zjZSTU" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zhujian.tech/posts/20cc7a49.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="zhujian"><meta itemprop="description" content="one bite at a time"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="做一个幸福的人"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 随机失活</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-06-05 10:06:48" itemprop="dateCreated datePublished" datetime="2019-06-05T10:06:48+00:00">2019-06-05</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-15 05:36:35" itemprop="dateModified" datetime="2020-02-15T05:36:35+00:00">2020-02-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/" itemprop="url" rel="index"><span itemprop="name">算法</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">编程</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/optimization/" itemprop="url" rel="index"><span itemprop="name">最优化</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/math/" itemprop="url" rel="index"><span itemprop="name">数学</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/programming-language/" itemprop="url" rel="index"><span itemprop="name">编程语言</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/codebase/" itemprop="url" rel="index"><span itemprop="name">代码库</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>486</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>1 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>参考：</p><p><a href="https://arxiv.org/abs/1207.0580" target="_blank" rel="noopener">Improving neural networks by preventing co-adaptation of feature detectors</a></p><a id="more"></a><p><a href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf" target="_blank" rel="noopener">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a></p><p><a href="http://cs231n.github.io/neural-networks-2/#reg" target="_blank" rel="noopener">Dropout</a></p><p>使用前馈神经网络进行检测，测试集的检测率总是低于训练集，尤其是训练集数量不大的情况下，原因在于神经网络在训练过程中不断调整参数以拟合训练数据，在此过程中也学习了训练集噪声，导致泛化能力减弱</p><p>随时失活（dropout）是一种正则化方法，其动机来自于进化中的性别作用理论（<code>a theory of the role of sex in evolution</code>），它通过训练多个不同网络模型，模拟模型组合的方式来提高网络性能，防止网络过拟合</p><p>主要内容如下：</p><ol><li>基础知识 - 伯努利分布/均匀分布</li><li>实现原理</li><li>模型描述及改进</li><li>3层神经网络测试</li></ol><h2 id="伯努利分布-均匀分布"><a href="#伯努利分布-均匀分布" class="headerlink" title="伯努利分布/均匀分布"></a>伯努利分布/均匀分布</h2><h3 id="伯努利分布"><a href="#伯努利分布" class="headerlink" title="伯努利分布"></a>伯努利分布</h3><p>参考：<a href="https://baike.baidu.com/item/%E4%BC%AF%E5%8A%AA%E5%88%A9%E5%88%86%E5%B8%83/7167021?fr=aladdin" target="_blank" rel="noopener">伯努利分布</a></p><p>伯努利分布（<code>Bernoulli distribution</code>）是离散随机变量分布，如果随机变量$X$只取0和1两个值，并且相应的概率为</p><script type="math/tex;mode=display">
P(X=1) = p, P(X=0) = 1-p, 0<p<1</script><p>则称随机变量$X$服从参数为$p$的伯努利分布</p><h3 id="均匀分布"><a href="#均匀分布" class="headerlink" title="均匀分布"></a>均匀分布</h3><p>参考：<a href="https://baike.baidu.com/item/%E5%9D%87%E5%8C%80%E5%88%86%E5%B8%83/954451?fr=aladdin" target="_blank" rel="noopener">均匀分布</a></p><p>均匀分布（<code>Uniform distribution</code>）也称为矩形分布，它是对称分布，在相同长度间隔的分布概率是等可能的。均匀分布由两个参数$a$和$b$定义，分别表示数轴上的最小值和最大值，缩写为$U(a,b)$</p><h3 id="numpy实现"><a href="#numpy实现" class="headerlink" title="numpy实现"></a>numpy实现</h3><p><strong>失活掩模实现：先创建均匀分布在[0,1)中的数组，再比较忽略概率得到失火掩模</strong></p><p>函数<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.ranf.html" target="_blank" rel="noopener">numpy.random.ranf</a>和<a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.rand.html" target="_blank" rel="noopener">numpy.random.rand</a>都实现了<code>[0,1)</code>之间均匀分布，<strong>区别在于<code>ranf</code>输入维数元组，<code>rand</code>分别输入每个维数大小</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># [0,1)之间的均匀分布实现</span><br><span class="line">&gt;&gt;&gt; size = (2,4)</span><br><span class="line">&gt;&gt;&gt; a = np.random.ranf(size)</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">array([[0.55782603, 0.3881068 , 0.30671933, 0.5932138 ],</span><br><span class="line">       [0.68501697, 0.31336583, 0.79142952, 0.09579494]])</span><br><span class="line"># 伯努利掩模实现</span><br><span class="line">&gt;&gt;&gt; p = 0.5</span><br><span class="line">&gt;&gt;&gt; b = a &lt; p</span><br><span class="line">&gt;&gt;&gt; b</span><br><span class="line">array([[False,  True,  True, False],</span><br><span class="line">       [False,  True, False,  True]])</span><br></pre></td></tr></table></figure><h2 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h2><p>随机失活目的在于避免神经元之间的共适应性（<code>co-adaptation</code>），它鼓励每个隐藏单元在学习有用功能时不依赖于特定的其他隐藏单元来纠正错误</p><p>另一种解释是随机失活过程相当于多网络之间的模型平均（<code>modeling averaging with neural networks</code>）。通过模型集成方式，用多个不同网络进行预测，再通过平均预测结果能够有效减少测试集误差率。理论上$n$个隐藏层神经元可组成$2^{n}$个稀疏网络，随机失活在每次训练过程中随机忽略隐藏层的神经元（忽略概率$p$是一个超参数），相当于每次训练不同的神经网络</p><p><img src="/imgs/随机失活/dropout_net.png" alt></p><p>这些神经网络在同一个神经元上共享权重，最后在测试阶段，不再进行随机失活操作，使用神经网络所有隐藏神经元进行计算，但需要对权重乘以概率$p$，以此来平衡<code>平均网络（mean network）</code>多倍于<code>随机失活网络（dropout network）</code>的激活单元</p><p><img src="/imgs/随机失活/dropout_net_2.png" alt></p><p><strong>注意一：通常设置隐藏层随机忽略概率$p$为0.5</strong></p><p><strong>注意二：论文中还提到输入层神经元也可进行随机失活，其概率通常接近1，比如设置概率为0.9</strong></p><p><strong>注意三：多次实验发现输入层神经元失活极易出现损失无法收敛问题</strong></p><h2 id="模型描述及改进"><a href="#模型描述及改进" class="headerlink" title="模型描述及改进"></a>模型描述及改进</h2><h3 id="原始模型描述及实现"><a href="#原始模型描述及实现" class="headerlink" title="原始模型描述及实现"></a>原始模型描述及实现</h3><p>计算符号参考<a href="https://www.zhujian.tech/posts/cb820bb8.html#more">网络符号定义</a>，标准神经网络的前向计算过程如下：</p><script type="math/tex;mode=display">
z_{i}^{(l)} = a^{(l-1)}\cdot w_{i}^{(l)} + b_{i}^{(l)}</script><script type="math/tex;mode=display">
a_{i}^{(l)} = f(z_{i}^{(l)})</script><p>使用随机失活操作后，其前向计算如下：</p><script type="math/tex;mode=display">
r_{i}^{(l-1)} = Bernoulli(p)</script><script type="math/tex;mode=display">
\tilde{a_{i}}^{(l-1)} = a_{i}^{(l-1)} * r_{i}^{(l-1)}</script><script type="math/tex;mode=display">
z_{i}^{(l)} = \tilde{a_{i}}^{(l-1)}\cdot w_{i}^{(l)} + b_{i}^{(l)}</script><script type="math/tex;mode=display">
a_{i}^{(l)} = f(z_{i}^{(l)})</script><p><img src="/imgs/随机失活/dropout_net_3.png" alt></p><p><strong>注意：前向传播过程中失活的神经元在反向传播中其对应梯度为0</strong></p><h4 id="numpy实现-1"><a href="#numpy实现-1" class="headerlink" title="numpy实现"></a>numpy实现</h4><p>对3层神经网络进行随机失活操作如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">class ThreeNet(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, D_in, D_h1, D_h2, D_out, p_h=1.0):</span><br><span class="line">        ...</span><br><span class="line">        self.p_h = p_h</span><br><span class="line"></span><br><span class="line">    def forward(self, inputs):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        前向计算，计算评分函数值</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.N = inputs.shape[0]</span><br><span class="line">        self.a0 = inputs</span><br><span class="line"></span><br><span class="line">        self.z1 = inputs.dot(self.w) + self.b</span><br><span class="line">        self.a1 = np.maximum(0, self.z1)</span><br><span class="line">        # 创建失活掩模</span><br><span class="line">        U1 = np.random.ranf(self.a1.shape) &lt; self.p_h</span><br><span class="line">        self.a1 *= U1</span><br><span class="line"></span><br><span class="line">        self.z2 = self.a1.dot(self.w2) + self.b2</span><br><span class="line">        self.a2 = np.maximum(0, self.z2)</span><br><span class="line">        # 创建失活掩模</span><br><span class="line">        U2 = np.random.ranf(self.a2.shape) &lt; self.p_h</span><br><span class="line">        self.a2 *= U2</span><br><span class="line"></span><br><span class="line">        self.z3 = self.a2.dot(self.w3) + self.b3</span><br><span class="line">        expscores = np.exp(self.z3)</span><br><span class="line">        self.h = expscores / np.sum(expscores, axis=1, keepdims=True)</span><br><span class="line">        return self.h</span><br><span class="line"></span><br><span class="line">    def backward(self, output):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        反向传播，计算梯度</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        delta = self.h</span><br><span class="line">        delta[range(self.N), output] -= 1</span><br><span class="line">        delta /= self.N</span><br><span class="line"></span><br><span class="line">        self.dw3 = self.a2.T.dot(delta)</span><br><span class="line">        self.db3 = np.sum(delta, axis=0, keepdims=True)</span><br><span class="line"></span><br><span class="line">        da2 = delta.dot(self.w3.T)</span><br><span class="line">        # 失活掩模</span><br><span class="line">        da2 *= self.U2</span><br><span class="line">        dz2 = da2</span><br><span class="line">        dz2[self.z2 &lt; 0] = 0</span><br><span class="line"></span><br><span class="line">        self.dw2 = self.a1.T.dot(dz2)</span><br><span class="line">        self.db2 = np.sum(dz2, axis=0, keepdims=True)</span><br><span class="line"></span><br><span class="line">        da1 = dz2.dot(self.w2.T)</span><br><span class="line">        # 失活掩模</span><br><span class="line">        da1 *= self.U1</span><br><span class="line">        dz1 = da1</span><br><span class="line">        dz1[self.z1 &lt; 0] = 0</span><br><span class="line"></span><br><span class="line">        self.dw = self.a0.T.dot(dz1)</span><br><span class="line">        self.db = np.sum(dz1, axis=0, keepdims=True)</span><br><span class="line"></span><br><span class="line">    def update(self, lr=1e-3, reg_rate=0.0):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    def predict(self, inputs):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        前向计算，计算评分函数值</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        z1 = inputs.dot(self.w) + self.b</span><br><span class="line">        # 输出向量乘以忽略概率p</span><br><span class="line">        a1 = np.maximum(0, z1) * self.p_h</span><br><span class="line"></span><br><span class="line">        z2 = a1.dot(self.w2) + self.b2</span><br><span class="line">        # 输出向量乘以忽略概率p</span><br><span class="line">        a2 = np.maximum(0, z2) * self.p_h</span><br><span class="line"></span><br><span class="line">        z3 = a2.dot(self.w3) + self.b3</span><br><span class="line">        expscores = np.exp(z3)</span><br><span class="line">        self.h = expscores / np.sum(expscores, axis=1, keepdims=True)</span><br><span class="line">        return self.h</span><br></pre></td></tr></table></figure><h3 id="改进一：反向失活"><a href="#改进一：反向失活" class="headerlink" title="改进一：反向失活"></a>改进一：反向失活</h3><p>原始模型针对前向/后向传播进行了失活掩模操作，对预测阶段隐藏层输出值进行了<strong>等比例缩放</strong></p><p>实际操作中预测时间越短越好，所以采用<strong>反向失活</strong>（<code>inverted dropout</code>）策略</p><p>假设原先隐藏层输出为$X$，所以在预测阶段需要称以概率$p$，最终值为$pX$</p><p>在反向失活操作中，将$X \rightarrow pX$，那么在预测阶段就不需要再乘以概率$p$，在训练阶段就需要修改为$pX / p = X$</p><p><strong>相当于在训练阶段进行数据放大，在预测阶段正常输出即可</strong></p><p>模型描述如下：</p><script type="math/tex;mode=display">
r_{i}^{(l-1)} = Bernoulli(p) / p</script><h4 id="numpy实现-2"><a href="#numpy实现-2" class="headerlink" title="numpy实现"></a>numpy实现</h4><p>反向失活操作如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def dropout(shape, p):</span><br><span class="line">    assert len(shape) == 2</span><br><span class="line">    res = (np.random.ranf(shape) &lt; p) / p</span><br><span class="line"></span><br><span class="line">    if np.sum(res) == 0:</span><br><span class="line">        return 1.0 / p</span><br><span class="line">    return res</span><br></pre></td></tr></table></figure><p>集成到网络中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">class ThreeNet(object):</span><br><span class="line"></span><br><span class="line">    def __init__(self, D_in, D_h1, D_h2, D_out, p_h=1.0):</span><br><span class="line">        ...</span><br><span class="line">        self.p_h = p_h</span><br><span class="line"></span><br><span class="line">    def forward(self, inputs):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        前向计算，计算评分函数值</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.N = inputs.shape[0]</span><br><span class="line">        self.a0 = inputs</span><br><span class="line"></span><br><span class="line">        self.z1 = inputs.dot(self.w) + self.b</span><br><span class="line">        self.a1 = np.maximum(0, self.z1)</span><br><span class="line">        # 创建失活掩模</span><br><span class="line">        U1 = (np.random.ranf(self.a1.shape) &lt; self.p_h) / self.p_h</span><br><span class="line">        self.a1 *= U1</span><br><span class="line"></span><br><span class="line">        self.z2 = self.a1.dot(self.w2) + self.b2</span><br><span class="line">        self.a2 = np.maximum(0, self.z2)</span><br><span class="line">        # 创建失活掩模</span><br><span class="line">        U2 = (np.random.ranf(self.a2.shape) &lt; self.p_h) / self.p_h</span><br><span class="line">        self.a2 *= U2</span><br><span class="line"></span><br><span class="line">        self.z3 = self.a2.dot(self.w3) + self.b3</span><br><span class="line">        expscores = np.exp(self.z3)</span><br><span class="line">        self.h = expscores / np.sum(expscores, axis=1, keepdims=True)</span><br><span class="line">        return self.h</span><br><span class="line"></span><br><span class="line">    def backward(self, output):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        反向传播，计算梯度</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        delta = self.h</span><br><span class="line">        delta[range(self.N), output] -= 1</span><br><span class="line">        delta /= self.N</span><br><span class="line"></span><br><span class="line">        self.dw3 = self.a2.T.dot(delta)</span><br><span class="line">        self.db3 = np.sum(delta, axis=0, keepdims=True)</span><br><span class="line"></span><br><span class="line">        da2 = delta.dot(self.w3.T)</span><br><span class="line">        # 失活掩模</span><br><span class="line">        da2 *= self.U2</span><br><span class="line">        dz2 = da2</span><br><span class="line">        dz2[self.z2 &lt; 0] = 0</span><br><span class="line"></span><br><span class="line">        self.dw2 = self.a1.T.dot(dz2)</span><br><span class="line">        self.db2 = np.sum(dz2, axis=0, keepdims=True)</span><br><span class="line"></span><br><span class="line">        da1 = dz2.dot(self.w2.T)</span><br><span class="line">        # 失活掩模</span><br><span class="line">        da1 *= self.U1</span><br><span class="line">        dz1 = da1</span><br><span class="line">        dz1[self.z1 &lt; 0] = 0</span><br><span class="line"></span><br><span class="line">        self.dw = self.a0.T.dot(dz1)</span><br><span class="line">        self.db = np.sum(dz1, axis=0, keepdims=True)</span><br><span class="line"></span><br><span class="line">    def update(self, lr=1e-3, reg_rate=0.0):</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">    def predict(self, inputs):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        前向计算，计算评分函数值</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        z1 = inputs.dot(self.w) + self.b</span><br><span class="line">        a1 = np.maximum(0, z1)</span><br><span class="line"></span><br><span class="line">        z2 = a1.dot(self.w2) + self.b2</span><br><span class="line">        a2 = np.maximum(0, z2)</span><br><span class="line"></span><br><span class="line">        z3 = a2.dot(self.w3) + self.b3</span><br><span class="line">        expscores = np.exp(z3)</span><br><span class="line">        self.h = expscores / np.sum(expscores, axis=1, keepdims=True)</span><br><span class="line">        return self.h</span><br></pre></td></tr></table></figure><h3 id="空间失活"><a href="#空间失活" class="headerlink" title="空间失活"></a>空间失活</h3><p>参考：</p><p><a href="https://pytorch.org/docs/stable/nn.html#dropout2d" target="_blank" rel="noopener">Dropout2d</a></p><p><a href="https://discuss.pytorch.org/t/confusion-about-dropout2d/21958" target="_blank" rel="noopener">Confusion about dropout2d</a></p><p>在<code>4-D</code>张量中，相邻的特征可能是强相关的，因此标准失活操作将无法有效地规范网络，空间失活以通道为单位，随机对整个通道（<em>激活图</em>）进行清零</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; m = nn.Dropout2d(p=0.5)</span><br><span class="line">&gt;&gt;&gt; m(torch.arange(16).reshape(1,4,2,2).float())</span><br><span class="line">tensor([[[[ 0.,  2.],</span><br><span class="line">          [ 4.,  6.]],</span><br><span class="line"></span><br><span class="line">         [[ 8., 10.],</span><br><span class="line">          [12., 14.]],</span><br><span class="line"></span><br><span class="line">         [[ 0.,  0.],</span><br><span class="line">          [ 0.,  0.]],</span><br><span class="line"></span><br><span class="line">         [[ 0.,  0.],</span><br><span class="line">          [ 0.,  0.]]]])</span><br></pre></td></tr></table></figure><h4 id="numpy实现-3"><a href="#numpy实现-3" class="headerlink" title="numpy实现"></a>numpy实现</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def dropout2d(shape, p):</span><br><span class="line">    assert len(shape) == 4</span><br><span class="line">    N, C, H, W = shape[:4]</span><br><span class="line">    U = (np.random.rand(N * C, 1) &lt; p) / p</span><br><span class="line">    res = np.ones((N * C, H * W))</span><br><span class="line">    res *= U</span><br><span class="line"></span><br><span class="line">    if np.sum(res) == 0:</span><br><span class="line">        return 1.0 / p</span><br><span class="line">    return res.reshape(N, C, H, W)</span><br></pre></td></tr></table></figure><h2 id="3层神经网络测试"><a href="#3层神经网络测试" class="headerlink" title="3层神经网络测试"></a>3层神经网络测试</h2><p>3层随机失活神经网络实现如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">class ThreeLayerNet(Net):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    实现3层神经网络</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">    def __init__(self, num_in, num_h_one, num_h_two, num_out, momentum=0, nesterov=False, p_h=1.0, p_in=1.0):</span><br><span class="line">        super(ThreeLayerNet, self).__init__()</span><br><span class="line">        self.fc1 = FC(num_in, num_h_one, momentum=momentum, nesterov=nesterov)</span><br><span class="line">        self.relu1 = ReLU()</span><br><span class="line">        self.fc2 = FC(num_h_one, num_h_two, momentum=momentum, nesterov=nesterov)</span><br><span class="line">        self.relu2 = ReLU()</span><br><span class="line">        self.fc3 = FC(num_h_two, num_out, momentum=momentum, nesterov=nesterov)</span><br><span class="line"></span><br><span class="line">        self.p_h = p_h</span><br><span class="line">        self.p_in = p_in</span><br><span class="line">        self.U1 = None</span><br><span class="line">        self.U2 = None</span><br><span class="line"></span><br><span class="line">    def __call__(self, inputs):</span><br><span class="line">        return self.forward(inputs)</span><br><span class="line"></span><br><span class="line">    def forward(self, inputs):</span><br><span class="line">        # inputs.shape = [N, D_in]</span><br><span class="line">        assert len(inputs.shape) == 2</span><br><span class="line">        U0 = F.dropout(inputs.shape, self.p_in)</span><br><span class="line">        inputs *= U0</span><br><span class="line"></span><br><span class="line">        a1 = self.relu1(self.fc1(inputs))</span><br><span class="line">        self.U1 = F.dropout(a1.shape, self.p_h)</span><br><span class="line">        a1 *= self.U1</span><br><span class="line"></span><br><span class="line">        a2 = self.relu2(self.fc2(a1))</span><br><span class="line">        self.U2 = F.dropout(a2.shape, self.p_h)</span><br><span class="line">        a2 *= self.U2</span><br><span class="line"></span><br><span class="line">        z3 = self.fc3(a2)</span><br><span class="line"></span><br><span class="line">        return z3</span><br><span class="line"></span><br><span class="line">    def backward(self, grad_out):</span><br><span class="line">        da2 = self.fc3.backward(grad_out) * self.U2</span><br><span class="line">        dz2 = self.relu2.backward(da2)</span><br><span class="line">        da1 = self.fc2.backward(dz2) * self.U1</span><br><span class="line">        dz1 = self.relu1.backward(da1)</span><br><span class="line">        da0 = self.fc1.backward(dz1)</span><br><span class="line"></span><br><span class="line">    def update(self, lr=1e-3, reg=1e-3):</span><br><span class="line">        self.fc3.update(learning_rate=lr, regularization_rate=reg)</span><br><span class="line">        self.fc2.update(learning_rate=lr, regularization_rate=reg)</span><br><span class="line">        self.fc1.update(learning_rate=lr, regularization_rate=reg)</span><br><span class="line"></span><br><span class="line">    def predict(self, inputs):</span><br><span class="line">        # inputs.shape = [N, D_in]</span><br><span class="line">        assert len(inputs.shape) == 2</span><br><span class="line">        a1 = self.relu1(self.fc1(inputs))</span><br><span class="line">        a2 = self.relu2(self.fc2(a1))</span><br><span class="line">        z3 = self.fc3(a2)</span><br><span class="line"></span><br><span class="line">        return z3</span><br><span class="line"></span><br><span class="line">    def get_params(self):</span><br><span class="line">        return &#123;&apos;fc1&apos;: self.fc1.get_params(), &apos;fc2&apos;: self.fc2.get_params(), &apos;fc3&apos;: self.fc3.get_params(),</span><br><span class="line">                &apos;p_h&apos;: self.p_h, &apos;p_in&apos;: self.p_in&#125;</span><br><span class="line"></span><br><span class="line">    def set_params(self, params):</span><br><span class="line">        self.fc1.set_params(params[&apos;fc1&apos;])</span><br><span class="line">        self.fc2.set_params(params[&apos;fc2&apos;])</span><br><span class="line">        self.fc3.set_params(params[&apos;fc3&apos;])</span><br><span class="line">        self.p_h = params.get(&apos;p_h&apos;, 1.0)</span><br><span class="line">        self.p_in = params.get(&apos;p_in&apos;, 1.0)</span><br></pre></td></tr></table></figure><h3 id="训练细节"><a href="#训练细节" class="headerlink" title="训练细节"></a>训练细节</h3><p>利用<a href="https://www.zhujian.tech/posts/43d7ec86.html">cifar-10</a>进行测试，原始图像大小为<code>(32, 32, 3)</code>，加载数据实现： <a href>PyNet/data/load_cifar_10.py</a></p><p>测试参数如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># 迭代次数</span><br><span class="line">epochs = 300</span><br><span class="line"># 批量大小</span><br><span class="line">batch_size = 256</span><br><span class="line"># 输入维数</span><br><span class="line">D = 3072</span><br><span class="line"># 隐藏层大小</span><br><span class="line">H1 = 2000</span><br><span class="line">H2 = 800</span><br><span class="line"># 输出类别</span><br><span class="line">K = 40</span><br><span class="line"></span><br><span class="line"># 学习率</span><br><span class="line">lr = 1e-3</span><br><span class="line"># 正则化强度</span><br><span class="line">reg_rate = 1e-3</span><br><span class="line"># 隐藏层失活率</span><br><span class="line">p_h = 0.5</span><br></pre></td></tr></table></figure><p>每隔50次迭代学习率下降一半，共下降4次</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">if i % 50 == 49 and i &lt; 200:</span><br><span class="line">    lr /= 2</span><br></pre></td></tr></table></figure><p>测试代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">    for i in range(epochs):</span><br><span class="line">        start = time.time()</span><br><span class="line">        total_loss = 0</span><br><span class="line">        for j in range_list:</span><br><span class="line">            data = x_train[j:j + batch_size]</span><br><span class="line">            labels = y_train[j:j + batch_size]</span><br><span class="line"></span><br><span class="line">            scores = net.forward(data)</span><br><span class="line">            loss = criterion.forward(scores, labels)</span><br><span class="line">            total_loss += loss</span><br><span class="line">            dout = criterion.backward()</span><br><span class="line">            net.backward(dout)</span><br><span class="line">            net.update(lr=lr, reg=reg_rate)</span><br><span class="line">        end = time.time()</span><br><span class="line"></span><br><span class="line">        avg_loss = total_loss / len(range_list)</span><br><span class="line">        loss_list.append(float(&apos;%.4f&apos; % avg_loss))</span><br><span class="line">        print(&apos;epoch: %d time: %.2f loss: %.4f&apos; % (i + 1, end - start, avg_loss))</span><br><span class="line"></span><br><span class="line">        if i % 10 == 9:</span><br><span class="line">            # 计算训练数据集检测精度</span><br><span class="line">            train_accuracy = compute_accuracy(x_train, y_train, net, batch_size=batch_size)</span><br><span class="line">            train_accuracy_list.append(float(&apos;%.4f&apos; % train_accuracy))</span><br><span class="line">            if best_train_accuracy &lt; train_accuracy:</span><br><span class="line">                best_train_accuracy = train_accuracy</span><br><span class="line"></span><br><span class="line">                test_accuracy = compute_accuracy(x_test, y_test, net, batch_size=batch_size)</span><br><span class="line">                if best_test_accuracy &lt; test_accuracy:</span><br><span class="line">                    best_test_accuracy = test_accuracy</span><br><span class="line">                    # save_params(net.get_params(), path=&apos;./three-nn-dropout-epochs-%d.pkl&apos; % (i + 1))</span><br><span class="line"></span><br><span class="line">            print(&apos;best train accuracy: %.2f %%   best test accuracy: %.2f %%&apos; % (</span><br><span class="line">                best_train_accuracy * 100, best_test_accuracy * 100))</span><br><span class="line">            print(loss_list)</span><br><span class="line">            print(train_accuracy_list)</span><br><span class="line"></span><br><span class="line">        if i % 50 == 49 and收敛 i &lt; 200:</span><br><span class="line">            lr /= 2收敛</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>完整代码地址： <a href="https://github.com/zjZSTU/PyNet/blo收敛b/master/src/three_nn_dropout_cifar_10.py" target="_blank" rel="noopener">PyNet/src/t收敛hree_nn_dropout_cifar_10.py</a></p><h3 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h3><p>进行<code>2</code>种不同网络测试：</p><ul><li>标准神经网络$A$</li><li>失活神经网络$B$，隐藏层忽略概率为$p_h=0.5$</li></ul><p>训练<code>300</code>次后，最好的训练和测试精度如下：</p><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">训练精度</th><th style="text-align:center">测试精度</th></tr></thead><tbody><tr><td style="text-align:center">A</td><td style="text-align:center">100%</td><td style="text-align:center">57.63%</td></tr><tr><td style="text-align:center">B</td><td style="text-align:center">99.79%</td><td style="text-align:center">60.01%</td></tr></tbody></table></div><p>随机失活损失和训练精度如下：</p><p><img src="/imgs/随机失活/dropout_loss.png" alt></p><p><img src="/imgs/随机失活/dropout_accuracy.png" alt></p><p>标准3层神经网络损失和训练精度如下：</p><p><img src="/imgs/随机失活/nn_loss.png" alt></p><p><img src="/imgs/随机失活/nn_accuracy.png" alt></p><p>两者比较如下：</p><p><img src="/imgs/随机失活/dropout_nn_loss.png" alt></p><p><img src="/imgs/随机失活/dropout_nn_accuracy.png" alt></p><h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><p>从训练结果可看出失活神经网络比标准神经网络泛化能力更强</p><p>从图中可以看出，失活神经网络比标准神经网络需要更多训练才能收敛</p></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.jpg" alt="zhujian 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="zhujian 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zhujian</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zhujian.tech/posts/20cc7a49.html" title="随机失活">https://www.zhujian.tech/posts/20cc7a49.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/python/" rel="tag"># python</a> <a href="/tags/pytorch/" rel="tag"># pytorch</a> <a href="/tags/numpy/" rel="tag"># numpy</a> <a href="/tags/probability-theory/" rel="tag"># 概率论</a> <a href="/tags/dropout/" rel="tag"># 随机失活</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/49729a62.html" rel="next" title="主成分分析"><i class="fa fa-chevron-left"></i> 主成分分析</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/posts/2bee4fce.html" rel="prev" title="随机失活-pytorch">随机失活-pytorch<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#伯努利分布-均匀分布"><span class="nav-number">1.</span> <span class="nav-text">伯努利分布/均匀分布</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#伯努利分布"><span class="nav-number">1.1.</span> <span class="nav-text">伯努利分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#均匀分布"><span class="nav-number">1.2.</span> <span class="nav-text">均匀分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#numpy实现"><span class="nav-number">1.3.</span> <span class="nav-text">numpy实现</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实现原理"><span class="nav-number">2.</span> <span class="nav-text">实现原理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模型描述及改进"><span class="nav-number">3.</span> <span class="nav-text">模型描述及改进</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#原始模型描述及实现"><span class="nav-number">3.1.</span> <span class="nav-text">原始模型描述及实现</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#numpy实现-1"><span class="nav-number">3.1.1.</span> <span class="nav-text">numpy实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#改进一：反向失活"><span class="nav-number">3.2.</span> <span class="nav-text">改进一：反向失活</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#numpy实现-2"><span class="nav-number">3.2.1.</span> <span class="nav-text">numpy实现</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#空间失活"><span class="nav-number">3.3.</span> <span class="nav-text">空间失活</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#numpy实现-3"><span class="nav-number">3.3.1.</span> <span class="nav-text">numpy实现</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3层神经网络测试"><span class="nav-number">4.</span> <span class="nav-text">3层神经网络测试</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#训练细节"><span class="nav-number">4.1.</span> <span class="nav-text">训练细节</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#训练结果"><span class="nav-number">4.2.</span> <span class="nav-text">训练结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分析"><span class="nav-number">4.3.</span> <span class="nav-text">分析</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zhujian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">zhujian</p><div class="site-description" itemprop="description">one bite at a time</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">169</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">129</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/./atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zjZSTU" title="Github &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;zjZSTU" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/u012005313" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;u012005313" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i> CSDN</a></span><span class="links-of-author-item"><a href="/mailto:zjzstu@gmail.com" title="Gmail &amp;rarr; mailto:zjzstu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> Gmail</a></span><span class="links-of-author-item"><a href="/mailto:505169307@gmail.com" title="QQmail &amp;rarr; mailto:505169307@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> QQmail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://cloud.tencent.com/" title="https:&#x2F;&#x2F;cloud.tencent.com" rel="noopener" target="_blank">腾讯云</a></li><li class="links-of-blogroll-item"> <a href="https://www.aliyun.com/" title="https:&#x2F;&#x2F;www.aliyun.com" rel="noopener" target="_blank">阿里云</a></li><li class="links-of-blogroll-item"> <a href="https://developer.android.com/" title="https:&#x2F;&#x2F;developer.android.com&#x2F;" rel="noopener" target="_blank">Android Dev</a></li><li class="links-of-blogroll-item"> <a href="https://jenkins.io/" title="https:&#x2F;&#x2F;jenkins.io&#x2F;" rel="noopener" target="_blank">Jenkins</a></li><li class="links-of-blogroll-item"> <a href="http://cs231n.github.io/" title="http:&#x2F;&#x2F;cs231n.github.io&#x2F;" rel="noopener" target="_blank">cs231n</a></li><li class="links-of-blogroll-item"> <a href="https://pytorch.org/docs/stable/index.html" title="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;index.html" rel="noopener" target="_blank">pytorch</a></li><li class="links-of-blogroll-item"> <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" title="https:&#x2F;&#x2F;docs.docker.com&#x2F;install&#x2F;linux&#x2F;docker-ce&#x2F;ubuntu&#x2F;" rel="noopener" target="_blank">docker</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">zhujian</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">948k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">26:20</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-bar-chart-o"></i></span> <a href="https://tongji.baidu.com/web/27249108/overview/index?siteId=13647183" rel="noopener" target="_blank">百度统计</a></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div><div class="beian"> <a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备 19026415号</a> <span class="post-meta-divider">|</span> <img src="/images/beian_icon.png" style="display:inline-block;text-decoration:none;height:13px"> <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001748" rel="noopener" target="_blank">浙公网安备 33011802001748号</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span> <span class="site-uv" title="总访客量">访客数：<span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="site-pv" title="总访问量">阅读量：<span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="17,63,61" opacity="1" zindex="-1" count="199" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '8f22595c6dd29c94467d',
      clientSecret: 'e66bd90ebb9aa66c562c753dec6c90ceecbd51f2',
      repo: 'guestbook',
      owner: 'zjZSTU',
      admin: ['zjZSTU'],
      id: '4bf6de60837bc41512403244925e4c1c',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script></body></html>