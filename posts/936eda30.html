<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/./atom.xml" title="做一个幸福的人" type="application/atom+xml"><meta name="google-site-verification" content="Qr_3yqLtyErDFKHW7mE8PJz4qDUX-bf_fMLpSRckQe4"><meta name="baidu-site-verification" content="zOwIvKMV7f"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"搜索文章",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet"><meta name="description" content="参考：Annealing the learning rate学习率退火在标准随机梯度下降过程中，每次更新使用固定学习率（learning rate），迭代一定次数后损失值不再下降，一种解释是因为权重在最优点周围打转，如果能够在迭代过程中减小学习率，就能够更加接近最优点，实现更高的检测精度"><meta name="keywords" content="python,pytorch,numpy,matplotlib,学习率衰减"><meta property="og:type" content="article"><meta property="og:title" content="学习率退火"><meta property="og:url" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;posts&#x2F;936eda30.html"><meta property="og:site_name" content="做一个幸福的人"><meta property="og:description" content="参考：Annealing the learning rate学习率退火在标准随机梯度下降过程中，每次更新使用固定学习率（learning rate），迭代一定次数后损失值不再下降，一种解释是因为权重在最优点周围打转，如果能够在迭代过程中减小学习率，就能够更加接近最优点，实现更高的检测精度"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;学习率退火&#x2F;annealing.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;学习率退火&#x2F;iris_loss.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;学习率退火&#x2F;iris_accuracy.png"><meta property="og:updated_time" content="2020-02-15T05:36:35.875Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;学习率退火&#x2F;annealing.png"><link rel="canonical" href="https://www.zhujian.tech/posts/936eda30.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>学习率退火 | 做一个幸福的人</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e677aac1ac69b8826b9cfecb4e72e107";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">做一个幸福的人</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">面朝大海，春暖花开</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">129</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">48</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">169</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header> <a href="https://github.com/zjZSTU" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zhujian.tech/posts/936eda30.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="zhujian"><meta itemprop="description" content="one bite at a time"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="做一个幸福的人"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 学习率退火</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-05-29 13:38:50" itemprop="dateCreated datePublished" datetime="2019-05-29T13:38:50+00:00">2019-05-29</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-15 05:36:35" itemprop="dateModified" datetime="2020-02-15T05:36:35+00:00">2020-02-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/" itemprop="url" rel="index"><span itemprop="name">算法</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">编程</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/optimization/" itemprop="url" rel="index"><span itemprop="name">最优化</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/programming-language/" itemprop="url" rel="index"><span itemprop="name">编程语言</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/codebase/" itemprop="url" rel="index"><span itemprop="name">代码库</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>7k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>12 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>参考：</p><p><a href="http://cs231n.github.io/neural-networks-3/#anneal" target="_blank" rel="noopener">Annealing the learning rate</a></p><p><a href="https://zhuanlan.zhihu.com/p/21798784" target="_blank" rel="noopener">学习率退火</a></p><p>在标准随机梯度下降过程中，每次更新使用固定学习率（<code>learning rate</code>），迭代一定次数后损失值不再下降，一种解释是因为权重在最优点周围打转，如果能够在迭代过程中减小学习率，就能够更加接近最优点，实现更高的检测精度</p><a id="more"></a><p>学习率退火（<code>annealing the learning rate</code>）属于优化策略的一种，有<code>3</code>种方式实现学习率随时间下降</p><ol><li>随步数衰减（<code>step decay</code>）</li><li>指数衰减（<code>exponential decay</code>）</li><li><code>1/t</code>衰减（<code>1/t decay</code>）</li></ol><p>下面介绍这<code>3</code>种学习率退火实现，然后用<code>numpy</code>编程进行验证</p><h2 id="随步数衰减"><a href="#随步数衰减" class="headerlink" title="随步数衰减"></a>随步数衰减</h2><p>随步数衰减（<code>step decay</code>）指的是多次迭代后降低学习率再继续迭代</p><p>如果选择固定迭代次数，实现公式如下：</p><script type="math/tex;mode=display">
lr = lr_{0} * \beta^{t/T}</script><ul><li>$lr$表示学习率</li><li>$lr_{0}$表示初始学习率</li><li>$\beta$表示衰减因子，通常是0.5</li><li>$t$表示迭代次数</li><li>$T$是一个常量，表示迭代次数</li></ul><p>其中$t/T$是一个整数除法，比如$2/4=0, 5/4=1$</p><p>迭代多少次才进行学习率衰减取决于实际问题和模型，如果无法确定可以先打印出标准的随机梯度下降过程的验证集误差（<code>val error</code>），选择验证集误差不再下降的时候降低学习率</p><h2 id="指数衰减"><a href="#指数衰减" class="headerlink" title="指数衰减"></a>指数衰减</h2><p>指数衰减（<code>exponential decay</code>）指的是学习率随迭代次数指数下降，数学公式如下：</p><script type="math/tex;mode=display">
lr = lr_{0} e^{-kt}</script><ul><li>$lr$表示学习率</li><li>$lr_{0}$表示初始学习率</li><li>$k$表示衰减因子</li><li>$t$是迭代次数</li></ul><p>其衰减速度随指数下降，一方面可以提高初始学习率，另一方面可以结合随步数衰减策略，多次迭代后再衰减，这样可以探索更大的权重空间</p><h2 id="1-t衰减"><a href="#1-t衰减" class="headerlink" title="1/t衰减"></a>1/t衰减</h2><p><code>1/t</code>衰减实现公式如下：</p><script type="math/tex;mode=display">
lr = lr_{0}/(1+kt)</script><ul><li>$lr$表示学习率</li><li>$lr_{0}$表示初始学习率</li><li>$k$表示衰减因子</li><li>$t$是迭代次数</li></ul><h2 id="衰减比较"><a href="#衰减比较" class="headerlink" title="衰减比较"></a>衰减比较</h2><p>假定初始学习率为<code>1e-3</code>，衰减因子<code>k=0.5</code>，随步长衰减方式每隔<code>10</code>次迭代衰减一次，结果如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    lr = 1e-3</span><br><span class="line">    k = 0.5</span><br><span class="line">    a = np.repeat(np.arange(10), 10)</span><br><span class="line">    print(a)</span><br><span class="line"></span><br><span class="line">    x = np.arange(0, 100)</span><br><span class="line">    y1 = lr * np.power(k, a)</span><br><span class="line">    y2 = lr * np.exp(x * k * -1)</span><br><span class="line">    y3 = lr / (1 + k * x)</span><br><span class="line"></span><br><span class="line">    plt.title(&apos;初始学习率1e-3，衰减因子k=0.5&apos;)</span><br><span class="line">    plt.plot(x, y1, label=&apos;step decay&apos;)</span><br><span class="line">    plt.plot(x, y2, label=&apos;exponential decay&apos;)</span><br><span class="line">    plt.plot(x, y3, label=&apos;1/t decay&apos;)</span><br><span class="line">    plt.legend()</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure><p><img src="/imgs/学习率退火/annealing.png" alt></p><p>从数值上看，指数衰减最快，随步长衰减最不平滑，<code>1/t</code>衰减是前<code>2</code>者的折中</p><p>从概念上看，随步长衰减最具解释性</p><h2 id="Iris分类"><a href="#Iris分类" class="headerlink" title="Iris分类"></a>Iris分类</h2><p>参考<a href="https://www.zhujian.tech/posts/ba2ca878.html">iris数据集</a>，使用3层神经网络实现<code>Iris</code>数据集分类</p><p>网络和训练参数如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 批量大小</span><br><span class="line">N = 120</span><br><span class="line"># 输入维数</span><br><span class="line">D = 4</span><br><span class="line"># 隐藏层大小</span><br><span class="line">H1 = 20</span><br><span class="line">H2 = 20</span><br><span class="line"># 输出类别</span><br><span class="line">K = 3</span><br><span class="line"></span><br><span class="line"># 学习率</span><br><span class="line">learning_rate = 5e-2</span><br><span class="line"># 正则化强度</span><br><span class="line">lambda_rate = 1e-3</span><br></pre></td></tr></table></figure><p>训练<code>1</code>万次得到最好的训练集精度<code>98.33%</code>，验证集精度为<code>100.00%</code></p><p>使用随步数衰减方法，设置初始学习率为<code>1e-3</code>，每隔<code>1</code>万次迭代降低一半学习率</p><p><img src="/imgs/学习率退火/iris_loss.png" alt></p><p><img src="/imgs/学习率退火/iris_accuracy.png" alt></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">epoch: 23500 loss: 0.017741</span><br><span class="line">epoch: 24000 loss: 0.018153</span><br><span class="line">epoch: 24500 loss: 0.018558</span><br><span class="line">epoch: 25000 loss: 0.018764</span><br><span class="line">epoch: 25500 loss: 0.018719</span><br><span class="line">loss: [0.7379489541275116, 0.20908036151565673, 0.10376114688270188, 0.08329572639151012, 0.07412643490314004, 0.07430345900744695, 0.07329295342743611, 0.06805091543927848, 0.07108344457821464, 0.08216043914493876, 0.07653607556430879, 0.07156388982573988, 0.07343534475625284, 0.07208068217751779, 0.0720487384083792, 0.07222908671895177, 0.06718030446399169, 0.06926601609539103, 0.06213898417682324, 0.048173863501391634, 0.04090511152968822, 0.039191952291425525, 0.03774620932625705, 0.036470436045793926, 0.03520645248822737, 0.0339090050684377, 0.03254478210455497, 0.03136029152475116, 0.03083107298989087, 0.031088612177247885, 0.031830754113294016, 0.03244430157874315, 0.032756847542928104, 0.0328499065045292, 0.03287723349959883, 0.03279460842225148, 0.03199686035899375, 0.031768831964163566, 0.03155549009925631, 0.03148352140369718, 0.020641865478458574, 0.019643064986634092, 0.018938254489189885, 0.018306088376815275, 0.018001452040576762, 0.017746890705130948, 0.017741358711630295, 0.018153272132707534, 0.018558267622501148, 0.018764374296570147, 0.01871930452412146]</span><br><span class="line">train: [0.7, 0.9416666666666667, 0.975, 0.975, 0.975, 0.975, 0.975, 0.975, 0.9333333333333333, 0.975, 0.975, 0.975, 0.975, 0.975, 0.9666666666666667, 0.975, 0.9833333333333333, 0.9416666666666667, 0.9833333333333333, 0.9833333333333333, 0.975, 0.975, 0.975, 0.975, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9833333333333333, 0.9916666666666667, 0.9916666666666667, 0.9916666666666667, 0.9916666666666667, 0.9916666666666667, 0.9916666666666667, 0.9916666666666667, 0.9916666666666667, 0.9916666666666667, 0.9916666666666667, 1.0]</span><br><span class="line">test: [0.6, 0.9333333333333333, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 0.9666666666666667, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</span><br></pre></td></tr></table></figure><p>共训练<code>25500</code>次实现<code>100%</code>的训练集精度和测试集精度</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    x_train, x_test, y_train, y_test = load_data(shuffle=True, tsize=0.8)</span><br><span class="line"></span><br><span class="line">    net = ThreeLayerNet(D, H1, H2, K)</span><br><span class="line">    criterion = CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line">    loss_list = []</span><br><span class="line">    train_list = []</span><br><span class="line">    test_list = []</span><br><span class="line">    total_loss = 0</span><br><span class="line">    for i in range(epochs):</span><br><span class="line">        scores = net(x_train)</span><br><span class="line">        total_loss += criterion(scores, y_train)</span><br><span class="line"></span><br><span class="line">        grad_out = criterion.backward()</span><br><span class="line">        net.backward(grad_out)</span><br><span class="line">        net.update(lr=learning_rate, reg=0)</span><br><span class="line"></span><br><span class="line">        if (i % 500) == 499:</span><br><span class="line">            print(&apos;epoch: %d loss: %f&apos; % (i + 1, total_loss / 500))</span><br><span class="line">            loss_list.append(total_loss / 500)</span><br><span class="line">            total_loss = 0</span><br><span class="line"></span><br><span class="line">            train_accuracy = compute_accuracy(scores, y_train)</span><br><span class="line">            test_accuracy = compute_accuracy(net(x_test), y_test)</span><br><span class="line">            train_list.append(train_accuracy)</span><br><span class="line">            test_list.append(test_accuracy)</span><br><span class="line">            if train_accuracy &gt;= 0.9999 and test_accuracy &gt;= 0.9999:</span><br><span class="line">                save_params(net.get_params(), path=&apos;three_layer_net_iris.pkl&apos;)</span><br><span class="line">                break</span><br><span class="line">        if (i % 10000) == 9999:</span><br><span class="line">            # 每隔10000次降低学习率</span><br><span class="line">            learning_rate *= 0.5</span><br></pre></td></tr></table></figure><p>完整代码： <a href="https://github.com/zjZSTU/PyNet/blob/master/src/three_layer_net_iris.py" target="_blank" rel="noopener">PyNet/src/three_layer_net_iris.py</a></p><p>参数地址： <a href="https://github.com/zjZSTU/PyNet/blob/master/model/three_layer_net_iris.pkl" target="_blank" rel="noopener">PyNet/model/three_layer_net_iris.pkl</a></p><h2 id="Pytorch实现"><a href="#Pytorch实现" class="headerlink" title="Pytorch实现"></a>Pytorch实现</h2><p><code>Pytorch</code>提供模块<a href="https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate" target="_blank" rel="noopener">torch.optim.lr_scheduler</a>用于学习率退火实现</p><p>参考<a href="https://discuss.pytorch.org/t/how-to-use-torch-optim-lr-scheduler-exponentiallr/12444" target="_blank" rel="noopener">How to use torch.optim.lr_scheduler.ExponentialLR?</a>，<code>lr_scheduler</code>的<code>step</code>方法仅用于更新学习率，和反向传播无关</p><h3 id="随步长衰减"><a href="#随步长衰减" class="headerlink" title="随步长衰减"></a>随步长衰减</h3><p>有<code>3</code>种方法</p><ol><li>LambdaLR</li><li>StepLR</li><li>MultiStepLR</li></ol><h4 id="LambdaLR"><a href="#LambdaLR" class="headerlink" title="LambdaLR"></a>LambdaLR</h4><blockquote><p>class torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)</p></blockquote><ul><li><code>optimizer</code>是优化器</li><li><code>lr_lambda</code>是<code>lambda</code>函数，输入为迭代次数，用于计算衰减率</li></ul><p>每次迭代都通过<code>lambda</code>函数计算新的衰减率，再乘以初始学习率就是当前学习率</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># @Time    : 19-6-7 下午4:30</span><br><span class="line"># @Author  : zj</span><br><span class="line"></span><br><span class="line">from torch.optim.lr_scheduler import LambdaLR</span><br><span class="line">import torch.optim as optim</span><br><span class="line">import torch.nn as nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Net(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(32, 12)</span><br><span class="line"></span><br><span class="line">    def forward(self, *input):</span><br><span class="line">        return self.fc(input)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"></span><br><span class="line">optimer = optim.SGD(net.parameters(), lr=0.1)</span><br><span class="line"></span><br><span class="line">lambda1 = lambda epoch: epoch // 5 + 1</span><br><span class="line">lambda2 = lambda epoch: 0.95 ** epoch</span><br><span class="line"></span><br><span class="line">scheduler = LambdaLR(optimer, lr_lambda=lambda1)</span><br><span class="line"></span><br><span class="line">for epoch in range(20):</span><br><span class="line">    scheduler.step()</span><br><span class="line">    lr = scheduler.get_lr()</span><br><span class="line">    print(lr)</span><br></pre></td></tr></table></figure><p><code>lambda1</code>函数功能是每隔<code>5</code>次迭代提高<code>1</code>倍学习率，结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[0.1]</span><br><span class="line">[0.1]</span><br><span class="line">[0.1]</span><br><span class="line">[0.1]</span><br><span class="line">[0.1]</span><br><span class="line">[0.2]</span><br><span class="line">[0.2]</span><br><span class="line">[0.2]</span><br><span class="line">[0.2]</span><br><span class="line">[0.2]</span><br><span class="line">[0.30000000000000004]</span><br><span class="line">[0.30000000000000004]</span><br><span class="line">[0.30000000000000004]</span><br><span class="line">[0.30000000000000004]</span><br><span class="line">[0.30000000000000004]</span><br><span class="line">[0.4]</span><br><span class="line">[0.4]</span><br><span class="line">[0.4]</span><br><span class="line">[0.4]</span><br><span class="line">[0.4]</span><br></pre></td></tr></table></figure><h4 id="StepLR"><a href="#StepLR" class="headerlink" title="StepLR"></a>StepLR</h4><blockquote><p>class torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)</p></blockquote><p>每隔<code>step_size</code>次迭代降低<code>gamma</code>倍学习率</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">scheduler = StepLR(optimer, step_size=5, gamma=0.5)</span><br><span class="line"></span><br><span class="line">for epoch in range(20):</span><br><span class="line">    scheduler.step()</span><br><span class="line">    lr = scheduler.get_lr()</span><br><span class="line">    print(lr)</span><br></pre></td></tr></table></figure><p>每轮输出学习率如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[0.1]</span><br><span class="line">[0.1]</span><br><span class="line">[0.1]</span><br><span class="line">[0.1]</span><br><span class="line">[0.1]</span><br><span class="line">[0.05]</span><br><span class="line">[0.05]</span><br><span class="line">[0.05]</span><br><span class="line">[0.05]</span><br><span class="line">[0.05]</span><br><span class="line">[0.025]</span><br><span class="line">[0.025]</span><br><span class="line">[0.025]</span><br><span class="line">[0.025]</span><br><span class="line">[0.025]</span><br><span class="line">[0.0125]</span><br><span class="line">[0.0125]</span><br><span class="line">[0.0125]</span><br><span class="line">[0.0125]</span><br><span class="line">[0.0125]</span><br></pre></td></tr></table></figure><h4 id="MultiStepLR"><a href="#MultiStepLR" class="headerlink" title="MultiStepLR"></a>MultiStepLR</h4><blockquote><p>class torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1)</p></blockquote><p><code>StepLR</code>只能指定固定次数进行衰减，并且衰减会一直持续下去</p><p><code>MultiStepLR</code>可以指定哪个迭代次数进行衰减，并指定衰减次数</p><p><code>milestones</code>是一个升序列表，表示迭代下标，只有当前迭代次数是列表中的值时才衰减一次</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">scheduler = MultiStepLR(optimer, milestones=[3, 5, 10], gamma=0.5)</span><br><span class="line"></span><br><span class="line">for epoch in range(20):</span><br><span class="line">    scheduler.step()</span><br><span class="line">    lr = scheduler.get_lr()</span><br><span class="line">    print(lr)</span><br></pre></td></tr></table></figure><p>每轮输出学习率如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[0.1]</span><br><span class="line">[0.1]</span><br><span class="line">[0.1]</span><br><span class="line">[0.05]</span><br><span class="line">[0.05]</span><br><span class="line">[0.025]</span><br><span class="line">[0.025]</span><br><span class="line">[0.025]</span><br><span class="line">[0.025]</span><br><span class="line">[0.025]</span><br><span class="line">[0.0125]</span><br><span class="line">[0.0125]</span><br><span class="line">[0.0125]</span><br><span class="line">[0.0125]</span><br><span class="line">[0.0125]</span><br><span class="line">[0.0125]</span><br><span class="line">[0.0125]</span><br><span class="line">[0.0125]</span><br><span class="line">[0.0125]</span><br><span class="line">[0.0125]</span><br></pre></td></tr></table></figure><h3 id="指数衰减-1"><a href="#指数衰减-1" class="headerlink" title="指数衰减"></a>指数衰减</h3><blockquote><p>class torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1)</p></blockquote><p>每轮迭代中学习率乘以<code>gamma</code>衰减因子</p></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.jpg" alt="zhujian 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="zhujian 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zhujian</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zhujian.tech/posts/936eda30.html" title="学习率退火">https://www.zhujian.tech/posts/936eda30.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/python/" rel="tag"># python</a> <a href="/tags/pytorch/" rel="tag"># pytorch</a> <a href="/tags/numpy/" rel="tag"># numpy</a> <a href="/tags/matplotlib/" rel="tag"># matplotlib</a> <a href="/tags/learning-rate-decay/" rel="tag"># 学习率衰减</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/a2db6d6b.html" rel="next" title="LeNet5实现-pytorch"><i class="fa fa-chevron-left"></i> LeNet5实现-pytorch</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/posts/2b34c959.html" rel="prev" title="动量更新">动量更新<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#随步数衰减"><span class="nav-number">1.</span> <span class="nav-text">随步数衰减</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#指数衰减"><span class="nav-number">2.</span> <span class="nav-text">指数衰减</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-t衰减"><span class="nav-number">3.</span> <span class="nav-text">1/t衰减</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#衰减比较"><span class="nav-number">4.</span> <span class="nav-text">衰减比较</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Iris分类"><span class="nav-number">5.</span> <span class="nav-text">Iris分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Pytorch实现"><span class="nav-number">6.</span> <span class="nav-text">Pytorch实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#随步长衰减"><span class="nav-number">6.1.</span> <span class="nav-text">随步长衰减</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#LambdaLR"><span class="nav-number">6.1.1.</span> <span class="nav-text">LambdaLR</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#StepLR"><span class="nav-number">6.1.2.</span> <span class="nav-text">StepLR</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MultiStepLR"><span class="nav-number">6.1.3.</span> <span class="nav-text">MultiStepLR</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#指数衰减-1"><span class="nav-number">6.2.</span> <span class="nav-text">指数衰减</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zhujian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">zhujian</p><div class="site-description" itemprop="description">one bite at a time</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">169</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">129</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/./atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zjZSTU" title="Github &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;zjZSTU" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/u012005313" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;u012005313" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i> CSDN</a></span><span class="links-of-author-item"><a href="/mailto:zjzstu@gmail.com" title="Gmail &amp;rarr; mailto:zjzstu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> Gmail</a></span><span class="links-of-author-item"><a href="/mailto:505169307@gmail.com" title="QQmail &amp;rarr; mailto:505169307@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> QQmail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://cloud.tencent.com/" title="https:&#x2F;&#x2F;cloud.tencent.com" rel="noopener" target="_blank">腾讯云</a></li><li class="links-of-blogroll-item"> <a href="https://www.aliyun.com/" title="https:&#x2F;&#x2F;www.aliyun.com" rel="noopener" target="_blank">阿里云</a></li><li class="links-of-blogroll-item"> <a href="https://developer.android.com/" title="https:&#x2F;&#x2F;developer.android.com&#x2F;" rel="noopener" target="_blank">Android Dev</a></li><li class="links-of-blogroll-item"> <a href="https://jenkins.io/" title="https:&#x2F;&#x2F;jenkins.io&#x2F;" rel="noopener" target="_blank">Jenkins</a></li><li class="links-of-blogroll-item"> <a href="http://cs231n.github.io/" title="http:&#x2F;&#x2F;cs231n.github.io&#x2F;" rel="noopener" target="_blank">cs231n</a></li><li class="links-of-blogroll-item"> <a href="https://pytorch.org/docs/stable/index.html" title="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;index.html" rel="noopener" target="_blank">pytorch</a></li><li class="links-of-blogroll-item"> <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" title="https:&#x2F;&#x2F;docs.docker.com&#x2F;install&#x2F;linux&#x2F;docker-ce&#x2F;ubuntu&#x2F;" rel="noopener" target="_blank">docker</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">zhujian</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">948k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">26:20</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-bar-chart-o"></i></span> <a href="https://tongji.baidu.com/web/27249108/overview/index?siteId=13647183" rel="noopener" target="_blank">百度统计</a></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div><div class="beian"> <a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备 19026415号</a> <span class="post-meta-divider">|</span> <img src="/images/beian_icon.png" style="display:inline-block;text-decoration:none;height:13px"> <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001748" rel="noopener" target="_blank">浙公网安备 33011802001748号</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span> <span class="site-uv" title="总访客量">访客数：<span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="site-pv" title="总访问量">阅读量：<span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="17,63,61" opacity="1" zindex="-1" count="199" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '8f22595c6dd29c94467d',
      clientSecret: 'e66bd90ebb9aa66c562c753dec6c90ceecbd51f2',
      repo: 'guestbook',
      owner: 'zjZSTU',
      admin: ['zjZSTU'],
      id: 'bf094b223d26663406c5f2c0d4beb82b',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script></body></html>