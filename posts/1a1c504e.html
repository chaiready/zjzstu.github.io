<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/./atom.xml" title="做一个幸福的人" type="application/atom+xml"><meta name="google-site-verification" content="Qr_3yqLtyErDFKHW7mE8PJz4qDUX-bf_fMLpSRckQe4"><meta name="baidu-site-verification" content="zOwIvKMV7f"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"搜索文章",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet"><meta name="description" content="本文是PyTorch中关于微调CNN的一篇教程，里面利用预训练的Mask R-CNN模型，在PennFudan数据集上进行微调实现"><meta name="keywords" content="python,pytorch,torchvision,PennFudan"><meta property="og:type" content="article"><meta property="og:title" content="[译]TorchVision Object Detection Finetuning Tutorial"><meta property="og:url" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;posts&#x2F;1a1c504e.html"><meta property="og:site_name" content="做一个幸福的人"><meta property="og:description" content="本文是PyTorch中关于微调CNN的一篇教程，里面利用预训练的Mask R-CNN模型，在PennFudan数据集上进行微调实现"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;译-finetune&#x2F;tv_image02.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;译-finetune&#x2F;tv_image03.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;译-finetune&#x2F;tv_image04.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;译-finetune&#x2F;tv_image05.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;译-finetune&#x2F;tv_image07.png"><meta property="og:updated_time" content="2020-02-27T02:15:36.948Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;译-finetune&#x2F;tv_image02.png"><link rel="canonical" href="https://www.zhujian.tech/posts/1a1c504e.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>[译]TorchVision Object Detection Finetuning Tutorial | 做一个幸福的人</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e677aac1ac69b8826b9cfecb4e72e107";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">做一个幸福的人</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">面朝大海，春暖花开</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">129</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">48</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">169</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header> <a href="https://github.com/zjZSTU" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zhujian.tech/posts/1a1c504e.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="zhujian"><meta itemprop="description" content="one bite at a time"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="做一个幸福的人"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> [译]TorchVision Object Detection Finetuning Tutorial</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-02-26 10:17:57" itemprop="dateCreated datePublished" datetime="2020-02-26T10:17:57+00:00">2020-02-26</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-27 02:15:36" itemprop="dateModified" datetime="2020-02-27T02:15:36+00:00">2020-02-27</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">编程</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/programming-language/" itemprop="url" rel="index"><span itemprop="name">编程语言</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/data/" itemprop="url" rel="index"><span itemprop="name">数据</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/codebase/" itemprop="url" rel="index"><span itemprop="name">代码库</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/data/dataset/" itemprop="url" rel="index"><span itemprop="name">数据集</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/translation/" itemprop="url" rel="index"><span itemprop="name">翻译</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>20k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>34 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>本文是PyTorch中关于微调CNN的一篇教程，里面利用预训练的Mask R-CNN模型，在PennFudan数据集上进行微调实现</p><a id="more"></a><p>原文地址：<a href="https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html" target="_blank" rel="noopener">TorchVision Object Detection Finetuning Tutorial</a></p><p>本文章涉及脚本位于仓库<a href="https://github.com/pytorch/vision" target="_blank" rel="noopener">pytorch/vision</a></p><blockquote><p>For this tutorial, we will be finetuning a pre-trained Mask R-CNN model in the Penn-Fudan Database for Pedestrian Detection and Segmentation. It contains 170 images with 345 instances of pedestrians, and we will use it to illustrate how to use the new features in torchvision in order to train an instance segmentation model on a custom dataset.</p></blockquote><p>在本教程中，我们将在Penn-Fudan的行人检测和分割数据库中微调一个预先训练好的Mask R-CNN模型。数据库包含170幅共345个行人实例的图像，我们将用它来演示如何使用torchvision的新特性，以便在自定义数据集上训练一个实例分割模型</p><h2 id="Defining-the-Dataset"><a href="#Defining-the-Dataset" class="headerlink" title="Defining the Dataset"></a>Defining the Dataset</h2><p>定义数据集</p><blockquote><p>The reference scripts for training object detection, instance segmentation and person keypoint detection allows for easily supporting adding new custom datasets. The dataset should inherit from the standard <code>torch.utils.data.Dataset</code> class, and implement <code>__len__</code> and <code>__getitem__</code>.</p></blockquote><p>用于训练目标检测、实例分割和人物关键点检测的参考脚本能够很容易的添加新的定制数据集。数据集应该继承自标准的<code>torch.utils.data.Dataset</code>类，并实现方法<code>__len__</code>和<code>__getitem__</code></p><blockquote><p>The only specificity that we require is that the dataset <code>__getitem__</code> should return:</p><ul><li><code>image</code>: a PIL Image of size <code>(H, W)</code></li><li><code>target</code>: a dict containing the following fields<ul><li><code>boxes (FloatTensor[N, 4])</code>: the coordinates of the N bounding boxes in [x0, y0, x1, y1] format, ranging from 0 to W and 0 to H</li><li><code>labels (Int64Tensor[N])</code>: the label for each bounding box</li><li><code>image_id (Int64Tensor[1])</code>: an image identifier. It should be unique between all the images in the dataset, and is used during evaluation</li><li><code>area (Tensor[N])</code>: The area of the bounding box. This is used during evaluation with the COCO metric, to separate the metric scores between small, medium and large boxes.</li><li><code>iscrowd (UInt8Tensor[N])</code>: instances with iscrowd=True will be ignored during evaluation.</li><li><code>(optionally) masks (UInt8Tensor[N, H, W])</code>: The segmentation masks for each one of the objects</li><li><code>(optionally) keypoints (FloatTensor[N, K, 3])</code>: For each one of the N objects, it contains the K keypoints in [x, y, visibility] format, defining the object. visibility=0 means that the keypoint is not visible. Note that for data augmentation, the notion of flipping a keypoint is dependent on the data representation, and you should probably adapt references/detection/transforms.py for your new keypoint representation</li></ul></li></ul></blockquote><p>我们唯一需要的特性是数据集<code>__getitem__</code>应该返回:</p><ul><li><code>image</code>：大小为<code>(H, W)</code>的PIL Image</li><li><code>target</code>：包含以下字段的dict<ul><li><code>boxes (FloatTensor[N, 4])</code>：N表示边界框数目，4表示边界框格式，分别为[x0, y0, x1, y1]，宽度取值为[0, W]，长度取值为[0, H]</li><li><code>labels (Int64Tensor[N])</code>：每个边界框的标签</li><li><code>image_id (Int64Tensor[1])</code>：图像标识符</li><li><code>area (Tensor[N])</code>：边界框面积。这在使用COCO指标进行评估时使用，用于区分小、中、大框之间的指标得分</li><li><code>iscrowd (UInt8Tensor[N])</code>：iscrowd=True的实例将在评估期间被忽略</li><li>(可选) <code>masks (UInt8Tensor[N, H, W])</code>：每个目标的分割掩码</li><li>(可选) <code>keypoints (FloatTensor[N, K, 3])</code>：对于N个对象中的每一个，它包含[x, y, visibility]格式的K个关键点，用于定义对象。visibility=0表示关键点不可见。注意，对于数据扩充，翻转关键点的概念取决于数据表示，你应该为新的关键点表示调整references/detection/transforms.py</li></ul></li></ul><blockquote><p>If your model returns the above methods, they will make it work for both training and evaluation, and will use the evaluation scripts from pycocotools.</p></blockquote><p>自定义的上述方法能够既适用于训练也适用于评估，其中评估使用的是来自pycocotools的评估脚本</p><blockquote><p>Additionally, if you want to use aspect ratio grouping during training (so that each batch only contains images with similar aspect ratio), then it is recommended to also implement a get_height_and_width method, which returns the height and the width of the image. If this method is not provided, we query all elements of the dataset via <code>__getitem__</code> , which loads the image in memory and is slower than if a custom method is provided.</p></blockquote><p>此外，如果你希望在训练过程中使用纵横比分组(以便每一批只包含具有相似纵横比的图像)，那么最好实现方法get_height_and_width，该方法返回图像的高度和宽度。如果不提供此方法，我们通过<code>__getitem__</code>查询数据集的所有元素，这会将图像加载到内存中，比自定义方法还要慢</p><h2 id="Writing-a-custom-dataset-for-PennFudan"><a href="#Writing-a-custom-dataset-for-PennFudan" class="headerlink" title="Writing a custom dataset for PennFudan"></a>Writing a custom dataset for PennFudan</h2><p>自定义PennFudan数据集类</p><blockquote><p>Let’s write a dataset for the PennFudan dataset. After downloading and extracting the zip file, we have the following folder structure:</p></blockquote><p>实现PennFudan数据类，先<a href="https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip" target="_blank" rel="noopener">下载和解压zip文件</a>，其文件结构如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">PennFudanPed/</span><br><span class="line">  PedMasks/</span><br><span class="line">    FudanPed00001_mask.png</span><br><span class="line">    FudanPed00002_mask.png</span><br><span class="line">    FudanPed00003_mask.png</span><br><span class="line">    FudanPed00004_mask.png</span><br><span class="line">    ...</span><br><span class="line">  PNGImages/</span><br><span class="line">    FudanPed00001.png</span><br><span class="line">    FudanPed00002.png</span><br><span class="line">    FudanPed00003.png</span><br><span class="line">    FudanPed00004.png</span><br></pre></td></tr></table></figure><blockquote><p>Here is one example of a pair of images and segmentation masks</p></blockquote><p>下面是一对图像和分割掩码的示例</p><p><img src="/imgs/译-finetune/tv_image02.png" alt></p><blockquote><p>So each image has a corresponding segmentation mask, where each color correspond to a different instance. Let’s write a torch.utils.data.Dataset class for this dataset.</p></blockquote><p>每个图像有一个对应的分割掩码，其中不同的颜色表示不同的实例。实现数据类（继承自torch.utils.data.Dataset）如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line">import os</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">from PIL import Image</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class PennFudanDataset(object):</span><br><span class="line">    def __init__(self, root, transforms):</span><br><span class="line">        self.root = root</span><br><span class="line">        self.transforms = transforms</span><br><span class="line"></span><br><span class="line">        # load all image files, sorting them to</span><br><span class="line">        # ensure that they are aligned</span><br><span class="line">        # 通过排序确保图像和掩码文件一一对应</span><br><span class="line">        self.imgs = list(sorted(os.listdir(os.path.join(root, &quot;PNGImages&quot;))))</span><br><span class="line">        self.masks = list(sorted(os.listdir(os.path.join(root, &quot;PedMasks&quot;))))</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, idx):</span><br><span class="line">        # load images ad masks</span><br><span class="line">        # 加载图像和掩码</span><br><span class="line">        img_path = os.path.join(self.root, &quot;PNGImages&quot;, self.imgs[idx])</span><br><span class="line">        mask_path = os.path.join(self.root, &quot;PedMasks&quot;, self.masks[idx])</span><br><span class="line">        img = Image.open(img_path).convert(&quot;RGB&quot;)</span><br><span class="line">        # note that we haven&apos;t converted the mask to RGB,</span><br><span class="line">        # because each color corresponds to a different instance</span><br><span class="line">        # with 0 being background</span><br><span class="line">        # 其实也可以使用cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)</span><br><span class="line">        mask = Image.open(mask_path)</span><br><span class="line">        # convert the PIL Image into a numpy array</span><br><span class="line">        mask = np.array(mask)</span><br><span class="line">        # instances are encoded as different colors</span><br><span class="line">        obj_ids = np.unique(mask)</span><br><span class="line">        # first id is the background, so remove it</span><br><span class="line">        # 第一个表示</span><br><span class="line">        obj_ids = obj_ids[1:]</span><br><span class="line"></span><br><span class="line">        # split the color-encoded mask into a set</span><br><span class="line">        # of binary masks</span><br><span class="line">        # 针对不同的行人实例创建对应的掩码</span><br><span class="line">        masks = mask == obj_ids[:, None, None]</span><br><span class="line"></span><br><span class="line">        # get bounding box coordinates for each mask</span><br><span class="line">        # 计算每个掩码的边界框坐标</span><br><span class="line">        num_objs = len(obj_ids)</span><br><span class="line">        boxes = []</span><br><span class="line">        for i in range(num_objs):</span><br><span class="line">            pos = np.where(masks[i])</span><br><span class="line">            xmin = np.min(pos[1])</span><br><span class="line">            xmax = np.max(pos[1])</span><br><span class="line">            ymin = np.min(pos[0])</span><br><span class="line">            ymax = np.max(pos[0])</span><br><span class="line">            boxes.append([xmin, ymin, xmax, ymax])</span><br><span class="line"></span><br><span class="line">        # convert everything into a torch.Tensor</span><br><span class="line">        boxes = torch.as_tensor(boxes, dtype=torch.float32)</span><br><span class="line">        # there is only one class</span><br><span class="line">        labels = torch.ones((num_objs,), dtype=torch.int64)</span><br><span class="line">        masks = torch.as_tensor(masks, dtype=torch.uint8)</span><br><span class="line"></span><br><span class="line">        image_id = torch.tensor([idx])</span><br><span class="line">        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])</span><br><span class="line">        # suppose all instances are not crowd</span><br><span class="line">        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)</span><br><span class="line"></span><br><span class="line">        target = &#123;&#125;</span><br><span class="line">        target[&quot;boxes&quot;] = boxes</span><br><span class="line">        target[&quot;labels&quot;] = labels</span><br><span class="line">        target[&quot;masks&quot;] = masks</span><br><span class="line">        target[&quot;image_id&quot;] = image_id</span><br><span class="line">        target[&quot;area&quot;] = area</span><br><span class="line">        target[&quot;iscrowd&quot;] = iscrowd</span><br><span class="line"></span><br><span class="line">        if self.transforms is not None:</span><br><span class="line">            img, target = self.transforms(img, target)</span><br><span class="line"></span><br><span class="line">        return img, target</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return len(self.imgs)</span><br></pre></td></tr></table></figure><blockquote><p>That’s all for the dataset. Now let’s define a model that can perform predictions on this dataset.</p></blockquote><p>上述就是自定义数据类。下面定义一个模型，在上述数据集上进行预测</p><h2 id="Defining-your-model"><a href="#Defining-your-model" class="headerlink" title="Defining your model"></a>Defining your model</h2><p>定义模型</p><blockquote><p>In this tutorial, we will be using Mask R-CNN, which is based on top of Faster R-CNN. Faster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image.</p></blockquote><p>本教材使用<a href="https://arxiv.org/abs/1703.06870" target="_blank" rel="noopener">Mask R-CNN</a>，该模型基于<a href="https://arxiv.org/abs/1506.01497" target="_blank" rel="noopener">Faster R-CNN</a>。Faster R-CNN能够同时预测图像中潜在目标的预测框和类成绩</p><p><img src="/imgs/译-finetune/tv_image03.png" alt></p><blockquote><p>Mask R-CNN adds an extra branch into Faster R-CNN, which also predicts segmentation masks for each instance.</p></blockquote><p>Mask R-CNN在Faster R-CNN的基础上增加了一个额外的功能，就是同时能够预测每个实例的分割掩码</p><p><img src="/imgs/译-finetune/tv_image04.png" alt></p><blockquote><p>There are two common situations where one might want to modify one of the available models in torchvision modelzoo. The first is when we want to start from a pre-trained model, and just finetune the last layer. The other is when we want to replace the backbone of the model with a different one (for faster predictions, for example).</p></blockquote><p>当人们想要修改torchvision modelzoo中的可用模型时，有两种常见的情况。第一种是当我们想要使用一个预训练模型时，只需要微调最后一层。第二种是我们想用一个不同的模型来替换这个模型的主干（比如为了更快的预测）</p><blockquote><p>Let’s go see how we would do one or another in the following sections.</p></blockquote><p>让我们来看看在接下来的几节中会如何做</p><h3 id="1-Finetuning-from-a-pretrained-model"><a href="#1-Finetuning-from-a-pretrained-model" class="headerlink" title="1 - Finetuning from a pretrained model"></a>1 - Finetuning from a pretrained model</h3><p>1 - 对预处理模型进行微调</p><blockquote><p>Let’s suppose that you want to start from a model pre-trained on COCO and want to finetune it for your particular classes. Here is a possible way of doing it:</p></blockquote><p>假设您想从一个在COCO上预训练的模型开始，并想针对特定类对其进行微调。这里有一个可行的方法:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torchvision.models.detection.faster_rcnn import FastRCNNPredictor</span><br><span class="line"></span><br><span class="line"># load a model pre-trained pre-trained on COCO</span><br><span class="line"># 加载在COCO上预训练的模型</span><br><span class="line">model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)</span><br><span class="line"></span><br><span class="line"># replace the classifier with a new one, that has</span><br><span class="line"># num_classes which is user-defined</span><br><span class="line"># 替换新的分类器</span><br><span class="line">num_classes = 2  # 1 class (person) + background</span><br><span class="line"># get number of input features for the classifier</span><br><span class="line"># 获取分类器的输入特征数</span><br><span class="line">in_features = model.roi_heads.box_predictor.cls_score.in_features</span><br><span class="line"># replace the pre-trained head with a new one</span><br><span class="line">model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)</span><br></pre></td></tr></table></figure><h3 id="2-Modifying-the-model-to-add-a-different-backbone"><a href="#2-Modifying-the-model-to-add-a-different-backbone" class="headerlink" title="2 - Modifying the model to add a different backbone"></a>2 - Modifying the model to add a different backbone</h3><p>2 - 通过添加不同的主干来修改模型</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torchvision.models.detection import FasterRCNN</span><br><span class="line">from torchvision.models.detection.rpn import AnchorGenerator</span><br><span class="line"></span><br><span class="line"># load a pre-trained model for classification and return</span><br><span class="line"># only the features</span><br><span class="line">backbone = torchvision.models.mobilenet_v2(pretrained=True).features</span><br><span class="line"># FasterRCNN needs to know the number of</span><br><span class="line"># output channels in a backbone. For mobilenet_v2, it&apos;s 1280</span><br><span class="line"># so we need to add it here</span><br><span class="line">backbone.out_channels = 1280</span><br><span class="line"></span><br><span class="line"># let&apos;s make the RPN generate 5 x 3 anchors per spatial</span><br><span class="line"># location, with 5 different sizes and 3 different aspect</span><br><span class="line"># ratios. We have a Tuple[Tuple[int]] because each feature</span><br><span class="line"># map could potentially have different sizes and</span><br><span class="line"># aspect ratios</span><br><span class="line">anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),</span><br><span class="line">                                   aspect_ratios=((0.5, 1.0, 2.0),))</span><br><span class="line"></span><br><span class="line"># let&apos;s define what are the feature maps that we will</span><br><span class="line"># use to perform the region of interest cropping, as well as</span><br><span class="line"># the size of the crop after rescaling.</span><br><span class="line"># if your backbone returns a Tensor, featmap_names is expected to</span><br><span class="line"># be [0]. More generally, the backbone should return an</span><br><span class="line"># OrderedDict[Tensor], and in featmap_names you can choose which</span><br><span class="line"># feature maps to use.</span><br><span class="line">roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0],</span><br><span class="line">                                                output_size=7,</span><br><span class="line">                                                sampling_ratio=2)</span><br><span class="line"></span><br><span class="line"># put the pieces together inside a FasterRCNN model</span><br><span class="line">model = FasterRCNN(backbone,</span><br><span class="line">                   num_classes=2,</span><br><span class="line">                   rpn_anchor_generator=anchor_generator,</span><br><span class="line">                   box_roi_pool=roi_pooler)</span><br></pre></td></tr></table></figure><h3 id="An-Instance-segmentation-model-for-PennFudan-Dataset"><a href="#An-Instance-segmentation-model-for-PennFudan-Dataset" class="headerlink" title="An Instance segmentation model for PennFudan Dataset"></a>An Instance segmentation model for PennFudan Dataset</h3><p>PennFudan数据集的实例分割模型</p><blockquote><p>In our case, we want to fine-tune from a pre-trained model, given that our dataset is very small, so we will be following approach number 1.</p></blockquote><p>在我们的例子中，我们希望从预先训练好的模型中进行微调，因为我们的数据集非常小，所以我们将遵循方法1</p><blockquote><p>Here we want to also compute the instance segmentation masks, so we will be using Mask R-CNN:</p></blockquote><p>为了能够同时计算实例分割掩码，所以使用Mask R-CNN：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import torchvision</span><br><span class="line">from torchvision.models.detection.faster_rcnn import FastRCNNPredictor</span><br><span class="line">from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_model_instance_segmentation(num_classes):</span><br><span class="line">    # load an instance segmentation model pre-trained pre-trained on COCO</span><br><span class="line">    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)</span><br><span class="line"></span><br><span class="line">    # get number of input features for the classifier</span><br><span class="line">    in_features = model.roi_heads.box_predictor.cls_score.in_features</span><br><span class="line">    # replace the pre-trained head with a new one</span><br><span class="line">    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)</span><br><span class="line"></span><br><span class="line">    # now get the number of input features for the mask classifier</span><br><span class="line">    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels</span><br><span class="line">    hidden_layer = 256</span><br><span class="line">    # and replace the mask predictor with a new one</span><br><span class="line">    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,</span><br><span class="line">                                                       hidden_layer,</span><br><span class="line">                                                       num_classes)</span><br><span class="line"></span><br><span class="line">    return model</span><br></pre></td></tr></table></figure><blockquote><p>That’s it, this will make model be ready to be trained and evaluated on your custom dataset.</p></blockquote><p>上述代码就能够实现自定义数据集的训练和评估</p><h2 id="Putting-everything-together"><a href="#Putting-everything-together" class="headerlink" title="Putting everything together"></a>Putting everything together</h2><p>完整实现</p><blockquote><p>In references/detection/, we have a number of helper functions to simplify training and evaluating detection models. Here, we will use references/detection/engine.py, references/detection/utils.py and references/detection/transforms.py. Just copy them to your folder and use them here.</p></blockquote><p>在references/detection/目录中提供了许多能够简化训练和评估检测模型的辅助函数。在本次实验中，使用了references/detection/engine.py、references/detection/utils.py和references/detection/transforms.py</p><blockquote><p>Let’s write some helper functions for data augmentation / transformation:</p></blockquote><p>实现一些用于数据扩充/转换的辅助函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import transforms as T</span><br><span class="line"></span><br><span class="line">def get_transform(train):</span><br><span class="line">    transforms = []</span><br><span class="line">    transforms.append(T.ToTensor())</span><br><span class="line">    if train:</span><br><span class="line">        transforms.append(T.RandomHorizontalFlip(0.5))</span><br><span class="line">    return T.Compose(transforms)</span><br></pre></td></tr></table></figure><h2 id="Testing-forward-method-Optional"><a href="#Testing-forward-method-Optional" class="headerlink" title="Testing forward() method (Optional)"></a>Testing forward() method (Optional)</h2><blockquote><p>Before iterating over the dataset, it’s good to see what the model expects during training and inference time on sample data.</p></blockquote><p>在迭代数据集之前，最好能知道模型在样本数据的训练和推理时中需要什么</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)</span><br><span class="line">dataset = PennFudanDataset(&apos;PennFudanPed&apos;, get_transform(train=True))</span><br><span class="line">data_loader = torch.utils.data.DataLoader(</span><br><span class="line"> dataset, batch_size=2, shuffle=True, num_workers=4,</span><br><span class="line"> collate_fn=utils.collate_fn)</span><br><span class="line"># For Training</span><br><span class="line">images,targets = next(iter(data_loader))</span><br><span class="line">images = list(image for image in images)</span><br><span class="line">targets = [&#123;k: v for k, v in t.items()&#125; for t in targets]</span><br><span class="line">output = model(images,targets)   # Returns losses and detections</span><br><span class="line"># For inference</span><br><span class="line">model.eval()</span><br><span class="line">x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]</span><br><span class="line">predictions = model(x)           # Returns predictions</span><br></pre></td></tr></table></figure><blockquote><p>Let’s now write the main function which performs the training and the validation:</p></blockquote><p>下面编写执行训练和验证的主函数:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">from engine import train_one_epoch, evaluate</span><br><span class="line">import utils</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def main():</span><br><span class="line">    # train on the GPU or on the CPU, if a GPU is not available</span><br><span class="line">    device = torch.device(&apos;cuda&apos;) if torch.cuda.is_available() else torch.device(&apos;cpu&apos;)</span><br><span class="line"></span><br><span class="line">    # our dataset has two classes only - background and person</span><br><span class="line">    num_classes = 2</span><br><span class="line">    # use our dataset and defined transformations</span><br><span class="line">    dataset = PennFudanDataset(&apos;PennFudanPed&apos;, get_transform(train=True))</span><br><span class="line">    dataset_test = PennFudanDataset(&apos;PennFudanPed&apos;, get_transform(train=False))</span><br><span class="line"></span><br><span class="line">    # split the dataset in train and test set</span><br><span class="line">    indices = torch.randperm(len(dataset)).tolist()</span><br><span class="line">    dataset = torch.utils.data.Subset(dataset, indices[:-50])</span><br><span class="line">    dataset_test = torch.utils.data.Subset(dataset_test, indices[-50:])</span><br><span class="line"></span><br><span class="line">    # define training and validation data loaders</span><br><span class="line">    data_loader = torch.utils.data.DataLoader(</span><br><span class="line">        dataset, batch_size=2, shuffle=True, num_workers=4,</span><br><span class="line">        collate_fn=utils.collate_fn)</span><br><span class="line"></span><br><span class="line">    data_loader_test = torch.utils.data.DataLoader(</span><br><span class="line">        dataset_test, batch_size=1, shuffle=False, num_workers=4,</span><br><span class="line">        collate_fn=utils.collate_fn)</span><br><span class="line"></span><br><span class="line">    # get the model using our helper function</span><br><span class="line">    model = get_model_instance_segmentation(num_classes)</span><br><span class="line"></span><br><span class="line">    # move model to the right device</span><br><span class="line">    model.to(device)</span><br><span class="line"></span><br><span class="line">    # construct an optimizer</span><br><span class="line">    params = [p for p in model.parameters() if p.requires_grad]</span><br><span class="line">    optimizer = torch.optim.SGD(params, lr=0.005,</span><br><span class="line">                                momentum=0.9, weight_decay=0.0005)</span><br><span class="line">    # and a learning rate scheduler</span><br><span class="line">    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,</span><br><span class="line">                                                   step_size=3,</span><br><span class="line">                                                   gamma=0.1)</span><br><span class="line"></span><br><span class="line">    # let&apos;s train it for 10 epochs</span><br><span class="line">    num_epochs = 10</span><br><span class="line"></span><br><span class="line">    for epoch in range(num_epochs):</span><br><span class="line">        # train for one epoch, printing every 10 iterations</span><br><span class="line">        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)</span><br><span class="line">        # update the learning rate</span><br><span class="line">        lr_scheduler.step()</span><br><span class="line">        # evaluate on the test dataset</span><br><span class="line">        evaluate(model, data_loader_test, device=device)</span><br><span class="line"></span><br><span class="line">    print(&quot;That&apos;s it!&quot;)</span><br></pre></td></tr></table></figure><blockquote><p>You should get as output for the first epoch:</p></blockquote><p>第一轮预期输出如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">Epoch: [0]  [ 0/60]  eta: 0:01:18  lr: 0.000090  loss: 2.5213 (2.5213)  loss_classifier: 0.8025 (0.8025)  loss_box_reg: 0.2634 (0.2634)  loss_mask: 1.4265 (1.4265)  loss_objectness: 0.0190 (0.0190)  loss_rpn_box_reg: 0.0099 (0.0099)  time: 1.3121  data: 0.3024  max mem: 3485</span><br><span class="line">Epoch: [0]  [10/60]  eta: 0:00:20  lr: 0.000936  loss: 1.3007 (1.5313)  loss_classifier: 0.3979 (0.4719)  loss_box_reg: 0.2454 (0.2272)  loss_mask: 0.6089 (0.7953)  loss_objectness: 0.0197 (0.0228)  loss_rpn_box_reg: 0.0121 (0.0141)  time: 0.4198  data: 0.0298  max mem: 5081</span><br><span class="line">Epoch: [0]  [20/60]  eta: 0:00:15  lr: 0.001783  loss: 0.7567 (1.1056)  loss_classifier: 0.2221 (0.3319)  loss_box_reg: 0.2002 (0.2106)  loss_mask: 0.2904 (0.5332)  loss_objectness: 0.0146 (0.0176)  loss_rpn_box_reg: 0.0094 (0.0123)  time: 0.3293  data: 0.0035  max mem: 5081</span><br><span class="line">Epoch: [0]  [30/60]  eta: 0:00:11  lr: 0.002629  loss: 0.4705 (0.8935)  loss_classifier: 0.0991 (0.2517)  loss_box_reg: 0.1578 (0.1957)  loss_mask: 0.1970 (0.4204)  loss_objectness: 0.0061 (0.0140)  loss_rpn_box_reg: 0.0075 (0.0118)  time: 0.3403  data: 0.0044  max mem: 5081</span><br><span class="line">Epoch: [0]  [40/60]  eta: 0:00:07  lr: 0.003476  loss: 0.3901 (0.7568)  loss_classifier: 0.0648 (0.2022)  loss_box_reg: 0.1207 (0.1736)  loss_mask: 0.1705 (0.3585)  loss_objectness: 0.0018 (0.0113)  loss_rpn_box_reg: 0.0075 (0.0112)  time: 0.3407  data: 0.0044  max mem: 5081</span><br><span class="line">Epoch: [0]  [50/60]  eta: 0:00:03  lr: 0.004323  loss: 0.3237 (0.6703)  loss_classifier: 0.0474 (0.1731)  loss_box_reg: 0.1109 (0.1561)  loss_mask: 0.1658 (0.3201)  loss_objectness: 0.0015 (0.0093)  loss_rpn_box_reg: 0.0093 (0.0116)  time: 0.3379  data: 0.0043  max mem: 5081</span><br><span class="line">Epoch: [0]  [59/60]  eta: 0:00:00  lr: 0.005000  loss: 0.2540 (0.6082)  loss_classifier: 0.0309 (0.1526)  loss_box_reg: 0.0463 (0.1405)  loss_mask: 0.1568 (0.2945)  loss_objectness: 0.0012 (0.0083)  loss_rpn_box_reg: 0.0093 (0.0123)  time: 0.3489  data: 0.0042  max mem: 5081</span><br><span class="line">Epoch: [0] Total time: 0:00:21 (0.3570 s / it)</span><br><span class="line">creating index...</span><br><span class="line">index created!</span><br><span class="line">Test:  [ 0/50]  eta: 0:00:19  model_time: 0.2152 (0.2152)  evaluator_time: 0.0133 (0.0133)  time: 0.4000  data: 0.1701  max mem: 5081</span><br><span class="line">Test:  [49/50]  eta: 0:00:00  model_time: 0.0628 (0.0687)  evaluator_time: 0.0039 (0.0064)  time: 0.0735  data: 0.0022  max mem: 5081</span><br><span class="line">Test: Total time: 0:00:04 (0.0828 s / it)</span><br><span class="line">Averaged stats: model_time: 0.0628 (0.0687)  evaluator_time: 0.0039 (0.0064)</span><br><span class="line">Accumulating evaluation results...</span><br><span class="line">DONE (t=0.01s).</span><br><span class="line">Accumulating evaluation results...</span><br><span class="line">DONE (t=0.01s).</span><br><span class="line">IoU metric: bbox</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.606</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.984</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.780</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.313</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.582</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.612</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.270</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.672</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.672</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.755</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.664</span><br><span class="line">IoU metric: segm</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.704</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.979</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.871</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.325</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.488</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.727</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.316</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.748</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.749</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.650</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.673</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.758</span><br></pre></td></tr></table></figure><blockquote><p>So after one epoch of training, we obtain a COCO-style mAP of 60.6, and a mask mAP of 70.4.</p></blockquote><p>经过一个周期的训练后，获得了60.6的COCO风格mAP和70.4的掩码mAP</p><blockquote><p>After training for 10 epochs, I got the following metrics</p></blockquote><p>经过10个周期的训练后，得到以下指标</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">IoU metric: bbox</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.799</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.969</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.935</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.349</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.592</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.831</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.324</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.844</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.844</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.400</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.777</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.870</span><br><span class="line">IoU metric: segm</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.761</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.969</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.919</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.341</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.464</span><br><span class="line"> Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.788</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.303</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.799</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.799</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.400</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.769</span><br><span class="line"> Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.818</span><br></pre></td></tr></table></figure><blockquote><p>But what do the predictions look like? Let’s take one image in the dataset and verify</p></blockquote><p>实际预测是什么样的呢？使用一张图像进行验证</p><p><img src="/imgs/译-finetune/tv_image05.png" alt></p><blockquote><p>The trained model predicts 9 instances of person in this image, let’s see a couple of them:</p></blockquote><p>经过训练的模型预测了该图像中的9个人物实例，让我们来看看其中的几个:</p><p><img src="/imgs/译-finetune/tv_image07.png" alt></p><blockquote><p>The results look pretty good!</p></blockquote><p>结果确实很好！</p><h2 id="Wrapping-up"><a href="#Wrapping-up" class="headerlink" title="Wrapping up"></a>Wrapping up</h2><p>小结</p><blockquote><p>In this tutorial, you have learned how to create your own training pipeline for instance segmentation models, on a custom dataset. For that, you wrote a torch.utils.data.Dataset class that returns the images and the ground truth boxes and segmentation masks. You also leveraged a Mask R-CNN model pre-trained on COCO train2017 in order to perform transfer learning on this new dataset.</p></blockquote><p>在本教程中，您已经学习了如何在自定义数据集上为实例分割模型创建自己的训练流程。为此需要编写了一个torch.utils.data.Dataset类，该类返回image、标注边界框和分割掩码。您还利用了在COCO 2017 上预训练的一个Mask R-CNN模型，以便在这个新数据集上执行迁移学习</p><blockquote><p>For a more complete example, which includes multi-machine / multi-gpu training, check references/detection/train.py, which is present in the torchvision repo.</p></blockquote><p>更完整的例子，包括多机器/多gpu训练，check references/detection/train.py，均在torchvision仓库中存在</p><blockquote><p>You can download a full source file for this tutorial here.</p></blockquote><p>本教材完整程序下载：<a href="https://pytorch.org/tutorials/_static/tv-training-code.py" target="_blank" rel="noopener">tv-training-code.py</a></p></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.jpg" alt="zhujian 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="zhujian 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zhujian</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zhujian.tech/posts/1a1c504e.html" title="[译]TorchVision Object Detection Finetuning Tutorial">https://www.zhujian.tech/posts/1a1c504e.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/python/" rel="tag"># python</a> <a href="/tags/pytorch/" rel="tag"># pytorch</a> <a href="/tags/torchvision/" rel="tag"># torchvision</a> <a href="/tags/PennFudan/" rel="tag"># PennFudan</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/b738146b.html" rel="next" title="C++11实践"><i class="fa fa-chevron-left"></i> C++11实践</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/posts/6c61a203.html" rel="prev" title="[数据集]Penn-Fudan">[数据集]Penn-Fudan<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Defining-the-Dataset"><span class="nav-number">1.</span> <span class="nav-text">Defining the Dataset</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Writing-a-custom-dataset-for-PennFudan"><span class="nav-number">2.</span> <span class="nav-text">Writing a custom dataset for PennFudan</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Defining-your-model"><span class="nav-number">3.</span> <span class="nav-text">Defining your model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Finetuning-from-a-pretrained-model"><span class="nav-number">3.1.</span> <span class="nav-text">1 - Finetuning from a pretrained model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Modifying-the-model-to-add-a-different-backbone"><span class="nav-number">3.2.</span> <span class="nav-text">2 - Modifying the model to add a different backbone</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#An-Instance-segmentation-model-for-PennFudan-Dataset"><span class="nav-number">3.3.</span> <span class="nav-text">An Instance segmentation model for PennFudan Dataset</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Putting-everything-together"><span class="nav-number">4.</span> <span class="nav-text">Putting everything together</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Testing-forward-method-Optional"><span class="nav-number">5.</span> <span class="nav-text">Testing forward() method (Optional)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Wrapping-up"><span class="nav-number">6.</span> <span class="nav-text">Wrapping up</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zhujian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">zhujian</p><div class="site-description" itemprop="description">one bite at a time</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">169</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">129</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/./atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zjZSTU" title="Github &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;zjZSTU" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/u012005313" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;u012005313" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i> CSDN</a></span><span class="links-of-author-item"><a href="/mailto:zjzstu@gmail.com" title="Gmail &amp;rarr; mailto:zjzstu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> Gmail</a></span><span class="links-of-author-item"><a href="/mailto:505169307@gmail.com" title="QQmail &amp;rarr; mailto:505169307@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> QQmail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://cloud.tencent.com/" title="https:&#x2F;&#x2F;cloud.tencent.com" rel="noopener" target="_blank">腾讯云</a></li><li class="links-of-blogroll-item"> <a href="https://www.aliyun.com/" title="https:&#x2F;&#x2F;www.aliyun.com" rel="noopener" target="_blank">阿里云</a></li><li class="links-of-blogroll-item"> <a href="https://developer.android.com/" title="https:&#x2F;&#x2F;developer.android.com&#x2F;" rel="noopener" target="_blank">Android Dev</a></li><li class="links-of-blogroll-item"> <a href="https://jenkins.io/" title="https:&#x2F;&#x2F;jenkins.io&#x2F;" rel="noopener" target="_blank">Jenkins</a></li><li class="links-of-blogroll-item"> <a href="http://cs231n.github.io/" title="http:&#x2F;&#x2F;cs231n.github.io&#x2F;" rel="noopener" target="_blank">cs231n</a></li><li class="links-of-blogroll-item"> <a href="https://pytorch.org/docs/stable/index.html" title="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;index.html" rel="noopener" target="_blank">pytorch</a></li><li class="links-of-blogroll-item"> <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" title="https:&#x2F;&#x2F;docs.docker.com&#x2F;install&#x2F;linux&#x2F;docker-ce&#x2F;ubuntu&#x2F;" rel="noopener" target="_blank">docker</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">zhujian</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">948k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">26:20</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-bar-chart-o"></i></span> <a href="https://tongji.baidu.com/web/27249108/overview/index?siteId=13647183" rel="noopener" target="_blank">百度统计</a></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div><div class="beian"> <a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备 19026415号</a> <span class="post-meta-divider">|</span> <img src="/images/beian_icon.png" style="display:inline-block;text-decoration:none;height:13px"> <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001748" rel="noopener" target="_blank">浙公网安备 33011802001748号</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span> <span class="site-uv" title="总访客量">访客数：<span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="site-pv" title="总访问量">阅读量：<span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="17,63,61" opacity="1" zindex="-1" count="199" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '8f22595c6dd29c94467d',
      clientSecret: 'e66bd90ebb9aa66c562c753dec6c90ceecbd51f2',
      repo: 'guestbook',
      owner: 'zjZSTU',
      admin: ['zjZSTU'],
      id: 'd78c6d1a640f1f5152ebd3703ae47cc4',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script></body></html>