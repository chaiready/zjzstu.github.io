<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/./atom.xml" title="做一个幸福的人" type="application/atom+xml"><meta name="google-site-verification" content="Qr_3yqLtyErDFKHW7mE8PJz4qDUX-bf_fMLpSRckQe4"><meta name="baidu-site-verification" content="zOwIvKMV7f"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"搜索文章",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet"><meta name="description" content="参考：神经网络概述神经网络输入单个数据到神经网络，进行前向传播和反向传播的推导预备知识 链式法则 雅可比矩阵"><meta name="keywords" content="微积分,线性代数,神经网络"><meta property="og:type" content="article"><meta property="og:title" content="神经网络推导-单个数据"><meta property="og:url" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;posts&#x2F;cb820bb8.html"><meta property="og:site_name" content="做一个幸福的人"><meta property="og:description" content="参考：神经网络概述神经网络输入单个数据到神经网络，进行前向传播和反向传播的推导预备知识 链式法则 雅可比矩阵"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;神经网络推导-单个数据&#x2F;test_net.png"><meta property="og:updated_time" content="2020-02-15T05:36:35.875Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;神经网络推导-单个数据&#x2F;test_net.png"><link rel="canonical" href="https://www.zhujian.tech/posts/cb820bb8.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>神经网络推导-单个数据 | 做一个幸福的人</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e677aac1ac69b8826b9cfecb4e72e107";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">做一个幸福的人</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">面朝大海，春暖花开</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">129</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">48</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">169</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header> <a href="https://github.com/zjZSTU" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zhujian.tech/posts/cb820bb8.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="zhujian"><meta itemprop="description" content="one bite at a time"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="做一个幸福的人"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 神经网络推导-单个数据</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-05-01 10:22:08" itemprop="dateCreated datePublished" datetime="2019-05-01T10:22:08+00:00">2019-05-01</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-15 05:36:35" itemprop="dateModified" datetime="2020-02-15T05:36:35+00:00">2020-02-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/" itemprop="url" rel="index"><span itemprop="name">算法</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/deep-learning/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/math/" itemprop="url" rel="index"><span itemprop="name">数学</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/data-learning/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>16k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>27 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>参考：</p><p><a href="https://www.zhujian.tech/posts/7ca31f7.html#more">神经网络概述</a></p><p><a href="http://ufldl.stanford.edu/wiki/index.php/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C" target="_blank" rel="noopener">神经网络</a></p><p>输入单个数据到神经网络，进行前向传播和反向传播的推导</p><h2 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h2><ol><li>链式法则</li><li>雅可比矩阵</li></ol><a id="more"></a><h3 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h3><p>参考：<a href="https://baike.baidu.com/item/%E9%93%BE%E5%BC%8F%E6%B3%95%E5%88%99" target="_blank" rel="noopener">链式法则</a></p><p>反向传播（<code>backpropagatation</code>）的目的是进行可学习参数（<code>learnable parameters</code>）的更新，其实现方式是利用链式法则（<code>chain rule</code>）进行梯度计算</p><p><code>cs231n</code>的<a href="http://cs231n.github.io/optimization-2/" target="_blank" rel="noopener">Backpropagation, Intuitions</a> 给出了生动的关于链式求导的学习示例</p><h4 id="简单函数求导"><a href="#简单函数求导" class="headerlink" title="简单函数求导"></a>简单函数求导</h4><p>对于简单函数而言，其导数计算方式很简单。比如</p><script type="math/tex;mode=display">
f(x,y)=x\pm y \Rightarrow \frac{d f}{d x}=1 \ \  \frac{d f}{d y}=\pm 1  \\
f(x)=ax \Rightarrow \frac{d f}{d x}=a \\
f(x)=\frac {1}{x} \Rightarrow \frac{d f}{d x}=\frac {-1}{x^2} \\
f(x)=e^{x} \Rightarrow \frac{d f}{d x}=e^{x} \\
f(x,y)=max(x,y) \Rightarrow \frac{d f}{d x}=\mathbb{1}(x>=y) \ \ \frac{d f}{d y}=\mathbb{1}(y>=x)</script><h4 id="复合函数求导"><a href="#复合函数求导" class="headerlink" title="复合函数求导"></a>复合函数求导</h4><p>对于复合函数而言，直接计算导数很复杂，但它可以拆分为多个简单函数，然后逐一进行计算</p><p>以函数$f(x_{1},x_{2})$为例，其实现公式如下：</p><script type="math/tex;mode=display">
f(x_{1},x_{2})=\frac{1}{1+e^{-(w_{0}+w_{1}\cdot x_{1}+w_{2}\cdot x_{2})}}</script><p>其中函数可拆分成如下形式：</p><script type="math/tex;mode=display">
\sigma (x)=\frac {1}{1+e^{-x}} \\
p(x)= w\cdot x\\</script><p>对$\sigma (x)$和$p(x)$求导如下：</p><script type="math/tex;mode=display">
\frac{d \sigma}{d x} = \frac {-1}{(1+e^{-x})^2}\cdot (-e^{-x}) = \sigma (x)(1-\sigma (x)) \\
\frac{d p}{d x} = w</script><p>所以函数$f(x_{1},x_{2})$求导如下：</p><script type="math/tex;mode=display">
\frac{d f}{d x_{1}}=\frac{d \sigma}{d p}\cdot \frac{d p}{d x_{1}}=f(x_{1},x_{2})\cdot (1-f(x_{1},x_{2}))\cdot w_{1} \\
\frac{d f}{d x_{2}}=\frac{d \sigma}{d p}\cdot \frac{d p}{d x_{2}}=f (x_{1},x_{2})\cdot (1-f (x_{1},x_{2}))\cdot w_{2}</script><p><em>可以用相同的方式对权重$w_{1},w_{2}$求导</em></p><p><strong>所以链式法则指的是将复合函数拆分为一个个简单函数，通过组合简单函数的导数得到复合函数的导数，最后组成梯度进行权值更新</strong></p><h3 id="雅可比矩阵"><a href="#雅可比矩阵" class="headerlink" title="雅可比矩阵"></a>雅可比矩阵</h3><p>假设函数从$R^{n}$映射到$R^{m}$，其雅可比（Jacobian）矩阵就是从$R^{n}$到$R^{m}$的线性映射</p><p>如果函数由$m$个实函数组成：$y_{1}(x_{1},…,x_{n}),…,y_{m}(x_{1},…,x_{n})$，则其偏导数（如果存在）可以组成一个$m$行$n$列的雅可比矩阵</p><script type="math/tex;mode=display">
\left[ \begin{array}{ccc}{\frac{\partial y_{1}}{\partial x_{1}}} & {\cdots} & {\frac{\partial y_{1}}{\partial x_{n}}} \\ {\vdots} & {\ddots} & {\vdots} \\ {\frac{\partial y_{m}}{\partial x_{1}}} & {\cdots} & {\frac{\partial y_{m}}{\partial x_{n}}}\end{array}\right]</script><p>其大小为$m\times n$</p><p><strong>在神经网络中每次计算的输入输出结果都是向量或矩阵，所以其偏导数均可以组成Jacobian矩阵</strong></p><p>比如函数$f’(z^{l})$表示输出向量$a^{(l)}$对输入向量$z^{(l)}$求导，就是一个雅可比矩阵</p><script type="math/tex;mode=display">
\left[ \begin{array}{ccc}{\frac{\partial a^{(l)}_{1}}{\partial z^{(l)}_{1}}} & {\cdots} & {\frac{\partial a^{(1)}_{1}}{\partial z^{(l)}_{n}}} \\ {\vdots} & {\ddots} & {\vdots} \\ {\frac{\partial a^{(l)}_{m}}{\partial z^{(l)}_{1}}} & {\cdots} & {\frac{\partial a^{(l)}_{m}}{\partial z^{(l)}_{n}}}\end{array}\right]</script><p>其大小为$n^{(l)}\times n^{(l)}$</p><h2 id="网络符号定义"><a href="#网络符号定义" class="headerlink" title="网络符号定义"></a>网络符号定义</h2><p>规范神经网络的计算符号</p><p>关于神经元和层数</p><ul><li>$L$表示网络层数（不计入输入层）<ul><li>$L=2$，其中输入层是第<code>0</code>层，隐藏层是第<code>1</code>层，输出层是第<code>2</code>层</li></ul></li><li>$n^{(l)}$表示第$l$层的神经元个数（不包括偏置神经元）<ul><li>$n^{(0)}=3$，表示输入层神经元个数为<code>3</code></li><li>$n^{(1)}=4$，表示隐藏层神经元个数为<code>4</code></li><li>$n^{(2)}=2$，表示输出层神经元个数为<code>2</code></li></ul></li></ul><p>关于权重矩阵和偏置值</p><ul><li>$W^{(l)}$表示第$l-1$层到第$l$层的<strong>权重矩阵</strong>，矩阵行数为第$l$层的神经元个数，列数为第$l-1$层神经元个数<ul><li>$W^{(1)}$表示输入层到隐藏层的权重矩阵，大小为$R^{4\times 3}$</li><li>$W^{(2)}$表示隐藏层到输出层的权重矩阵，大小为$R^{2\times 4}$</li></ul></li><li>$W^{(l)}_{i,j}$表示第$l$层第$i$个神经元到第$l-1$第$j$个神经元的权值<ul><li>$i$的取值范围是$[1,n^{(l)}]$</li><li>$j$的取值范围是$[1, n^{(l-1)}]$</li></ul></li><li>$W^{(l)}_{i}$表示第$l$层第$i$个神经元对应的权重向量，大小为$n^{(l-1)}$</li><li>$W^{(l)}_{,j}$表示第$l-1$层第$j$个神经元对应的权重向量，大小为$n^{(l)}$</li><li>$b^{(l)}$表示第$l$层的<strong>偏置向量</strong><ul><li>$b^{(1)}$表示输入层到隐藏层的偏置向量，大小为$R^{4\times 1}$</li><li>$b^{(2)}$表示隐藏层到输出层的偏置向量，大小为$R^{2\times 1}$</li></ul></li><li>$b^{(l)}_{i}$表示第$l$层第$i$个神经元的偏置值<ul><li>$b^{(1)}_{2}$表示第$1$层隐藏层第$2$个神经元的偏置值</li></ul></li></ul><p>关于神经元输入向量和输出向量</p><ul><li>$a^{(l)}$表示第$l$层<strong>输出向量</strong>，$a^{(l)}=[a^{(l)}_{1},a^{(l)}_{2},…,a^{(l)}_{n^{l}}]^{T}$<ul><li>$a^{(0)}$表示输入层输出向量，大小为$R^{3\times 1}$</li><li>$a^{(1)}$表示隐藏层输出向量，大小为$R^{4\times 1}$</li><li>$a^{(2)}$表示输出层输出向量，大小为$R^{2\times 1}$</li></ul></li><li><p>$a^{(l)}_{i}$表示第$l$层第$i$个单元的输出值，其是输入向量经过激活计算后的值</p><ul><li>$a^{(1)}_{3}$表示隐含层第$3$个神经元的输入值，$a^{(1)}_{3}=g(z^{(1)}_{3})$</li></ul></li><li><p>$z^{(l)}$表示第$l$层<strong>输入向量</strong>，$z^{(l)}=[z^{(l)}_{1},z^{(l)}_{2},…,z^{(l)}_{n^{l}}]^{T}$</p><ul><li>$z^{(1)}$表示隐藏层的输入向量，大小为$R^{4\times 1}$</li><li>$z^{(2)}$表示输出层的输入向量，大小为$R^{2\times 1}$</li></ul></li><li>$z^{(l)}_{i}$表示第$l$层第$i$个单元的输入值，其是上一层输出向量和该层第$i$个神经元权重向量的加权累加和<ul><li>$z^{(1)}_{1}$表示隐藏层第$1$个神经元的输入值，$z^{(1)}_{1}=b^{(1)}_{1}+W^{(1)}_{1,1}\cdot a^{(0)}_{1}+W^{(1)}_{1,2}\cdot a^{(0)}_{2}+W^{(1)}_{1,3}\cdot a^{(0)}_{3}$</li></ul></li></ul><p>关于神经元激活函数</p><ul><li>$g()$表示激活函数操作</li></ul><p>关于评分函数和损失函数</p><ul><li>$h()$表示评分函数操作</li><li>$J()$表示代价函数操作</li></ul><p><strong>神经元执行步骤</strong></p><p>神经元操作分为<code>2</code>步计算：</p><ol><li>输入向量$z^{(l)}$=前一层神经元输出向量$a^{(l-1)}$与权重矩阵$W^{(l)}$的加权累加和+偏置向量</li></ol><script type="math/tex;mode=display">
z^{(l)}_{j}=W^{(l)}_{i,j}\cdot a^{(l-1)}_{i} + b^{(l)}_{j} \Rightarrow 
z^{(l)}=W^{(l)}\cdot a^{(l-1)} + b^{(l)}</script><ol><li>输出向量$a^{(l)}$=对输入向量$z^{(l)}$进行激活函数操作</li></ol><script type="math/tex;mode=display">
a^{(l)}_{i}=g(z_{i}^{(l)})
\Rightarrow 
a^{(l)}=g(z^{(l)})</script><h2 id="TestNet网络"><a href="#TestNet网络" class="headerlink" title="TestNet网络"></a>TestNet网络</h2><p><code>TestNet</code>是一个<code>2</code>层神经网络，结构如下：</p><ul><li>输入层有<code>3</code>个神经元</li><li>隐藏层有<code>4</code>个神经元</li><li>输出层有<code>2</code>个神经元</li></ul><p><img src="/imgs/神经网络推导-单个数据/test_net.png" alt></p><ul><li>激活函数为<code>relu</code>函数</li><li>评分函数为<code>softmax</code>回归</li><li>代价函数为交叉熵损失</li></ul><p>对输入层</p><script type="math/tex;mode=display">
a^{(0)}=
\begin{bmatrix}
a^{(0)}_{1}\\ 
a^{(0)}_{2}\\ 
a^{(0)}_{3}
\end{bmatrix}
\in R^{3\times 1}</script><p>对隐藏层</p><script type="math/tex;mode=display">
W^{(1)}
=\begin{bmatrix}
W^{(1)}_{1,1} & W^{(1)}_{1,2} & W^{(1)}_{1,3}\\ 
W^{(1)}_{2,1} & W^{(1)}_{2,2} & W^{(1)}_{2,3}\\ 
W^{(1)}_{3,1} & W^{(1)}_{3,2} & W^{(1)}_{3,3}\\ 
W^{(1)}_{4,1} & W^{(1)}_{4,2} & W^{(1)}_{4,3}
\end{bmatrix}
\in R^{4\times 3}</script><script type="math/tex;mode=display">
b^{(1)}=
\begin{bmatrix}
b^{(1)}_{1}\\ 
b^{(1)}_{2}\\ 
b^{(1)}_{3}\\
b^{(1)}_{4}
\end{bmatrix}
\in R^{4\times 1}</script><script type="math/tex;mode=display">
z^{(1)}=
\begin{bmatrix}
z^{(1)}_{1}\\ 
z^{(1)}_{2}\\ 
z^{(1)}_{3}\\
z^{(1)}_{4}
\end{bmatrix}
\in R^{4\times 1}</script><script type="math/tex;mode=display">
a^{(1)}=
\begin{bmatrix}
a^{(1)}_{1}\\ 
a^{(1)}_{2}\\ 
a^{(1)}_{3}\\
a^{(1)}_{4}
\end{bmatrix}
\in R^{4\times 1}</script><p>对输出层</p><script type="math/tex;mode=display">
W^{(2)}
=\begin{bmatrix}
W^{(2)}_{1,1} & W^{(2)}_{1,2} & W^{(2)}_{1,3} & W^{(2)}_{1,4}\\ 
W^{(2)}_{2,1} & W^{(2)}_{2,2} & W^{(2)}_{2,3} & W^{(2)}_{2,4}
\end{bmatrix}
\in R^{2\times 4}</script><script type="math/tex;mode=display">
b^{(2)}=
\begin{bmatrix}
b^{(2)}_{1}\\ 
b^{(2)}_{2}
\end{bmatrix}
\in R^{4\times 1}</script><script type="math/tex;mode=display">
z^{(2)}=
\begin{bmatrix}
z^{(2)}_{1}\\ 
z^{(2)}_{2}
\end{bmatrix}
\in R^{4\times 1}</script><p>评分值</p><script type="math/tex;mode=display">
h(z^{(2)})
=\begin{bmatrix}
p(y=1)\\ 
p(y=2)
\end{bmatrix}
\in R^{2\times 1}</script><p>损失值</p><script type="math/tex;mode=display">
J(z^{(2)})=(-1)\cdot 1(y=1)\ln p(y=1)+(-1)\cdot 1(y=2)\ln p(y=2)\in R^{1}</script><h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><ul><li>对于输入层神经元，其得到输入数据后直接输出到下一层，并没有进行权值操作和激活函数操作，所以严格意义上讲输入层不是真正的神经元</li><li>对于输出层神经元，其得到输入数据，进行加权求和后直接输出进行评分函数计算，没有进行激活函数操作</li></ul><p>输入层到隐藏层计算</p><script type="math/tex;mode=display">
z^{(1)}_{1}=W^{(1)}_{1}\cdot a^{(0)}+b^{(1)}_{1}
=W^{(1)}_{1,1}\cdot a^{(0)}_{1}
+W^{(1)}_{1,2}\cdot a^{(0)}_{2}
+W^{(1)}_{1,3}\cdot a^{(0)}_{3}
+b^{(1)}_{1}</script><script type="math/tex;mode=display">
z^{(1)}_{2}=W^{(1)}_{2}\cdot a^{(0)}+b^{(1)}_{2}
=W^{(1)}_{2,1}\cdot a^{(0)}_{1}
+W^{(1)}_{2,2}\cdot a^{(0)}_{2}
+W^{(1)}_{2,3}\cdot a^{(0)}_{3}
+b^{(1)}_{2}</script><script type="math/tex;mode=display">
z^{(1)}_{3}=W^{(1)}_{3}\cdot a^{(0)}+b^{(1)}_{3}
=W^{(1)}_{3,1}\cdot a^{(0)}_{1}
+W^{(1)}_{3,2}\cdot a^{(0)}_{2}
+W^{(1)}_{3,3}\cdot a^{(0)}_{3}
+b^{(1)}_{3}</script><script type="math/tex;mode=display">
z^{(1)}_{4}=W^{(1)}_{4}\cdot a^{(0)}+b^{(1)}_{4}
=W^{(1)}_{4,1}\cdot a^{(0)}_{1}
+W^{(1)}_{4,2}\cdot a^{(0)}_{2}
+W^{(1)}_{4,3}\cdot a^{(0)}_{3}
+b^{(1)}_{4}</script><script type="math/tex;mode=display">
\Rightarrow z^{(1)}
=[z^{(1)}_{1},z^{(1)}_{2},z^{(1)}_{3},z^{(1)}_{4}]^{T}
=W^{(1)}\cdot a^{(0)}+b^{(1)}</script><p>隐藏层输入向量到输出向量</p><script type="math/tex;mode=display">
a^{(1)}_{1}=relu(z^{(1)}_{1}) \\
a^{(1)}_{2}=relu(z^{(1)}_{2}) \\
a^{(1)}_{3}=relu(z^{(1)}_{3}) \\
a^{(1)}_{4}=relu(z^{(1)}_{4})</script><script type="math/tex;mode=display">
\Rightarrow 
a^{(1)}=[a^{(1)}_{1},a^{(1)}_{2},a^{(1)}_{3},a^{(1)}_{4}]^{T}
=relu(z^{(1)})</script><p>隐藏层到输出层计算</p><script type="math/tex;mode=display">
z^{(2)}_{1}=W^{(2)}_{1}\cdot a^{(1)}+b^{(2)}_{1}
=W^{(2)}_{1,1}\cdot a^{(1)}_{1}
+W^{(2)}_{1,2}\cdot a^{(1)}_{2}
+W^{(2)}_{1,3}\cdot a^{(1)}_{3}
+W^{(2)}_{1,4}\cdot a^{(1)}_{4}
+b^{(2)}_{1}</script><script type="math/tex;mode=display">
z^{(2)}_{2}=W^{(2)}_{2}\cdot a^{(1)}+b^{(2)}_{2}
=W^{(2)}_{2,1}\cdot a^{(1)}_{1}
+W^{(2)}_{2,2}\cdot a^{(1)}_{2}
+W^{(2)}_{2,3}\cdot a^{(1)}_{3}
+W^{(2)}_{2,4}\cdot a^{(1)}_{4}
+b^{(2)}_{2}</script><script type="math/tex;mode=display">
\Rightarrow z^{(2)}
=[z^{(2)}_{1},z^{(2)}_{2}]^{T}
=W^{(2)}\cdot a^{(1)}+b^{(2)}</script><p>评分操作</p><script type="math/tex;mode=display">
p(y=1)=\frac {exp(z^{(2)}_{1})}{\sum exp(z^{(2)})} \ \ 
p(y=2)=\frac {exp(z^{(2)}_{2})}{\sum exp(z^{(2)})}</script><script type="math/tex;mode=display">
\Rightarrow h(z^{(2)})
=[p(y=1),p(y=2)]^{T}
=[\frac {exp(z^{(2)}_{1})}{\sum exp(z^{(2)})}, \frac {exp(z^{(2)}_{2})}{\sum exp(z^{(2)})}]^{T}</script><p>损失值</p><script type="math/tex;mode=display">
J(z^{(2)})=(-1)\cdot 1(y=1)\ln p(y=1)+(-1)\cdot 1(y=2)\ln p(y=2)</script><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>计算输出层输入向量梯度</p><script type="math/tex;mode=display">
\frac {\partial J}{\partial z^{(2)}_{1}}=
(-1)\cdot \frac {1(y=1)}{p(y=1)}\cdot \frac {\partial p(y=1)}{\partial z^{(2)}_{1}}
+(-1)\cdot \frac {1(y=2)}{p(y=2)}\cdot \frac {\partial p(y=2)}{\partial z^{(2)}_{1}}</script><script type="math/tex;mode=display">
\frac {\partial p(y=1)}{\partial z^{(2)}_{1}}
=\frac {exp(z^{(2)}_{1})\cdot \sum exp(z^{(2)})-exp(z^{(2)}_{1})\cdot exp(z^{(2)}_{1})}{(\sum exp(z^{(2)}))^{2}}
=\frac {exp(z^{(2)}_{1})}{\sum exp(z^{(2)})}
-(\frac {exp(z^{(2)}_{1})}{\sum exp(z^{(2)})})^2
=p(y=1)-(p(y=1))^2</script><script type="math/tex;mode=display">
\frac {\partial p(y=2)}{\partial z^{(2)}_{1}}
=\frac {-exp(z^{(2)}_{2})\cdot exp(z^{(2)}_{1})}{(\sum exp(z^{(2)}))^2}
=(-1)\cdot \frac {exp(z^{(2)}_{1})}{\sum exp(z^{(2)})}\cdot \frac {exp(z^{(2)}_{2})}{\sum exp(z^{(2)})}
=(-1)\cdot p(y=1)p(y=2)</script><script type="math/tex;mode=display">
\Rightarrow 
\frac {\partial J}{\partial z^{(2)}_{1}}
=(-1)\cdot \frac {1(y=1)}{p(y=1)}\cdot (p(y=1)-(p(y=1))^2)
+(-1)\cdot \frac {1(y=2)}{p(y=2)}\cdot (-1)\cdot p(y=1)p(y=2) \\
=(-1)\cdot 1(y=1)\cdot (1-p(y=1))
+1(y=2)\cdot p(y=1)
=p(y=1)-1(y=1)</script><script type="math/tex;mode=display">
\Rightarrow
\frac {\partial J}{\partial z^{(2)}_{2}}
=p(y=2)-1(y=2)</script><script type="math/tex;mode=display">
\Rightarrow \frac {\partial J}{\partial z^{(2)}}
=[p(y=1)-1(y=1), p(y=2)-1(y=2)]^{T}</script><p>计算输出层权重向量梯度</p><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(2)}_{1,1}}
=\frac {\partial J}{\partial z^{(2)}_{1}}\cdot \frac {\partial z^{(2)}_{1}}{\partial W^{(2)}_{1,1}}
=(p(y=1)-1(y=1))\cdot a^{(1)}_{1}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(2)}_{1,2}}
=\frac {\partial J}{\partial z^{(2)}_{1}}\cdot \frac {\partial z^{(2)}_{1}}{\partial W^{(2)}_{1,2}}
=(p(y=1)-1(y=1))\cdot a^{(1)}_{2}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(2)}_{1,3}}
=\frac {\partial J}{\partial z^{(2)}_{1}}\cdot \frac {\partial z^{(2)}_{1}}{\partial W^{(2)}_{1,3}}
=(p(y=1)-1(y=1))\cdot a^{(1)}_{3}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(2)}_{1,4}}
=\frac {\partial J}{\partial z^{(2)}_{1}}\cdot \frac {\partial z^{(2)}_{1}}{\partial W^{(2)}_{1,4}}
=(p(y=1)-1(y=1))\cdot a^{(1)}_{4}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(2)}_{2,1}}
=\frac {\partial J}{\partial z^{(2)}_{2}}\cdot \frac {\partial z^{(2)}_{2}}{\partial W^{(2)}_{2,1}}
=(p(y=2)-1(y=2))\cdot a^{(1)}_{1}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(2)}_{2,2}}
=\frac {\partial J}{\partial z^{(2)}_{2}}\cdot \frac {\partial z^{(2)}_{2}}{\partial W^{(2)}_{2,2}}
=(p(y=2)-1(y=2))\cdot a^{(1)}_{2}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(2)}_{2,3}}
=\frac {\partial J}{\partial z^{(2)}_{2}}\cdot \frac {\partial z^{(2)}_{2}}{\partial W^{(2)}_{2,3}}
=(p(y=2)-1(y=2))\cdot a^{(1)}_{3}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(2)}_{2,4}}
=\frac {\partial J}{\partial z^{(2)}_{2}}\cdot \frac {\partial z^{(2)}_{2}}{\partial W^{(2)}_{2,4}}
=(p(y=2)-1(y=2))\cdot a^{(1)}_{4}</script><script type="math/tex;mode=display">
\Rightarrow 
\frac {\partial J}{\partial W^{(2)}}
=\begin{bmatrix}
\frac {\partial J}{\partial W^{(2)}_{1,1}} & \frac {\partial J}{\partial W^{(2)}_{1,2}} & \frac {\partial J}{\partial W^{(2)}_{1,3}} & \frac {\partial J}{\partial W^{(2)}_{1,4}}\\ 
\frac {\partial J}{\partial W^{(2)}_{2,1}} & \frac {\partial J}{\partial W^{(2)}_{2,2}} & \frac {\partial J}{\partial W^{(2)}_{2,3}} & \frac {\partial J}{\partial W^{(2)}_{2,4}}
\end{bmatrix}</script><script type="math/tex;mode=display">
=\begin{bmatrix}
(p(y=1)-1(y=1))\cdot a^{(1)}_{1} & (p(y=1)-1(y=1))\cdot a^{(1)}_{2} & (p(y=1)-1(y=1))\cdot a^{(1)}_{3} & (p(y=1)-1(y=1))\cdot a^{(1)}_{4}\\ 
(p(y=2)-1(y=2))\cdot a^{(1)}_{1} & (p(y=2)-1(y=2))\cdot a^{(1)}_{2} & (p(y=2)-1(y=2))\cdot a^{(1)}_{3} & (p(y=2)-1(y=2))\cdot a^{(1)}_{4}
\end{bmatrix}</script><script type="math/tex;mode=display">
=\begin{bmatrix}
p(y=1)-1(y=1)\\ 
p(y=2)-1(y=2)
\end{bmatrix} 
\begin{bmatrix}
a^{(1)}_{1}\\ 
a^{(1)}_{2}\\ 
a^{(1)}_{3}\\ 
a^{(1)}_{4}
\end{bmatrix}
=R^{2\times 1}\cdot R^{1\times 4}
=R^{2\times 4}</script><p>计算隐藏层输出向量梯度</p><script type="math/tex;mode=display">
\frac {\partial J}{\partial a^{(1)}_{1}}
=\frac {\partial J}{\partial z^{(2)}_{1}}\cdot \frac {\partial z^{(2)}_{1}}{\partial a^{(1)}_{1}}
+\frac {\partial J}{\partial z^{(2)}_{2}}\cdot \frac {\partial z^{(2)}_{2}}{\partial a^{(1)}_{1}}
=(p(y=1)-1(y=1))\cdot W^{(2)}_{1,1}
+(p(y=2)-1(y=2))\cdot W^{(2)}_{2,1}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial a^{(1)}_{2}}
=\frac {\partial J}{\partial z^{(2)}_{1}}\cdot \frac {\partial z^{(2)}_{1}}{\partial a^{(1)}_{2}}
+\frac {\partial J}{\partial z^{(2)}_{2}}\cdot \frac {\partial z^{(2)}_{2}}{\partial a^{(1)}_{2}}
=(p(y=1)-1(y=1))\cdot W^{(2)}_{1,2}
+(p(y=2)-1(y=2))\cdot W^{(2)}_{2,2}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial a^{(1)}_{3}}
=\frac {\partial J}{\partial z^{(2)}_{1}}\cdot \frac {\partial z^{(2)}_{1}}{\partial a^{(1)}_{3}}
+\frac {\partial J}{\partial z^{(2)}_{2}}\cdot \frac {\partial z^{(2)}_{2}}{\partial a^{(1)}_{3}}
=(p(y=1)-1(y=1))\cdot W^{(2)}_{1,3}
+(p(y=2)-1(y=2))\cdot W^{(2)}_{2,3}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial a^{(1)}_{4}}
=\frac {\partial J}{\partial z^{(2)}_{1}}\cdot \frac {\partial z^{(2)}_{1}}{\partial a^{(1)}_{4}}
+\frac {\partial J}{\partial z^{(2)}_{2}}\cdot \frac {\partial z^{(2)}_{2}}{\partial a^{(1)}_{4}}
=(p(y=1)-1(y=1))\cdot W^{(2)}_{1,4}
+(p(y=2)-1(y=2))\cdot W^{(2)}_{2,4}</script><script type="math/tex;mode=display">
\Rightarrow \frac {\partial J}{\partial a^{(1)}}
=\begin{bmatrix}
W^{(2)}_{1,1} & W^{(2)}_{2,1}\\ 
W^{(2)}_{1,2} & W^{(2)}_{2,2}\\ 
W^{(2)}_{1,3} & W^{(2)}_{2,3}\\ 
W^{(2)}_{1,4} & W^{(2)}_{2,4}
\end{bmatrix}
\begin{bmatrix}
p(y=1)-1(y=1)\\ 
p(y=2)-1(y=2)
\end{bmatrix}
=R^{4\times 2}\cdot R^{2\times 1}
=R^{4\times 1}</script><p>计算隐藏层输入向量的梯度</p><script type="math/tex;mode=display">
\frac {\partial J}{\partial z^{(1)}_{1}}
=\frac {\partial J}{\partial a^{(1)}_{1}}\cdot \frac {\partial a^{(1)}_{1}}{\partial z^{(1)}_{1}}
=((p(y=1)-1(y=1))\cdot W^{(2)}_{1,1}
+(p(y=2)-1(y=2))\cdot W^{(2)}_{2,1})\cdot 1(z^{(1)}_{1}\geq 0)</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial z^{(1)}_{2}}
=\frac {\partial J}{\partial a^{(1)}_{2}}\cdot \frac {\partial a^{(1)}_{2}}{\partial z^{(1)}_{2}}
=((p(y=1)-1(y=1))\cdot W^{(2)}_{1,2}
+(p(y=2)-1(y=2))\cdot W^{(2)}_{2,2})\cdot 1(z^{(1)}_{2}\geq 0)</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial z^{(1)}_{3}}
=\frac {\partial J}{\partial a^{(1)}_{3}}\cdot \frac {\partial a^{(1)}_{3}}{\partial z^{(1)}_{3}}
=((p(y=1)-1(y=1))\cdot W^{(2)}_{1,3}
+(p(y=2)-1(y=2))\cdot W^{(2)}_{2,3})\cdot 1(z^{(1)}_{3}\geq 0)</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial z^{(1)}_{4}}
=\frac {\partial J}{\partial a^{(1)}_{4}}\cdot \frac {\partial a^{(1)}_{4}}{\partial z^{(1)}_{4}}
=((p(y=1)-1(y=1))\cdot W^{(2)}_{1,4}
+(p(y=2)-1(y=2))\cdot W^{(2)}_{2,4})\cdot 1(z^{(1)}_{4}\geq 0)</script><script type="math/tex;mode=display">
\Rightarrow \frac {\partial J}{\partial z^{(1)}}
=\frac {\partial J}{\partial a^{(1)}}\cdot \frac {\partial a^{(1)}}{\partial z^{(1)}}
=(
\begin{bmatrix}
W^{(2)}_{1,1} & W^{(2)}_{2,1}\\ 
W^{(2)}_{1,2} & W^{(2)}_{2,2}\\ 
W^{(2)}_{1,3} & W^{(2)}_{2,3}\\ 
W^{(2)}_{1,4} & W^{(2)}_{2,4}
\end{bmatrix}
\begin{bmatrix}
p(y=1)-1(y=1)\\ 
p(y=2)-1(y=2)
\end{bmatrix}
)
*\begin{bmatrix}
1(z^{(1)}_{1}\geq 0) \\ 
1(z^{(1)}_{2}\geq 0) \\ 
1(z^{(1)}_{3}\geq 0) \\ 
1(z^{(1)}_{4}\geq 0)
\end{bmatrix}
=(R^{4\times 2}\cdot R^{2\times 1})* R^{4\times 1}
=R^{4\times 1}</script><p>计算隐藏层权重向量的梯度</p><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(1)}_{1,1}}
=\frac {\partial J}{\partial z^{(1)}_{1}}\cdot
\frac {\partial z^{(1)}_{1}}{\partial W^{(1)}_{1,1}}
=((p(y=1)-1(y=1))\cdot W^{(2)}_{1,1}
+(p(y=2)-1(y=2))\cdot W^{(2)}_{2,1})\cdot 1(z^{(1)}_{1}\geq 0)\cdot a^{(0)}_{1}</script><script type="math/tex;mode=display">
\Rightarrow \frac {\partial J}{\partial W^{(1)}_{i,j}}
=\frac {\partial J}{\partial z^{(1)}_{i}}\cdot
\frac {\partial z^{(1)}_{i}}{\partial W^{(1)}_{i,j}}
=((p(y=1)-1(y=1))\cdot W^{(2)}_{1,i}
+(p(y=2)-1(y=2))\cdot W^{(2)}_{2,i})\cdot 1(z^{(1)}_{i}\geq 0)\cdot a^{(0)}_{j}</script><script type="math/tex;mode=display">
\Rightarrow \frac {\partial J}{\partial W^{(1)}_{i,:}}
=((p(y=1)-1(y=1))\cdot W^{(2)}_{1,i}
+(p(y=2)-1(y=2))\cdot W^{(2)}_{2,i})\cdot 1(z^{(1)}_{i}\geq 0)\cdot a^{(0)}</script><script type="math/tex;mode=display">
\Rightarrow \frac {\partial J}{\partial W^{(1)}}
=\frac {\partial J}{\partial z^{(1)}_{1}}\cdot (a^{(0)})^{T}
=R^{4\times 1}\cdot R^{1\times 3}
=R^{4\times 3}</script><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><code>TestNet</code>网络的前向操作如下：</p><script type="math/tex;mode=display">
z^{(1)}
=W^{(1)}\cdot a^{(0)}+b^{(1)}</script><script type="math/tex;mode=display">
a^{(1)}
=g(z^{(1)})</script><script type="math/tex;mode=display">
z^{(2)}
=W^{(2)}\cdot a^{(1)}+b^{(2)}</script><script type="math/tex;mode=display">
h(z^{(2)})
=[p(y=1),p(y=2)]^{T}
=[\frac {exp(z^{(2)}_{1})}{\sum exp(z^{(2)})}, \frac {exp(z^{(2)}_{2})}{\sum exp(z^{(2)})}]^{T}</script><script type="math/tex;mode=display">
J(z^{(2)})=(-1)\cdot 1(y=1)\ln p(y=1)+(-1)\cdot 1(y=2)\ln p(y=2)</script><p>反向传播如下：</p><script type="math/tex;mode=display">
\frac {\partial J}{\partial z^{(2)}}
=[p(y=1)-1(y=1), p(y=2)-1(y=2)]^{T}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(2)}}
=\frac {\partial J}{\partial z^{(2)}}\cdot a^{(1)}
=\begin{bmatrix}
p(y=1)-1(y=1)\\ 
p(y=2)-1(y=2)
\end{bmatrix} 
\begin{bmatrix}
a^{(1)}_{1}\\ 
a^{(1)}_{2}\\ 
a^{(1)}_{3}\\ 
a^{(1)}_{4}
\end{bmatrix}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial a^{(1)}}
=(W^{(2)})^{T}\cdot \frac {\partial J}{\partial z^{(2)}}
=\begin{bmatrix}
W^{(2)}_{1,1} & W^{(2)}_{2,1}\\ 
W^{(2)}_{1,2} & W^{(2)}_{2,2}\\ 
W^{(2)}_{1,3} & W^{(2)}_{2,3}\\ 
W^{(2)}_{1,4} & W^{(2)}_{2,4}
\end{bmatrix}
\begin{bmatrix}
p(y=1)-1(y=1)\\ 
p(y=2)-1(y=2)
\end{bmatrix}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial z^{(1)}}
=\frac {\partial J}{\partial a^{(1)}}*1(z^{(1)}\geq 0)
=(
\begin{bmatrix}
W^{(2)}_{1,1} & W^{(2)}_{2,1}\\ 
W^{(2)}_{1,2} & W^{(2)}_{2,2}\\ 
W^{(2)}_{1,3} & W^{(2)}_{2,3}\\ 
W^{(2)}_{1,4} & W^{(2)}_{2,4}
\end{bmatrix}
\begin{bmatrix}
p(y=1)-1(y=1)\\ 
p(y=2)-1(y=2)
\end{bmatrix}
)
*\begin{bmatrix}
1(z^{(1)}_{1}\geq 0) \\ 
1(z^{(1)}_{2}\geq 0) \\ 
1(z^{(1)}_{3}\geq 0) \\ 
1(z^{(1)}_{4}\geq 0)
\end{bmatrix}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(1)}}
=\frac {\partial J}{\partial z^{(1)}_{1}}\cdot (a^{(0)})^{T}</script><p>参考<a href="http://ufldl.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95" target="_blank" rel="noopener">反向传导算法</a>和<a href="https://zhuanlan.zhihu.com/p/22473137" target="_blank" rel="noopener">神经网络反向传播的数学原理</a>，设每层输入向量为残差$\delta^{(l)}=\frac{\partial J(W, b)}{\partial z^{(l)}}$，用于表示该层对最终输出值的残差造成的影响；而最终输出值的残差$\delta^{(L)}$就是损失函数对输出层输入向量的梯度</p><p>前向传播执行步骤</p><ol><li><p>层与层之间的操作就是输出向量和权值矩阵的加权求和以及对输入向量的函数激活</p><script type="math/tex;mode=display">
 z^{(l)} = W^{(l)}\cdot a^{(l-1)}+b^{(l)} \\
 a^{(l)} = g(z^{(l)})</script></li><li><p>输出层输出结果后，进行评分函数的计算，得到最终的计算结果（<em>以softmax分类为例</em>）</p><script type="math/tex;mode=display">
 h(z^{(2)})
 =[p(y=1),...,p(y=n^{(L)})]^{T}
 =[\frac {exp(z^{(2)}_{1})}{\sum exp(z^{(2)})}, ...,\frac {exp(z^{(2)}_{n^{(L)}})}{\sum exp(z^{(2)})}]^{T}</script></li><li><p>损失函数根据计算结果判断最终损失值（<em>以交叉熵损失为例</em>）</p><script type="math/tex;mode=display">
 J(z^{(L)})=(-1)\cdot 1(y=1)\ln p(y=1)+...+(-1)\cdot 1(y=n^{(L)})\ln p(y=n^{(L)})</script></li></ol><p>反向传播执行步骤</p><ol><li><p>计算损失函数对于输出层输入向量的梯度(最终层残差)</p><script type="math/tex;mode=display">
 \delta^{(L)}=
 \frac{\varphi J}{\varphi z^{(L)}}
 =[p(y=1)-1(y=1), ..., p(y=n^{(L)})-1(y=n^{(L)})]^{T}</script></li><li><p>计算中间隐藏层的残差值（$L-1,L-2,…1$）</p><script type="math/tex;mode=display">
 \delta^{(l)}=
 \frac{\varphi J}{\varphi z^{(l)}}
 =(\frac{\varphi J}{\varphi z^{(l+1)}}\cdot \frac{\varphi z^{(l+1)}}{\varphi a^{(l)}})
 *\frac{\varphi a^{(l)}}{\varphi z^{(l)}}
 =((W^{(l+1)})^{T}\cdot \delta^{(l+1)})
 *\frac{\varphi a^{(l)}}{\varphi z^{(l)}}</script></li><li><p>完成所有的可学习参数（权值矩阵和偏置向量）的梯度计算</p><script type="math/tex;mode=display">
 \nabla_{W^{(l)}} J(W, b)= \delta^{(l)}\cdot a^{(l-1)}\\
 \nabla_{b^{(l)}} J(W, b)= \delta^{(l)}</script></li><li><p>更新权值矩阵和偏置向量</p><script type="math/tex;mode=display">
 W^{(l)}=W^{(l)}-\alpha\left[\nabla W^{(l)}+\lambda W^{(l)}\right] \\
 b^{(l)}=b^{(l)}-\alpha \nabla b^{(l)}</script></li></ol><h4 id="初始化数据的必要性"><a href="#初始化数据的必要性" class="headerlink" title="初始化数据的必要性"></a>初始化数据的必要性</h4><p>梯度与输入数据<strong>呈正相关</strong>，权值更新公式如下：</p><script type="math/tex;mode=display">
W^{(l)}_{i,j} = W^{(l)}_{i,j} - \alpha \cdot \frac{\varphi J}{\varphi W^{(l)}_{i,j}}</script><p><strong>如果输入数据放大<code>1000</code>倍，那么梯度至少放大<code>1000</code>倍，这时需要极小的$\alpha$才能平衡每次更新的大小，所以初始化数据很有必要</strong></p></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.jpg" alt="zhujian 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="zhujian 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zhujian</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zhujian.tech/posts/cb820bb8.html" title="神经网络推导-单个数据">https://www.zhujian.tech/posts/cb820bb8.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/calculus/" rel="tag"># 微积分</a> <a href="/tags/linear-albegra/" rel="tag"># 线性代数</a> <a href="/tags/nerual-network/" rel="tag"># 神经网络</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/f86c970.html" rel="next" title="激活函数"><i class="fa fa-chevron-left"></i> 激活函数</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/posts/66015d4d.html" rel="prev" title="神经网络推导-批量数据">神经网络推导-批量数据<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#预备知识"><span class="nav-number">1.</span> <span class="nav-text">预备知识</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#链式法则"><span class="nav-number">1.1.</span> <span class="nav-text">链式法则</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#简单函数求导"><span class="nav-number">1.1.1.</span> <span class="nav-text">简单函数求导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#复合函数求导"><span class="nav-number">1.1.2.</span> <span class="nav-text">复合函数求导</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#雅可比矩阵"><span class="nav-number">1.2.</span> <span class="nav-text">雅可比矩阵</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#网络符号定义"><span class="nav-number">2.</span> <span class="nav-text">网络符号定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TestNet网络"><span class="nav-number">3.</span> <span class="nav-text">TestNet网络</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#前向传播"><span class="nav-number">4.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播"><span class="nav-number">5.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#小结"><span class="nav-number">6.</span> <span class="nav-text">小结</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#初始化数据的必要性"><span class="nav-number">6.0.1.</span> <span class="nav-text">初始化数据的必要性</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zhujian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">zhujian</p><div class="site-description" itemprop="description">one bite at a time</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">169</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">129</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/./atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zjZSTU" title="Github &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;zjZSTU" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/u012005313" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;u012005313" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i> CSDN</a></span><span class="links-of-author-item"><a href="/mailto:zjzstu@gmail.com" title="Gmail &amp;rarr; mailto:zjzstu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> Gmail</a></span><span class="links-of-author-item"><a href="/mailto:505169307@gmail.com" title="QQmail &amp;rarr; mailto:505169307@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> QQmail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://cloud.tencent.com/" title="https:&#x2F;&#x2F;cloud.tencent.com" rel="noopener" target="_blank">腾讯云</a></li><li class="links-of-blogroll-item"> <a href="https://www.aliyun.com/" title="https:&#x2F;&#x2F;www.aliyun.com" rel="noopener" target="_blank">阿里云</a></li><li class="links-of-blogroll-item"> <a href="https://developer.android.com/" title="https:&#x2F;&#x2F;developer.android.com&#x2F;" rel="noopener" target="_blank">Android Dev</a></li><li class="links-of-blogroll-item"> <a href="https://jenkins.io/" title="https:&#x2F;&#x2F;jenkins.io&#x2F;" rel="noopener" target="_blank">Jenkins</a></li><li class="links-of-blogroll-item"> <a href="http://cs231n.github.io/" title="http:&#x2F;&#x2F;cs231n.github.io&#x2F;" rel="noopener" target="_blank">cs231n</a></li><li class="links-of-blogroll-item"> <a href="https://pytorch.org/docs/stable/index.html" title="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;index.html" rel="noopener" target="_blank">pytorch</a></li><li class="links-of-blogroll-item"> <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" title="https:&#x2F;&#x2F;docs.docker.com&#x2F;install&#x2F;linux&#x2F;docker-ce&#x2F;ubuntu&#x2F;" rel="noopener" target="_blank">docker</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">zhujian</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">948k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">26:20</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-bar-chart-o"></i></span> <a href="https://tongji.baidu.com/web/27249108/overview/index?siteId=13647183" rel="noopener" target="_blank">百度统计</a></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div><div class="beian"> <a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备 19026415号</a> <span class="post-meta-divider">|</span> <img src="/images/beian_icon.png" style="display:inline-block;text-decoration:none;height:13px"> <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001748" rel="noopener" target="_blank">浙公网安备 33011802001748号</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span> <span class="site-uv" title="总访客量">访客数：<span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="site-pv" title="总访问量">阅读量：<span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="17,63,61" opacity="1" zindex="-1" count="199" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '8f22595c6dd29c94467d',
      clientSecret: 'e66bd90ebb9aa66c562c753dec6c90ceecbd51f2',
      repo: 'guestbook',
      owner: 'zjZSTU',
      admin: ['zjZSTU'],
      id: 'a0ba13aefbd994a8c2cd8f1247d933f8',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script></body></html>