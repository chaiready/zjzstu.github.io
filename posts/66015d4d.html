<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/./atom.xml" title="做一个幸福的人" type="application/atom+xml"><meta name="google-site-verification" content="Qr_3yqLtyErDFKHW7mE8PJz4qDUX-bf_fMLpSRckQe4"><meta name="baidu-site-verification" content="zOwIvKMV7f"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"搜索文章",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet"><meta name="description" content="参考：神经网络推导-单个数据输入批量数据到神经网络，进行前向传播和反向传播的推导TestNet网络TestNet是一个2层神经网络，结构如下："><meta name="keywords" content="微积分,线性代数,神经网络"><meta property="og:type" content="article"><meta property="og:title" content="神经网络推导-批量数据"><meta property="og:url" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;posts&#x2F;66015d4d.html"><meta property="og:site_name" content="做一个幸福的人"><meta property="og:description" content="参考：神经网络推导-单个数据输入批量数据到神经网络，进行前向传播和反向传播的推导TestNet网络TestNet是一个2层神经网络，结构如下："><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;神经网络推导-批量数据&#x2F;test_net.png"><meta property="og:updated_time" content="2020-02-15T05:36:35.875Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;神经网络推导-批量数据&#x2F;test_net.png"><link rel="canonical" href="https://www.zhujian.tech/posts/66015d4d.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>神经网络推导-批量数据 | 做一个幸福的人</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e677aac1ac69b8826b9cfecb4e72e107";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">做一个幸福的人</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">面朝大海，春暖花开</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">129</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">48</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">169</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header> <a href="https://github.com/zjZSTU" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zhujian.tech/posts/66015d4d.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="zhujian"><meta itemprop="description" content="one bite at a time"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="做一个幸福的人"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 神经网络推导-批量数据</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-05-06 14:43:47" itemprop="dateCreated datePublished" datetime="2019-05-06T14:43:47+00:00">2019-05-06</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-15 05:36:35" itemprop="dateModified" datetime="2020-02-15T05:36:35+00:00">2020-02-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/" itemprop="url" rel="index"><span itemprop="name">算法</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/deep-learning/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/math/" itemprop="url" rel="index"><span itemprop="name">数学</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/data-learning/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>23k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>38 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>参考：<a href="https://www.zhujian.tech/posts/cb820bb8.html">神经网络推导-单个数据</a></p><p>输入批量数据到神经网络，进行前向传播和反向传播的推导</p><h2 id="TestNet网络"><a href="#TestNet网络" class="headerlink" title="TestNet网络"></a>TestNet网络</h2><p><code>TestNet</code>是一个<code>2</code>层神经网络，结构如下：</p><a id="more"></a><ul><li>输入层有<code>3</code>个神经元</li><li>隐藏层有<code>4</code>个神经元</li><li>输出层有<code>2</code>个神经元</li></ul><p><img src="/imgs/神经网络推导-批量数据/test_net.png" alt></p><ul><li>激活函数为<code>relu</code>函数</li><li>评分函数为<code>softmax</code>回归</li><li>代价函数为交叉熵损失</li></ul><h3 id="网络符号定义"><a href="#网络符号定义" class="headerlink" title="网络符号定义"></a>网络符号定义</h3><p>规范神经网络的计算符号</p><p>关于神经元和层数</p><ul><li>$L$表示网络层数（不计入输入层）<ul><li>$L=2$，其中输入层是第<code>0</code>层，隐藏层是第<code>1</code>层，输出层是第<code>2</code>层</li></ul></li><li>$n^{(l)}$表示第$l$层的神经元个数（不包括偏置神经元）<ul><li>$n^{(0)}=3$，表示输入层神经元个数为<code>3</code></li><li>$n^{(1)}=4$，表示隐藏层神经元个数为<code>4</code></li><li>$n^{(2)}=2$，表示输出层神经元个数为<code>2</code></li></ul></li></ul><p>关于权重矩阵和偏置值</p><ul><li>$W^{(l)}$表示第$l-1$层到第$l$层的<strong>权重矩阵</strong>，矩阵行数为第$l-1$层的神经元个数，列数为第$l$层神经元个数<ul><li>$W^{(1)}$表示输入层到隐藏层的权重矩阵，大小为$R^{3\times 4}$</li><li>$W^{(2)}$表示隐藏层到输出层的权重矩阵，大小为$R^{4\times 2}$</li></ul></li><li>$W^{(l)}_{i,j}$表示第$l-1$层第$i$个神经元到第$l$第$j$个神经元的权值<ul><li>$i$的取值范围是$[1,n^{(l-1)}]$</li><li>$j$的取值范围是$[1, n^{(l)}]$</li></ul></li><li>$W^{(l)}_{i}$表示第$l-1$层第$i$个神经元对应的权重向量，大小为$n^{(l)}$</li><li>$W^{(l)}_{,j}$表示第$l$层第$j$个神经元对应的权重向量，大小为$n^{(l-1)}$</li><li>$b^{(l)}$表示第$l$层的<strong>偏置向量</strong><ul><li>$b^{(1)}$表示输入层到隐藏层的偏置向量，大小为$R^{1\times 4}$</li><li>$b^{(2)}$表示隐藏层到输出层的偏置向量，大小为$R^{1\times 2}$</li></ul></li><li>$b^{(l)}_{i}$表示第$l$层第$i$个神经元的偏置值<ul><li>$b^{(1)}_{2}$表示第$1$层隐藏层第$2$个神经元的偏置值</li></ul></li></ul><p>关于神经元输入向量和输出向量</p><ul><li>$a^{(l)}$表示第$l$层<strong>输出向量</strong>，$a^{(l)}=[a^{(l)}_{1},a^{(l)}_{2},…,a^{(l)}_{m}]^{T}$<ul><li>$a^{(0)}$表示输入层输出向量，大小为$R^{m\times 3}$</li><li>$a^{(1)}$表示隐藏层输出向量，大小为$R^{m\times 4}$</li><li>$a^{(2)}$表示输出层输出向量，大小为$R^{m\times 2}$</li></ul></li><li><p>$a^{(l)}_{i}$表示第$l$层第$i$个单元的输出值，其是输入向量经过激活计算后的值</p><ul><li>$a^{(1)}_{3}$表示隐含层第$3$个神经元的输入值，$a^{(1)}_{3}=g(z^{(1)}_{3})$</li></ul></li><li><p>$z^{(l)}$表示第$l$层<strong>输入向量</strong>，$z^{(l)}=[z^{(l)}_{1},z^{(l)}_{2},…,z^{(l)}_{m}]^{T}$</p><ul><li>$z^{(1)}$表示隐藏层的输入向量，大小为$R^{m\times 4}$</li><li>$z^{(2)}$表示输出层的输入向量，大小为$R^{m\times 2}$</li></ul></li><li>$z^{(l)}_{i,j}$表示第$l$层第$j$个单元的输入值，其是上一层输出向量第$i$个数据和该层第$j$个神经元权重向量的加权累加和<ul><li>$z^{(1)}_{1,2}$表示隐藏层第$2$个神经元的输入值，$z^{(1)}_{1,2}=b^{(2)}_{2}+a^{(0)}_{1,1}\cdot W^{(1)}_{1,2}+a^{(0)}_{1,2}\cdot W^{(1)}_{2,2}+a^{(0)}_{1,3}\cdot W^{(1)}_{3,2}$</li></ul></li></ul><p>关于神经元激活函数</p><ul><li>$g()$表示激活函数操作</li></ul><p>关于评分函数和损失函数</p><ul><li>$h()$表示评分函数操作</li><li>$J()$表示代价函数操作</li></ul><p><strong>神经元执行步骤</strong></p><p>神经元操作分为<code>2</code>步计算：</p><ol><li>输入向量$z^{(l)}$=前一层神经元输出向量$a^{(l-1)}$与权重矩阵$W^{(l)}$的加权累加和+偏置向量</li></ol><script type="math/tex;mode=display">
z^{(l)}_{i,j}=a^{(l-1)}_{i}\cdot W^{(l)}_{,j} + b^{(l)}_{j} \Rightarrow 
z^{(l)}=a^{(l-1)}\cdot W^{(l)} + b^{(l)}</script><ol><li>输出向量$a^{(l)}$=对输入向量$z^{(l)}$进行激活函数操作</li></ol><script type="math/tex;mode=display">
a^{(l)}_{i}=g(z_{i}^{(l)})
\Rightarrow 
a^{(l)}=g(z^{(l)})</script><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>对输入层</p><script type="math/tex;mode=display">
a^{(0)}
=\begin{bmatrix}
a^{(0)}_{1}\\ 
\vdots\\ 
a^{(0)}_{m}
\end{bmatrix}
=\begin{bmatrix}
a^{(0)}_{1,1} & a^{(0)}_{1,2} & a^{(0)}_{1,3}\\ 
\vdots & \vdots & \vdots\\ 
a^{(0)}_{m,1} & a^{(0)}_{m,2} & a^{(0)}_{m,3}
\end{bmatrix}\in R^{m\times 3}</script><p>对隐藏层</p><script type="math/tex;mode=display">
W^{(1)}
=\begin{bmatrix}
W^{(1)}_{1,1} & W^{(1)}_{1,2} & W^{(1)}_{1,3} & W^{(1)}_{1,4}\\ 
W^{(1)}_{2,1} & W^{(1)}_{2,2} & W^{(1)}_{2,3} & W^{(1)}_{2,4}\\ 
W^{(1)}_{3,1} & W^{(1)}_{3,2} & W^{(1)}_{3,3} & W^{(1)}_{3,4}
\end{bmatrix}
\in R^{3\times 4}</script><script type="math/tex;mode=display">
b^{(1)}=[[b^{(1)}_{1},b^{(1)}_{2},b^{(1)}_{3},b^{(1)}_{4}]]\in R^{1\times 4}</script><script type="math/tex;mode=display">
z^{(1)}
=\begin{bmatrix}
z^{(0)}_{1,1} & z^{(0)}_{1,2} & z^{(0)}_{1,3} & z^{(0)}_{1,4}\\ 
\vdots & \vdots & \vdots & \vdots\\ 
z^{(0)}_{m,1} & z^{(0)}_{m,2} & z^{(0)}_{m,3} & z^{(0)}_{m,4}
\end{bmatrix}\in R^{m\times 4}</script><script type="math/tex;mode=display">
a^{(1)}
=\begin{bmatrix}
a^{(0)}_{1,1} & a^{(0)}_{1,2} & a^{(0)}_{1,3} & a^{(0)}_{1,4}\\ 
\vdots & \vdots & \vdots & \vdots\\ 
a^{(0)}_{m,1} & a^{(0)}_{m,2} & a^{(0)}_{m,3} & a^{(0)}_{m,4}
\end{bmatrix}\in R^{m\times 4}</script><p>对输出层</p><script type="math/tex;mode=display">
W^{(2)}
=\begin{bmatrix}
W^{(2)}_{1,1} & W^{(2)}_{1,2}\\
W^{(2)}_{2,1} & W^{(2)}_{2,2}\\ 
W^{(2)}_{3,1} & W^{(2)}_{3,2}\\
W^{(2)}_{4,1} & W^{(2)}_{4,2}
\end{bmatrix}
\in R^{4\times 2}</script><script type="math/tex;mode=display">
b^{(2)}=[[b^{(2)}_{1},b^{(2)}_{2}]]\in R^{1\times 2}</script><script type="math/tex;mode=display">
z^{(2)}
=\begin{bmatrix}
z^{(2)}_{1,1} & z^{(0)}_{1,2}\\ 
\vdots & \vdots\\ 
z^{(2)}_{m,1} & z^{(0)}_{m,2}
\end{bmatrix}\in R^{m\times 2}</script><p>评分值</p><script type="math/tex;mode=display">
h(z^{(2)})
=\begin{bmatrix}
p(y_{1}=1) & p(y_{1}=2)\\ 
\vdots & \vdots\\ 
p(y_{m}=1) & p(y_{m}=2)
\end{bmatrix}\in R^{m\times 2}</script><p>损失值</p><script type="math/tex;mode=display">
J(z^{(2)})=(-1)\sum_{i=1}^{m} \sum_{j=1}^{2}\cdot 1(y_{m,j}=1)\ln p(y_{m,j}=1)\in R^{1}</script><h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p>输入层到隐藏层计算</p><script type="math/tex;mode=display">
z^{(1)}_{i,1}=a^{(0)}_{i}\cdot W^{(1)}_{,1}+b^{(1)}_{1}
=a^{(0)}_{i,1}\cdot W^{(1)}_{1,1} 
+a^{(0)}_{i,2}\cdot W^{(1)}_{2,1} 
+a^{(0)}_{i,3}\cdot W^{(1)}_{3,1} 
+b^{(1)}_{1,1}</script><script type="math/tex;mode=display">
z^{(1)}_{i,2}=a^{(0)}_{i}\cdot W^{(1)}_{,2}+b^{(1)}_{2}
=a^{(0)}_{i,1}\cdot W^{(1)}_{1,2} 
+a^{(0)}_{i,2}\cdot W^{(1)}_{2,2} 
+a^{(0)}_{i,3}\cdot W^{(1)}_{3,2} 
+b^{(1)}_{1,2}</script><script type="math/tex;mode=display">
z^{(1)}_{i,3}=a^{(0)}_{i}\cdot W^{(1)}_{,3}+b^{(1)}_{3}
=a^{(0)}_{i,1}\cdot W^{(1)}_{1,3} 
+a^{(0)}_{i,2}\cdot W^{(1)}_{2,3} 
+a^{(0)}_{i,3}\cdot W^{(1)}_{3,3} 
+b^{(1)}_{1,3}</script><script type="math/tex;mode=display">
z^{(1)}_{i,4}=a^{(0)}_{i}\cdot W^{(1)}_{,4}+b^{(1)}_{4}
=a^{(0)}_{i,1}\cdot W^{(1)}_{1,4} 
+a^{(0)}_{i,2}\cdot W^{(1)}_{2,4} 
+a^{(0)}_{i,3}\cdot W^{(1)}_{3,4} 
+b^{(1)}_{1,4}</script><script type="math/tex;mode=display">
\Rightarrow z^{(1)}_{i}
=[z^{(1)}_{i,1},z^{(1)}_{i,2},z^{(1)}_{i,3},z^{(1)}_{i,4}]
=a^{(0)}_{i}\cdot W^{(1)}+b^{(1)}</script><script type="math/tex;mode=display">
\Rightarrow z^{(1)}
=a^{(0)}\cdot W^{(1)}+b^{(1)}</script><p>隐藏层输入向量到输出向量</p><script type="math/tex;mode=display">
a^{(1)}_{i,1}=relu(z^{(1)}_{i,1}) \\
a^{(1)}_{i,2}=relu(z^{(1)}_{i,2}) \\
a^{(1)}_{i,3}=relu(z^{(1)}_{i,3}) \\
a^{(1)}_{i,4}=relu(z^{(1)}_{i,4})</script><script type="math/tex;mode=display">
\Rightarrow 
a^{(1)}_{i}=[a^{(1)}_{i,1},a^{(1)}_{i,2},a^{(1)}_{i,3},a^{(1)}_{i,4}]
=relu(z^{(1)}_{i})</script><script type="math/tex;mode=display">
\Rightarrow 
a^{(1)}=relu(z^{(1)})</script><p>隐藏层到输出层计算</p><script type="math/tex;mode=display">
z^{(2)}_{i,1}=a^{(1)}_{i}\cdot W^{(2)}_{,1}+b^{(2)}_{1,1}
=a^{(1)}_{i,1}\cdot W^{(2)}_{1,1}
+a^{(1)}_{i,2}\cdot W^{(2)}_{2,1}
+a^{(1)}_{i,3}\cdot W^{(2)}_{3,1}
+a^{(1)}_{i,4}\cdot W^{(2)}_{4,1}
+b^{(2)}_{1,1}</script><script type="math/tex;mode=display">
z^{(2)}_{i,2}=a^{(1)}_{i}\cdot W^{(2)}_{,2}+b^{(2)}_{1,2}
=a^{(1)}_{i,1}\cdot W^{(2)}_{1,2}
+a^{(1)}_{i,2}\cdot W^{(2)}_{2,2}
+a^{(1)}_{i,3}\cdot W^{(2)}_{3,2}
+a^{(1)}_{i,4}\cdot W^{(2)}_{4,2}
+b^{(2)}_{1,2}</script><script type="math/tex;mode=display">
\Rightarrow z^{(2)}_{i}
=[z^{(2)}_{i,1},z^{(2)}_{i,2}]
=a^{(1)}_{i}\cdot W^{(2)}+b^{(2)}</script><script type="math/tex;mode=display">
\Rightarrow z^{(2)}
=a^{(1)}\cdot W^{(2)}+b^{(2)}</script><p>评分操作</p><script type="math/tex;mode=display">
p(y_{i}=1)=\frac {exp(z^{(2)}_{i,1})}{\sum exp(z^{(2)}_{i})} \\
p(y_{i}=2)=\frac {exp(z^{(2)}_{i,2})}{\sum exp(z^{(2)}_{i})}</script><script type="math/tex;mode=display">
\Rightarrow h(z^{(2)}_{i})
=[p(y_{i}=1),p(y_{i}=2)]
=[\frac {exp(z^{(2)}_{i,1})}{\sum exp(z^{(2)}_{i})}, \frac {exp(z^{(2)}_{i,2})}{\sum exp(z^{(2)}_{i})}]</script><script type="math/tex;mode=display">
\Rightarrow h(z^{(2)})
=\begin{bmatrix}
p(y_{1}=1) & p(y_{1}=2) \\ 
\vdots & \vdots\\ 
p(y_{m}=1) & p(y_{m}=2)
\end{bmatrix}</script><p>损失值</p><script type="math/tex;mode=display">
J(z^{(2)})=(-1)\sum_{i=1}^{m} \sum_{j=1}^{2}\cdot 1(y_{m,j}=1)\ln p(y_{m,j}=1)</script><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>计算输出层输入向量梯度</p><script type="math/tex;mode=display">
\frac {\partial J}{\partial z^{(2)}_{i,1}}=
(-1)\cdot \frac {1(y_{i}=1)}{p(y_{i}=1)}\cdot \frac {\partial p(y_{i}=1)}{\partial z^{(2)}_{i,1}}
+(-1)\cdot \frac {1(y_{i}=2)}{p(y_{i}=2)}\cdot \frac {\partial p(y_{i}=2)}{\partial z^{(2)}_{i,1}}</script><script type="math/tex;mode=display">
\frac {\partial p(y_{i}=1)}{\partial z^{(2)}_{i,1}}
=\frac {exp(z^{(2)}_{i,1})\cdot \sum exp(z^{(2)}_{i})-exp(z^{(2)}_{i,1})\cdot exp(z^{(2)}_{i,1})}{(\sum exp(z^{(2)}_{i}))^2}
=\frac {exp(z^{(2)}_{i,1})}{\sum exp(z^{(2)}_{i})}
-(\frac {exp(z^{(2)}_{i,1})}{\sum exp(z^{(2)}_{i})})^2
=p(y_{i}=1)-(p(y_{i}=1))^2</script><script type="math/tex;mode=display">
\frac {\partial p(y_{i}=2)}{\partial z^{(2)}_{i,1}}
=\frac {-exp(z^{(2)}_{i,2})\cdot exp(z^{(2)}_{i,1})}{(\sum exp(z^{(2)}_{i}))^2}
=(-1)\cdot \frac {exp(z^{(2)}_{i,1})}{\sum exp(z^{(2)}_{i})}\cdot \frac {exp(z^{(2)}_{i,2})}{\sum exp(z^{(2)}_{i})}
=(-1)\cdot p(y_{i}=1)p(y_{i}=2)</script><script type="math/tex;mode=display">
\Rightarrow \frac {\partial J}{\partial z^{(2)}_{i,1}}
=(-1)\cdot \frac {1(y_{i}=1)}{p(y_{i}=1)}\cdot (p(y_{i}=1)-(p(y_{i}=1))^2)
+(-1)\cdot \frac {1(y_{i}=2)}{p(y_{i}=2)}\cdot (-1)\cdot p(y_{i}=1)p(y_{i}=2) \\
=(-1)\cdot 1(y_{i}=1)\cdot (1-p(y_{i}=1))
+1(y_{i}=2)\cdot p(y_{i}=1)
=p(y_{i}=1)-1(y_{i}=1)</script><script type="math/tex;mode=display">
\Rightarrow \frac {\partial J}{\partial z^{(2)}_{i,2}}
=p(y_{i}=2)-1(y_{i}=2)</script><script type="math/tex;mode=display">
\Rightarrow \frac {\partial J}{\partial z^{(2)}_{i}}
=[p(y_{i}=1)-1(y_{i}=1), p(y_{i}=2)-1(y_{i}=2)]</script><script type="math/tex;mode=display">
\Rightarrow \frac {\partial J}{\partial z^{(2)}}
=\begin{bmatrix}
p(y_{1}=1)-1(y_{1}=1) & p(y_{1}=2)-1(y_{1}=2)\\ 
\vdots & \vdots\\ 
p(y_{m}=1)-1(y_{m}=1) & p(y_{m}=2)-1(y_{m}=2)
\end{bmatrix}</script><p>计算输出层权重向量梯度</p><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(2)}_{1,1}}
=\frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(2)}_{i,1}}\cdot \frac {\partial z^{(2)}_{i,1}}{\partial W^{(2)}_{1,1}}
=\frac {1}{m}\sum_{i=1}^{m} ((p(y_{i}=1)-1(y_{i}=1))\cdot a^{(1)}_{i,1})</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(2)}_{2,1}}
=\frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(2)}_{i,1}}\cdot \frac {\partial z^{(2)}_{i,1}}{\partial W^{(2)}_{2,1}}
=\frac {1}{m}\sum_{i=1}^{m} ((p(y_{i}=1)-1(y_{i}=1))\cdot a^{(1)}_{i,2})</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(2)}_{3,1}}
=\frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(2)}_{i,1}}\cdot \frac {\partial z^{(2)}_{i,1}}{\partial W^{(2)}_{3,1}}
=\frac {1}{m}\sum_{i=1}^{m} ((p(y_{i}=1)-1(y_{i}=1))\cdot a^{(1)}_{i,3})</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(2)}_{4,1}}
=\frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(2)}_{i,1}}\cdot \frac {\partial z^{(2)}_{i,1}}{\partial W^{(2)}_{4,1}}
=\frac {1}{m}\sum_{i=1}^{m} ((p(y_{i}=1)-1(y_{i}=1))\cdot a^{(1)}_{i,4})</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(2)}_{1,2}}
=\frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(2)}_{i,2}}\cdot \frac {\partial z^{(2)}_{2}}{\partial W^{(2)}_{1,2}}
=\frac {1}{m}\sum_{i=1}^{m} ((p(y_{i}=2)-1(y_{i}=2))\cdot a^{(1)}_{i,1})</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(2)}_{2,2}}
=\frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(2)}_{i,2}}\cdot \frac {\partial z^{(2)}_{2}}{\partial W^{(2)}_{2,2}}
=\frac {1}{m}\sum_{i=1}^{m} ((p(y_{i}=2)-1(y_{i}=2))\cdot a^{(1)}_{i,2})</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(2)}_{3,2}}
=\frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(2)}_{i,2}}\cdot \frac {\partial z^{(2)}_{2}}{\partial W^{(2)}_{3,2}}
=\frac {1}{m}\sum_{i=1}^{m} ((p(y_{i}=2)-1(y_{i}=2))\cdot a^{(1)}_{i,3})</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(2)}_{4,2}}
=\frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(2)}_{i,2}}\cdot \frac {\partial z^{(2)}_{2}}{\partial W^{(2)}_{4,2}}
=\frac {1}{m}\sum_{i=1}^{m} ((p(y_{i}=2)-1(y_{i}=2))\cdot a^{(1)}_{i,4})</script><script type="math/tex;mode=display">
\Rightarrow \frac {\partial J}{\partial W^{(2)}}
=\begin{bmatrix}
\frac {\partial J}{\partial W^{(2)}_{1,1}} & \frac {\partial J}{\partial W^{(2)}_{1,2}}\\
\frac {\partial J}{\partial W^{(2)}_{2,1}} & \frac {\partial J}{\partial W^{(2)}_{2,2}}\\ 
\frac {\partial J}{\partial W^{(2)}_{3,1}} & \frac {\partial J}{\partial W^{(2)}_{3,2}}\\
\frac {\partial J}{\partial W^{(2)}_{4,1}} & \frac {\partial J}{\partial W^{(2)}_{4,2}}
\end{bmatrix}</script><script type="math/tex;mode=display">
=\begin{bmatrix}
\frac {1}{m}\sum_{i=1}^{m} ((p(y_{i}=1)-1(y_{i}=1))\cdot a^{(1)}_{i,1}) & \frac {1}{m}\sum_{i=1}^{m} ((p(y_{i}=1)-1(y_{i}=1))\cdot a^{(1)}_{i,2})\\ 
\frac {1}{m}\sum_{i=1}^{m} ((p(y_{i}=1)-1(y_{i}=1))\cdot a^{(1)}_{i,3}) & \frac {1}{m}\sum_{i=1}^{m} ((p(y_{i}=1)-1(y_{i}=1))\cdot a^{(1)}_{i,4})\\
\frac {1}{m}\sum_{i=1}^{m} ((p(y_{i}=2)-1(y_{i}=2))\cdot a^{(1)}_{i,1}) & \frac {1}{m}\sum_{i=1}^{m} ((p(y_{i}=2)-1(y_{i}=2))\cdot a^{(1)}_{i,2})\\ 
\frac {1}{m}\sum_{i=1}^{m} ((p(y_{i}=2)-1(y_{i}=2))\cdot a^{(1)}_{i,3}) & \frac {1}{m}\sum_{i=1}^{m} ((p(y_{i}=2)-1(y_{i}=2))\cdot a^{(1)}_{i,4})
\end{bmatrix}</script><script type="math/tex;mode=display">
=\frac {1}{m}\sum_{i=1}^{m}
\begin{bmatrix}
a^{(1)}_{i,1}\\ 
a^{(1)}_{i,2}\\ 
a^{(1)}_{i,3}\\ 
a^{(1)}_{i,4}
\end{bmatrix} 
\begin{bmatrix}
p(y_{i}=1)-1(y_{i}=1) & p(y_{i}=2)-1(y_{i}=2)
\end{bmatrix}\\
=\frac {1}{m}\sum_{i=1}^{m} ((a^{(1)}_{i})^{T}\cdot \frac {\partial J}{\partial z^{(2)}_{i}}) 
=\frac {1}{m} (a^{(1)})^{T}\cdot \frac {\partial J}{\partial z^{(2)}}
=\frac {1}{m}\sum_{i=1}^{m} (R^{4\times m}\cdot R^{m\times 2})
=R^{4\times 2}</script><p>计算隐藏层输出向量梯度</p><script type="math/tex;mode=display">
\frac {\partial J}{\partial a^{(1)}_{i,1}}
=\frac {\partial J}{\partial z^{(2)}_{i,1}}\cdot \frac {\partial z^{(2)}_{i,1}}{\partial a^{(1)}_{i,1}}
+\frac {\partial J}{\partial z^{(2)}_{i,2}}\cdot \frac {\partial z^{(2)}_{i,2}}{\partial a^{(1)}_{i,1}}
=(p(y_{i}=1)-1(y_{i}=1))\cdot W^{(2)}_{1,1}
+(p(y_{i}=2)-1(y_{i}=2))\cdot W^{(2)}_{1,2}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial a^{(1)}_{i,2}}
=\frac {\partial J}{\partial z^{(2)}_{i,1}}\cdot \frac {\partial z^{(2)}_{i,1}}{\partial a^{(1)}_{i,2}}
+\frac {\partial J}{\partial z^{(2)}_{i,2}}\cdot \frac {\partial z^{(2)}_{i,2}}{\partial a^{(1)}_{i,2}}
=(p(y_{i}=1)-1(y_{i}=1))\cdot W^{(2)}_{2,1}
+(p(y_{i}=2)-1(y_{i}=2))\cdot W^{(2)}_{2,2}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial a^{(1)}_{i,3}}
=\frac {\partial J}{\partial z^{(2)}_{i,1}}\cdot \frac {\partial z^{(2)}_{i,1}}{\partial a^{(1)}_{i,3}}
+\frac {\partial J}{\partial z^{(2)}_{i,2}}\cdot \frac {\partial z^{(2)}_{i,2}}{\partial a^{(1)}_{i,3}}
=(p(y_{i}=1)-1(y_{i}=1))\cdot W^{(2)}_{3,1}
+(p(y_{i}=2)-1(y_{i}=2))\cdot W^{(2)}_{3,2}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial a^{(1)}_{i,4}}
=\frac {\partial J}{\partial z^{(2)}_{i,1}}\cdot \frac {\partial z^{(2)}_{i,1}}{\partial a^{(1)}_{i,4}}
+\frac {\partial J}{\partial z^{(2)}_{i,2}}\cdot \frac {\partial z^{(2)}_{i,2}}{\partial a^{(1)}_{i,4}}
=(p(y_{i}=1)-1(y_{i}=1))\cdot W^{(2)}_{4,1}
+(p(y_{i}=2)-1(y_{i}=2))\cdot W^{(2)}_{4,2}</script><script type="math/tex;mode=display">
\Rightarrow \frac {\partial J}{\partial a^{(1)}_{i}}
=\begin{bmatrix}
p(y_{i}=1)-1(y_{i}=1) & p(y_{i}=2)-1(y_{i}=2)
\end{bmatrix}
\begin{bmatrix}
W^{(2)}_{1,1} & W^{(2)}_{2,1} & W^{(2)}_{3,1} & W^{(2)}_{4,1}\\ 
W^{(2)}_{1,2} & W^{(2)}_{2,2} & W^{(2)}_{3,2} & W^{(2)}_{4,2}
\end{bmatrix} \\
=\frac {\partial J}{\partial z^{(2)}_{i}}\cdot (W^{(2)})^T
=R^{1\times 2}\cdot R^{2\times 4}
=R^{1\times 4}</script><script type="math/tex;mode=display">
\Rightarrow \frac {\partial J}{\partial a^{(1)}}
=\begin{bmatrix}
p(y_{1}=1)-1(y_{1}=1) & p(y_{1}=2)-1(y_{1}=2)\\ 
\vdots & \vdots\\ 
p(y_{m}=1)-1(y_{m}=1) & p(y_{m}=2)-1(y_{m}=2)
\end{bmatrix}
\begin{bmatrix}
W^{(2)}_{1,1} & W^{(2)}_{2,1} & W^{(2)}_{3,1} & W^{(2)}_{4,1}\\ 
W^{(2)}_{1,2} & W^{(2)}_{2,2} & W^{(2)}_{3,2} & W^{(2)}_{4,2}
\end{bmatrix} \\
=\frac {\partial J}{\partial z^{(2)}}\cdot (W^{(2)})^T
=R^{m\times 2}\cdot R^{2\times 4}
=R^{m\times 4}</script><p>计算隐藏层输入向量的梯度</p><script type="math/tex;mode=display">
\frac {\partial J}{\partial z^{(1)}_{i,1}}
=\frac {\partial J}{\partial a^{(1)}_{i,1}}\cdot \frac {\partial a^{(1)}_{i,1}}{\partial z^{(1)}_{i,1}}
=((p(y_{i}=1)-1(y_{i}=1))\cdot W^{(2)}_{1,1}
+(p(y=2)-1(y=2))\cdot W^{(2)}_{1,2})\cdot 1(z^{(1)}_{i,1}\geq 0)</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial z^{(1)}_{i,2}}
=\frac {\partial J}{\partial a^{(1)}_{i,2}}\cdot \frac {\partial a^{(1)}_{i,2}}{\partial z^{(1)}_{i,2}}
=((p(y_{i}=1)-1(y_{i}=1))\cdot W^{(2)}_{2,1}
+(p(y_{i}=2)-1(y_{i}=2))\cdot W^{(2)}_{2,2})\cdot 1(z^{(1)}_{i,2}\geq 0)</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial z^{(1)}_{i,3}}
=\frac {\partial J}{\partial a^{(1)}_{i,3}}\cdot \frac {\partial a^{(1)}_{i,3}}{\partial z^{(1)}_{i,3}}
=((p(y_{i}=1)-1(y_{i}=1))\cdot W^{(2)}_{3,1}
+(p(y_{i}=2)-1(y_{i}=2))\cdot W^{(2)}_{3,2})\cdot 1(z^{(1)}_{i,3}\geq 0)</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial z^{(1)}_{i,4}}
=\frac {\partial J}{\partial a^{(1)}_{i,4}}\cdot \frac {\partial a^{(1)}_{i,4}}{\partial z^{(1)}_{i,4}}
=((p(y_{i}=1)-1(y_{i}=1))\cdot W^{(2)}_{4,1}
+(p(y_{i}=2)-1(y_{i}=2))\cdot W^{(2)}_{4,2})\cdot 1(z^{(1)}_{i,4}\geq 0)</script><script type="math/tex;mode=display">
\Rightarrow \frac {\partial J}{\partial z^{(1)}_{i}}
=(\begin{bmatrix}
p(y_{i}=1)-1(y_{i}=1) & p(y_{i}=2)-1(y_{i}=2)
\end{bmatrix}
\begin{bmatrix}
W^{(2)}_{1,1} & W^{(2)}_{2,1} & W^{(2)}_{3,1} & W^{(2)}_{4,1}\\ 
W^{(2)}_{1,2} & W^{(2)}_{2,2} & W^{(2)}_{3,2} & W^{(2)}_{4,2}
\end{bmatrix})
*\begin{bmatrix}
\frac {\partial a^{(1)}_{i,1}}{\partial z^{(1)}_{i,1}}&
\frac {\partial a^{(1)}_{i,2}}{\partial z^{(1)}_{i,2}}& 
\frac {\partial a^{(1)}_{i,3}}{\partial z^{(1)}_{i,3}}& 
\frac {\partial a^{(1)}_{i,4}}{\partial z^{(1)}_{i,4}}
\end{bmatrix}\\
=(R^{1\times 2}\cdot R^{2\times 4})* R^{1\times 4}
=R^{1\times 4}</script><script type="math/tex;mode=display">
\Rightarrow \frac {\partial J}{\partial z^{(1)}_{i}}
=(\begin{bmatrix}
p(y_{i}=1)-1(y_{i}=1) & p(y_{i}=2)-1(y_{i}=2)
\end{bmatrix}
\begin{bmatrix}
W^{(2)}_{1,1} & W^{(2)}_{2,1} & W^{(2)}_{3,1} & W^{(2)}_{4,1}\\ 
W^{(2)}_{1,2} & W^{(2)}_{2,2} & W^{(2)}_{3,2} & W^{(2)}_{4,2}
\end{bmatrix})
*
\begin{bmatrix}
1(z^{(1)}_{i,1}\geq 0) & 1(z^{(1)}_{i,2}\geq 0) & 1(z^{(1)}_{i,3}\geq 0) & 1(z^{(1)}_{i,4}\geq 0)
\end{bmatrix}\\
=(R^{1\times 2}\cdot R^{2\times 4})\ast  R^{1\times 4}
=R^{1\times 4}</script><script type="math/tex;mode=display">
\Rightarrow \frac {\partial J}{\partial z^{(1)}}
=(\begin{bmatrix}
p(y_{1}=1)-1(y_{1}=1) & p(y_{1}=2)-1(y_{1}=2)\\ 
\vdots & \vdots\\ 
p(y_{m}=1)-1(y_{m}=1) & p(y_{m}=2)-1(y_{m}=2)
\end{bmatrix}
\begin{bmatrix}
W^{(2)}_{1,1} & W^{(2)}_{2,1} & W^{(2)}_{3,1} & W^{(2)}_{4,1}\\ 
W^{(2)}_{1,2} & W^{(2)}_{2,2} & W^{(2)}_{3,2} & W^{(2)}_{4,2}
\end{bmatrix})
*
\begin{bmatrix}
1(z^{(1)}_{1,1}\geq 0) & 1(z^{(1)}_{1,2}\geq 0) & 1(z^{(1)}_{1,3}\geq 0) & 1(z^{(1)}_{1,4}\geq 0)\\
\vdots & \vdots\\ 
1(z^{(1)}_{m,1}\geq 0) & 1(z^{(1)}_{m,2}\geq 0) & 1(z^{(1)}_{m,3}\geq 0) & 1(z^{(1)}_{m,4}\geq 0)
\end{bmatrix}\\
=\frac {\partial J}{\partial a^{(1)}} * 1(z^{(1)}\geq 0)
=(R^{m\times 2}\cdot R^{2\times 4})\ast R^{m\times 4}
=R^{m\times 4}</script><p>计算隐藏层权重向量的梯度</p><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(1)}_{1,1}}
=\frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(1)}_{i,1}}\cdot
\frac {\partial z^{(1)}_{i,1}}{\partial W^{(1)}_{1,1}}
=\frac {1}{m}\sum_{i=1}^{m} ((p(y_{i}=1)-1(y_{i}=1))\cdot W^{(2)}_{1,1}
+(p(y_{i}=2)-1(y_{i}=2))\cdot W^{(2)}_{1,2})\cdot 1(z^{(1)}_{i,1}\geq 0)\cdot a^{(0)}_{i,1}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(1)}_{1,2}}
=\frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(1)}_{i,2}}\cdot
\frac {\partial z^{(1)}_{i,2}}{\partial W^{(1)}_{1,2}}
=\frac {1}{m}\sum_{i=1}^{m} ((p(y_{i}=1)-1(y_{i}=1))\cdot W^{(2)}_{2,1}
+(p(y_{i}=2)-1(y_{i}=2))\cdot W^{(2)}_{2,2})\cdot 1(z^{(1)}_{i,2}\geq 0)\cdot a^{(0)}_{i,1}</script><script type="math/tex;mode=display">
\Rightarrow \frac {\partial J}{\partial W^{(1)}_{k,l}}
=\frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(1)}_{i,l}}\cdot
\frac {\partial z^{(1)}_{i,l}}{\partial W^{(1)}_{k,l}}
=\frac {1}{m}\sum_{i=1}^{m} ((p(y_{i}=1)-1(y_{i}=1))\cdot W^{(2)}_{l,1}
+(p(y_{i}=2)-1(y_{i}=2))\cdot W^{(2)}_{l,2})\cdot 1(z^{(1)}_{i,l}\geq 0)\cdot a^{(0)}_{i,k}</script><script type="math/tex;mode=display">
\Rightarrow \frac {\partial J}{\partial W^{(1)}}
=\begin{bmatrix}
\frac {\partial J}{\partial W^{(1)}_{1,1}} & \frac {\partial J}{\partial W^{(1)}_{1,2}} & \frac {\partial J}{\partial W^{(1)}_{1,3}}  & \frac {\partial J}{\partial W^{(1)}_{1,4}}\\ 
\frac {\partial J}{\partial W^{(1)}_{2,1}} & \frac {\partial J}{\partial W^{(1)}_{2,2}} & \frac {\partial J}{\partial W^{(1)}_{2,3}}  & \frac {\partial J}{\partial W^{(1)}_{2,4}}\\ 
\frac {\partial J}{\partial W^{(1)}_{3,1}} & \frac {\partial J}{\partial W^{(1)}_{3,2}} & \frac {\partial J}{\partial W^{(1)}_{3,3}}  & \frac {\partial J}{\partial W^{(1)}_{3,4}}
\end{bmatrix}\\
=\begin{bmatrix}
\frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(1)}_{i,1}}\cdot \frac {\partial z^{(1)}_{i,1}}{\partial W^{(1)}_{1,1}} 
& \frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(1)}_{i,2}}\cdot \frac {\partial z^{(1)}_{i,2}}{\partial W^{(1)}_{1,2}} 
& \frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(1)}_{i,3}}\cdot \frac {\partial z^{(1)}_{i,3}}{\partial W^{(1)}_{1,3}}  
& \frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(1)}_{i,4}}\cdot \frac {\partial z^{(1)}_{i,4}}{\partial W^{(1)}_{1,4}}\\ 
\frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(1)}_{i,1}}\cdot \frac {\partial z^{(1)}_{i,1}}{\partial W^{(1)}_{2,1}} 
& \frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(1)}_{i,2}}\cdot \frac {\partial z^{(1)}_{i,2}}{\partial W^{(1)}_{2,2}} 
& \frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(1)}_{i,3}}\cdot \frac {\partial z^{(1)}_{i,3}}{\partial W^{(1)}_{2,3}}  
& \frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(1)}_{i,4}}\cdot \frac {\partial z^{(1)}_{i,4}}{\partial W^{(1)}_{2,4}}\\ 
\frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(1)}_{i,1}}\cdot \frac {\partial z^{(1)}_{i,1}}{\partial W^{(1)}_{3,1}} 
& \frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(1)}_{i,2}}\cdot \frac {\partial z^{(1)}_{i,2}}{\partial W^{(1)}_{3,2}} 
& \frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(1)}_{i,3}}\cdot \frac {\partial z^{(1)}_{i,3}}{\partial W^{(1)}_{3,3}}  
& \frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(1)}_{i,4}}\cdot \frac {\partial z^{(1)}_{i,4}}{\partial W^{(1)}_{3,4}}
\end{bmatrix}\\
=\frac {1}{m}\sum_{i=1}^{m} \begin{bmatrix}
\frac {\partial J}{\partial z^{(1)}_{i,1}}\cdot \frac {\partial z^{(1)}_{i,1}}{\partial W^{(1)}_{1,1}} 
& \frac {\partial J}{\partial z^{(1)}_{i,2}}\cdot \frac {\partial z^{(1)}_{i,2}}{\partial W^{(1)}_{1,2}} 
& \frac {\partial J}{\partial z^{(1)}_{i,3}}\cdot \frac {\partial z^{(1)}_{i,3}}{\partial W^{(1)}_{1,3}}  
& \frac {\partial J}{\partial z^{(1)}_{i,4}}\cdot \frac {\partial z^{(1)}_{i,4}}{\partial W^{(1)}_{1,4}}\\ 
\frac {\partial J}{\partial z^{(1)}_{i,1}}\cdot \frac {\partial z^{(1)}_{i,1}}{\partial W^{(1)}_{2,1}} 
& \frac {\partial J}{\partial z^{(1)}_{i,2}}\cdot \frac {\partial z^{(1)}_{i,2}}{\partial W^{(1)}_{2,2}} 
& \frac {\partial J}{\partial z^{(1)}_{i,3}}\cdot \frac {\partial z^{(1)}_{i,3}}{\partial W^{(1)}_{2,3}}  
& \frac {\partial J}{\partial z^{(1)}_{i,4}}\cdot \frac {\partial z^{(1)}_{i,4}}{\partial W^{(1)}_{2,4}}\\ 
\frac {\partial J}{\partial z^{(1)}_{i,1}}\cdot \frac {\partial z^{(1)}_{i,1}}{\partial W^{(1)}_{3,1}} 
& \frac {\partial J}{\partial z^{(1)}_{i,2}}\cdot \frac {\partial z^{(1)}_{i,2}}{\partial W^{(1)}_{3,2}} 
&  \frac {\partial J}{\partial z^{(1)}_{i,3}}\cdot \frac {\partial z^{(1)}_{i,3}}{\partial W^{(1)}_{3,3}}  
& \frac {\partial J}{\partial z^{(1)}_{i,4}}\cdot \frac {\partial z^{(1)}_{i,4}}{\partial W^{(1)}_{3,4}}
\end{bmatrix}\\
=\frac {1}{m}\sum_{i=1}^{m} \begin{bmatrix}
\frac {\partial J}{\partial z^{(1)}_{i,1}}\cdot a^{(0)}_{i,1} 
& \frac {\partial J}{\partial z^{(1)}_{i,2}}\cdot a^{(0)}_{i,1}
& \frac {\partial J}{\partial z^{(1)}_{i,3}}\cdot a^{(0)}_{i,1}
& \frac {\partial J}{\partial z^{(1)}_{i,4}}\cdot a^{(0)}_{i,1}\\ 
\frac {\partial J}{\partial z^{(1)}_{i,1}}\cdot a^{(0)}_{i,2}
& \frac {\partial J}{\partial z^{(1)}_{i,2}}\cdot a^{(0)}_{i,2}
& \frac {\partial J}{\partial z^{(1)}_{i,3}}\cdot a^{(0)}_{i,2}
& \frac {\partial J}{\partial z^{(1)}_{i,4}}\cdot a^{(0)}_{i,2}\\ 
\frac {\partial J}{\partial z^{(1)}_{i,1}}\cdot a^{(0)}_{i,3}
& \frac {\partial J}{\partial z^{(1)}_{i,2}}\cdot a^{(0)}_{i,3}
&  \frac {\partial J}{\partial z^{(1)}_{i,3}}\cdot a^{(0)}_{i,3}
& \frac {\partial J}{\partial z^{(1)}_{i,4}}\cdot a^{(0)}_{i,3}
\end{bmatrix}\\
=\frac {1}{m}\sum_{i=1}^{m} 
\begin{bmatrix}
a^{(0)}_{i,1}\\
a^{(0)}_{i,2}\\
a^{(0)}_{i,3}
\end{bmatrix}
\begin{bmatrix}
\frac {\partial J}{\partial z^{(1)}_{i,1}} 
& \frac {\partial J}{\partial z^{(1)}_{i,2}}
& \frac {\partial J}{\partial z^{(1)}_{i,3}}
& \frac {\partial J}{\partial z^{(1)}_{i,4}} 
\end{bmatrix}
=\frac {1}{m}\sum_{i=1}^{m} (a^{(0)}_{i})^T\cdot \frac {\partial J}{\partial z^{(1)}_{i}}
=\frac {1}{m} (a^{(0)})^T\cdot \frac {\partial J}{\partial z^{(1)}}
=R^{3\times m}\cdot R^{m\times 4}
=R^{3\times 4}</script><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p><code>TestNet</code>网络的前向操作如下：</p><script type="math/tex;mode=display">
z^{(1)}
=a^{(0)}\cdot W^{(1)}+b^{(1)}</script><script type="math/tex;mode=display">
a^{(1)}=relu(z^{(1)})</script><script type="math/tex;mode=display">
z^{(2)}
=a^{(1)}\cdot W^{(2)}+b^{(2)}</script><script type="math/tex;mode=display">
h(z^{(2)})
=\begin{bmatrix}
p(y_{1}=1) & p(y_{1}=2) \\ 
\vdots & \vdots\\ 
p(y_{m}=1) & p(y_{m}=2)
\end{bmatrix}</script><script type="math/tex;mode=display">
J(z^{(2)})=(-1)\sum_{i=1}^{m} \sum_{j=1}^{2}\cdot 1(y_{m,j}=1)\ln p(y_{m,j}=1)</script><p>反向传播如下：</p><script type="math/tex;mode=display">
\frac {\partial J}{\partial z^{(2)}}
=\begin{bmatrix}
p(y_{1}=1)-1(y_{1}=1) & p(y_{1}=2)-1(y_{1}=2)\\ 
\vdots & \vdots\\ 
p(y_{m}=1)-1(y_{m}=1) & p(y_{m}=2)-1(y_{m}=2)
\end{bmatrix}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(2)}}
=\frac {1}{m} (a^{(1)})^{T}\cdot \frac {\partial J}{\partial z^{(2)}}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial b^{(2)}}
=\frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(2)}_{i}}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial a^{(1)}}
=\frac {\partial J}{\partial z^{(2)}}\cdot (W^{(2)})^T</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial z^{(1)}}
=\frac {\partial J}{\partial a^{(1)}} * 1(z^{(1)}\geq 0)</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial W^{(1)}}
=\frac {1}{m} (a^{(0)})^T\cdot \frac {\partial J}{\partial z^{(1)}}</script><script type="math/tex;mode=display">
\frac {\partial J}{\partial b^{(1)}}
=\frac {1}{m}\sum_{i=1}^{m} \frac {\partial J}{\partial z^{(1)}_{i}}</script><p>假设批量数据大小为$m$，数据维数为$D$，网络层数为$L$（$1,2,…,l,…,L$），输出类别为$C$</p><p>参考<a href="http://ufldl.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95" target="_blank" rel="noopener">反向传导算法</a>和<a href="https://zhuanlan.zhihu.com/p/22473137" target="_blank" rel="noopener">神经网络反向传播的数学原理</a>，设每层输入向量为残差$\delta^{(l)}=\frac{\partial J(W, b)}{\partial z^{(l)}}$，用于表示该层对最终输出值的残差造成的影响；而最终输出值的残差$\delta^{(L)}$就是损失函数对输出层输入向量的梯度</p><p>前向传播执行步骤</p><ol><li><p>层与层之间的操作就是输出向量和权值矩阵的加权求和以及对输入向量的函数激活（<em>以relu为例</em>）</p><script type="math/tex;mode=display">
 z^{(l)} = a^{(l-1)}\cdot W^{(l)}+b^{(l)} \\
 a^{(l)} = relu(z^{(l)})</script></li><li><p>输出层输出结果后，进行评分函数的计算，得到最终的计算结果（<em>以softmax分类为例</em>）</p><script type="math/tex;mode=display">
 h(z^{(L)})
 =\begin{bmatrix}
 p(y_{1}=1) & \dots & p(y_{1}=C) \\ 
 \vdots & \vdots & \vdots\\ 
 p(y_{m}=1) & \dots & p(y_{m}=C)
 \end{bmatrix}
 =\begin{bmatrix}
 \frac {exp(z^{(2)}_{1,1})}{\sum exp(z^{(2)}_{1})} & \dots & \frac {exp(z^{(2)}_{1,C})}{\sum exp(z^{(2)}_{1})} \\ 
 \vdots & \vdots & \vdots\\ 
 \frac {exp(z^{(2)}_{m,1})}{\sum exp(z^{(2)}_{m})} & \dots & \frac {exp(z^{(2)}_{m,C})}{\sum exp(z^{(2)}_{m})}
 \end{bmatrix}</script></li><li><p>损失函数根据计算结果判断最终损失值（<em>以交叉熵损失为例</em>）</p><script type="math/tex;mode=display">
 J(z^{(L)})=(-1)\sum_{i=1}^{m} \sum_{j=1}^{2}\cdot 1(y_{m,j}=1)\ln p(y_{m,j}=1)</script></li></ol><p>反向传播执行步骤</p><ol><li><p>计算损失函数对于输出层输入向量的梯度(最终层残差)</p><script type="math/tex;mode=display">
 \delta^{(L)}=
 \frac {\partial J}{\partial z^{(L)}}
 =\begin{bmatrix}
 p(y_{1}=1)-1(y_{1}=1) & \dots & p(y_{1}=C)-1(y_{1}=C)\\ 
 \vdots & \vdots & \vdots\\ 
 p(y_{m}=1)-1(y_{m}=1) & \dots & p(y_{m}=C)-1(y_{m}=C)
 \end{bmatrix}</script></li><li><p>计算中间隐藏层的残差值（$L-1,L-2,…1$）</p><script type="math/tex;mode=display">
 \delta^{(l)}=
 \frac{\varphi J}{\varphi z^{(l)}}
 =(\frac{\varphi J}{\varphi z^{(l+1)}}\cdot \frac{\varphi z^{(l+1)}}{\varphi a^{(l)}})
 *\frac{\varphi a^{(l)}}{\varphi z^{(l)}}
 =(\delta^{(l+1)}\cdot (W^{(l+1)})^{T})
 *1(z^{(l)}\geq 0)</script></li><li><p>完成所有的可学习参数（权值矩阵和偏置向量）的梯度计算</p><script type="math/tex;mode=display">
 \nabla_{W^{(l)}} J(W, b)= \frac {1}{m} (a^{(l-1)})^{T}\cdot \delta^{(l)}\\
 \nabla_{b^{(l)}} J(W, b)= \frac {1}{m}\sum_{i=1}^{m} \delta^{(l)}_{i}</script></li><li><p>更新权值矩阵和偏置向量</p><script type="math/tex;mode=display">
 W^{(l)}=W^{(l)}-\alpha\left[\nabla W^{(l)}+\lambda W^{(l)}\right] \\
 b^{(l)}=b^{(l)}-\alpha \nabla b^{(l)}</script></li></ol></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.jpg" alt="zhujian 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="zhujian 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zhujian</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zhujian.tech/posts/66015d4d.html" title="神经网络推导-批量数据">https://www.zhujian.tech/posts/66015d4d.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/calculus/" rel="tag"># 微积分</a> <a href="/tags/linear-albegra/" rel="tag"># 线性代数</a> <a href="/tags/nerual-network/" rel="tag"># 神经网络</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/cb820bb8.html" rel="next" title="神经网络推导-单个数据"><i class="fa fa-chevron-left"></i> 神经网络推导-单个数据</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/posts/a9bec5e9.html" rel="prev" title="导数、微分和梯度">导数、微分和梯度<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#TestNet网络"><span class="nav-number">1.</span> <span class="nav-text">TestNet网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#网络符号定义"><span class="nav-number">1.1.</span> <span class="nav-text">网络符号定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#网络结构"><span class="nav-number">1.2.</span> <span class="nav-text">网络结构</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#前向传播"><span class="nav-number">2.</span> <span class="nav-text">前向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#反向传播"><span class="nav-number">3.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#小结"><span class="nav-number">4.</span> <span class="nav-text">小结</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zhujian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">zhujian</p><div class="site-description" itemprop="description">one bite at a time</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">169</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">129</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/./atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zjZSTU" title="Github &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;zjZSTU" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/u012005313" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;u012005313" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i> CSDN</a></span><span class="links-of-author-item"><a href="/mailto:zjzstu@gmail.com" title="Gmail &amp;rarr; mailto:zjzstu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> Gmail</a></span><span class="links-of-author-item"><a href="/mailto:505169307@gmail.com" title="QQmail &amp;rarr; mailto:505169307@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> QQmail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://cloud.tencent.com/" title="https:&#x2F;&#x2F;cloud.tencent.com" rel="noopener" target="_blank">腾讯云</a></li><li class="links-of-blogroll-item"> <a href="https://www.aliyun.com/" title="https:&#x2F;&#x2F;www.aliyun.com" rel="noopener" target="_blank">阿里云</a></li><li class="links-of-blogroll-item"> <a href="https://developer.android.com/" title="https:&#x2F;&#x2F;developer.android.com&#x2F;" rel="noopener" target="_blank">Android Dev</a></li><li class="links-of-blogroll-item"> <a href="https://jenkins.io/" title="https:&#x2F;&#x2F;jenkins.io&#x2F;" rel="noopener" target="_blank">Jenkins</a></li><li class="links-of-blogroll-item"> <a href="http://cs231n.github.io/" title="http:&#x2F;&#x2F;cs231n.github.io&#x2F;" rel="noopener" target="_blank">cs231n</a></li><li class="links-of-blogroll-item"> <a href="https://pytorch.org/docs/stable/index.html" title="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;index.html" rel="noopener" target="_blank">pytorch</a></li><li class="links-of-blogroll-item"> <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" title="https:&#x2F;&#x2F;docs.docker.com&#x2F;install&#x2F;linux&#x2F;docker-ce&#x2F;ubuntu&#x2F;" rel="noopener" target="_blank">docker</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">zhujian</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">948k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">26:20</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-bar-chart-o"></i></span> <a href="https://tongji.baidu.com/web/27249108/overview/index?siteId=13647183" rel="noopener" target="_blank">百度统计</a></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div><div class="beian"> <a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备 19026415号</a> <span class="post-meta-divider">|</span> <img src="/images/beian_icon.png" style="display:inline-block;text-decoration:none;height:13px"> <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001748" rel="noopener" target="_blank">浙公网安备 33011802001748号</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span> <span class="site-uv" title="总访客量">访客数：<span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="site-pv" title="总访问量">阅读量：<span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="17,63,61" opacity="1" zindex="-1" count="199" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '8f22595c6dd29c94467d',
      clientSecret: 'e66bd90ebb9aa66c562c753dec6c90ceecbd51f2',
      repo: 'guestbook',
      owner: 'zjZSTU',
      admin: ['zjZSTU'],
      id: '40cfad9ba1b380f9578cdb8e73850b92',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script></body></html>