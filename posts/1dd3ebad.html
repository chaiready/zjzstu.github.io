<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/./atom.xml" title="做一个幸福的人" type="application/atom+xml"><meta name="google-site-verification" content="Qr_3yqLtyErDFKHW7mE8PJz4qDUX-bf_fMLpSRckQe4"><meta name="baidu-site-verification" content="zOwIvKMV7f"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"搜索文章",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet"><meta name="description" content="为了理清如何进行神经网络的前向传播和反向传播的推导，找了很多资料，前向传播比较简单，重点在于如何进行反向传播的梯度计算"><meta name="keywords" content="python,神经网络"><meta property="og:type" content="article"><meta property="og:title" content="神经网络推导-矩阵计算"><meta property="og:url" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;posts&#x2F;1dd3ebad.html"><meta property="og:site_name" content="做一个幸福的人"><meta property="og:description" content="为了理清如何进行神经网络的前向传播和反向传播的推导，找了很多资料，前向传播比较简单，重点在于如何进行反向传播的梯度计算"><meta property="og:locale" content="zh-CN"><meta property="og:updated_time" content="2020-02-15T05:36:35.875Z"><meta name="twitter:card" content="summary"><link rel="canonical" href="https://www.zhujian.tech/posts/1dd3ebad.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>神经网络推导-矩阵计算 | 做一个幸福的人</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e677aac1ac69b8826b9cfecb4e72e107";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">做一个幸福的人</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">面朝大海，春暖花开</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">129</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">48</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">169</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header> <a href="https://github.com/zjZSTU" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zhujian.tech/posts/1dd3ebad.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="zhujian"><meta itemprop="description" content="one bite at a time"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="做一个幸福的人"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 神经网络推导-矩阵计算</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-05-15 14:21:36" itemprop="dateCreated datePublished" datetime="2019-05-15T14:21:36+00:00">2019-05-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-15 05:36:35" itemprop="dateModified" datetime="2020-02-15T05:36:35+00:00">2020-02-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/" itemprop="url" rel="index"><span itemprop="name">算法</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">编程</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/deep-learning/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/data-learning/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>13k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>22 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>为了理清如何进行神经网络的前向传播和反向传播的推导，找了很多资料，前向传播比较简单，重点在于如何进行反向传播的梯度计算</p><a id="more"></a><p><code>cs231n</code>课程推荐的计算方式是先进行单个元素求导，再逐步泛化到批量数据求梯度，参考</p><p><a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=2ahUKEwiAvpOU85ziAhXFneAKHfsgDzwQFjAAegQIABAC&amp;url=http%3A%2F%2Fcs231n.stanford.edu%2Fhandouts%2Fderivatives.pdf&amp;usg=AOvVaw2olyLQPXL2R8J3UMpk8Zeo" target="_blank" rel="noopener">Derivatives, Backpropagation, and Vectorization - CS231n</a></p><p><a href="cs231n.stanford.edu/vecDerivs.pdf">Vector, Matrix, and Tensor Derivatives - CS231n</a></p><p><a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=2&amp;ved=2ahUKEwiAvpOU85ziAhXFneAKHfsgDzwQFjABegQIBRAC&amp;url=http%3A%2F%2Fcs231n.stanford.edu%2Fslides%2F2017%2Fcs231n_2017_lecture4.pdf&amp;usg=AOvVaw1o70kb0znbYAjhWXntxnlS" target="_blank" rel="noopener">Backpropagation and Neural Networks - CS231n</a></p><p>自己也根据参考资料进行2层神经网络逐元素的推导</p><p><a href="https://www.zhujian.tech/posts/cb820bb8.html#more">神经网络推导-单个数据</a></p><p><a href="https://www.zhujian.tech/posts/66015d4d.html#more">神经网络推导-批量数据</a></p><p>最好的方式当然是进行矩阵求导，在网上看了很多博客，比较好的有</p><p><a href="https://zhuanlan.zhihu.com/p/24709748" target="_blank" rel="noopener">矩阵求导术（上）</a></p><p><a href="https://zhuanlan.zhihu.com/p/32368246" target="_blank" rel="noopener">[矩阵求导]神经网络反向传播梯度计算数学原理</a></p><p>神经网络矩阵计算最重要的内容是进行实值标量矩阵的一阶微分以及$Jacobian$矩阵的辨识，参考《矩阵分析与应用》，有以下先导知识</p><p><a href="https://www.zhujian.tech/posts/a9bec5e9.html#more">导数、微分和梯度</a></p><p><a href="https://www.zhujian.tech/posts/d1deacd1.html#more">矩阵基础</a></p><p><a href="https://www.zhujian.tech/posts/29422005.html#more">Jacobian矩阵和梯度矩阵</a></p><p><a href="https://www.zhujian.tech/posts/b9ab243b.html#more">实值标量函数一阶微分和Jacobian矩阵辨识</a></p><p>使用矩阵微分能够很便捷的实现神经网络反向求导，关键部分是辨识$Jacobian$矩阵，再转换成梯度矩阵</p><h2 id="推导一"><a href="#推导一" class="headerlink" title="推导一"></a>推导一</h2><p>文章<a href="https://zhuanlan.zhihu.com/p/32368246" target="_blank" rel="noopener">[矩阵求导]神经网络反向传播梯度计算数学原理</a>给出了一个很好的推导方式，首先给出实现代码，然后使用矩阵计算逐步解释代码</p><p><code>PyTorch</code>教程<a href="https://pytorch.org/tutorials/beginner/pytorch_with_examples.html" target="_blank" rel="noopener">Learning PyTorch with Examples</a>给出了一个<code>2</code>层神经网络的<code>numpy</code>实现</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># N is batch size; D_in is input dimension;</span><br><span class="line"># H is hidden dimension; D_out is output dimension.</span><br><span class="line">N, D_in, H, D_out = 64, 1000, 100, 10</span><br><span class="line"></span><br><span class="line"># Create random input and output data</span><br><span class="line">x = np.random.randn(N, D_in)</span><br><span class="line">y = np.random.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"># Randomly initialize weights</span><br><span class="line">w1 = np.random.randn(D_in, H)</span><br><span class="line">w2 = np.random.randn(H, D_out)</span><br><span class="line"></span><br><span class="line">learning_rate = 1e-6</span><br><span class="line">for t in range(500):</span><br><span class="line">    # Forward pass: compute predicted y</span><br><span class="line">    h = x.dot(w1)</span><br><span class="line">    h_relu = np.maximum(h, 0)</span><br><span class="line">    y_pred = h_relu.dot(w2)</span><br><span class="line"></span><br><span class="line">    # Compute and print loss</span><br><span class="line">    loss = np.square(y_pred - y).sum()</span><br><span class="line">    print(t, loss)</span><br><span class="line"></span><br><span class="line">    # Backprop to compute gradients of w1 and w2 with respect to loss</span><br><span class="line">    grad_y_pred = 2.0 * (y_pred - y)</span><br><span class="line">    grad_w2 = h_relu.T.dot(grad_y_pred)</span><br><span class="line">    grad_h_relu = grad_y_pred.dot(w2.T)</span><br><span class="line">    grad_h = grad_h_relu.copy()</span><br><span class="line">    grad_h[h &lt; 0] = 0</span><br><span class="line">    grad_w1 = x.T.dot(grad_h)</span><br><span class="line"></span><br><span class="line">    # Update weights</span><br><span class="line">    w1 -= learning_rate * grad_w1</span><br><span class="line">    w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure><p>第一步：定义网络参数</p><ul><li>批量数据大小$N=64$</li><li>输入层神经元个数$D_{in}=1000$</li><li>隐藏层神经元个数$H=100$</li><li>输出层神经元个数$D_{out}=10$</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># N is batch size; D_in is input dimension;</span><br><span class="line"># H is hidden dimension; D_out is output dimension.</span><br><span class="line">N, D_in, H, D_out = 64, 1000, 100, 10</span><br></pre></td></tr></table></figure><p>第二步：初始化数据、权重（<em>该网络没有偏置向量</em>）以及学习率</p><ul><li>输入数据$x\in R^{N\times D_{in}}$</li><li>输出数据$y\in R^{N\times D_{out}}$</li><li>隐藏层权重矩阵$w1\in R^{D_{in}\times H}$</li><li>输出层权重矩阵$w2\in R^{H\times D_{out}}$</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># Create random input and output data</span><br><span class="line">x = np.random.randn(N, D_in)</span><br><span class="line">y = np.random.randn(N, D_out)</span><br><span class="line"></span><br><span class="line"># Randomly initialize weights</span><br><span class="line">w1 = np.random.randn(D_in, H)</span><br><span class="line">w2 = np.random.randn(H, D_out)</span><br><span class="line"></span><br><span class="line">learning_rate = 1e-6</span><br></pre></td></tr></table></figure><p>第三步：迭代计算，输入批量数据到神经网络，进行前向传播</p><script type="math/tex;mode=display">
h=x\cdot w1\\
h_{relu}=max(0, h)\\
y_{pred}=h_{relu}\cdot w2</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Forward pass: compute predicted y</span><br><span class="line">h = x.dot(w1)</span><br><span class="line">h_relu = np.maximum(h, 0)</span><br><span class="line">y_pred = h_relu.dot(w2)</span><br></pre></td></tr></table></figure><p>第四步：迭代计算，计算损失函数（<em>误差平方和 - L1范数的平方</em>）</p><script type="math/tex;mode=display">
loss=\begin{Vmatrix}
y_{pred}-y
\end{Vmatrix}^{2}</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Compute and print loss</span><br><span class="line">loss = np.square(y_pred - y).sum()</span><br><span class="line">print(t, loss)</span><br></pre></td></tr></table></figure><p><strong>第五步：迭代计算，反向传播，计算输出层输入向量梯度</strong></p><p>设$y_{pred}-y=X$，$X$大小为$N\times D_{out}$，则</p><script type="math/tex;mode=display">
loss=\begin{Vmatrix}
X
\end{Vmatrix}^{2}=
(vec(X))^{T}\cdot vec(X)</script><p>对损失函数$loss(y_{pred})$求输出层输入向量的微分</p><script type="math/tex;mode=display">
dloss=d(tr(loss))=tr(dloss)=tr(d((vec(X))^{T}\cdot vec(X)
))\\
=tr(d(vec(X)^{T})\cdot vec(X)+vec(X)^{T}\cdot dvec(X))\\
=tr(d(vec(X)^{T})\cdot vec(X))+tr(vec(X)^{T}\cdot dvec(X))\\
=tr((dvec(X))^{T}\cdot vec(X))+tr(vec(X)^{T}\cdot dvec(X))\\
=tr((vec(X))^{T}\cdot dvec(X))+tr(vec(X)^{T}\cdot dvec(X))\\
=tr(2(vec(X))^{T}\cdot dvec(X))\\
=tr(2X^{T}\cdot dX)</script><p>所以Jacobian矩阵为$D_{X}f(X)=2X^{T}$，梯度矩阵为$\bigtriangledown_{X}f(X)=2X=2(y_{pred}-y)$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grad_y_pred = 2.0 * (y_pred - y)</span><br></pre></td></tr></table></figure><p><strong>第六步：迭代计算，反向传播，计算输出层权重向量以及隐藏层输出向量梯度</strong></p><script type="math/tex;mode=display">
y_{pred}=h_{relu}\cdot w2
\Rightarrow 
dy_{pred}=dh_{relu}\cdot w2+h_{relu}\cdot dw2</script><script type="math/tex;mode=display">
dloss=tr(2X^{T}\cdot dX)
=tr(2(y_{pred} - y)^{T}\cdot d((y_{pred} - y)))
=tr(2(y_{pred} - y)^{T}\cdot dy_{pred})\\
=tr(2(y_{pred} - y)^{T}\cdot (dh_{relu}\cdot w2+h_{relu}\cdot dw2))\\
=tr(2(y_{pred} - y)^{T}\cdot dh_{relu}\cdot w2)+tr(2(y_{pred} - y)^{T}\cdot h_{relu}\cdot dw2)\\
=tr(w2\cdot 2(y_{pred} - y)^{T}\cdot dh_{relu})+tr(2(y_{pred} - y)^{T}\cdot h_{relu}\cdot dw2)</script><p>输出层权重向量的Jacobian矩阵为$2(y_{pred} - y)^{T}\cdot h_{relu}$，梯度矩阵为$(h_{relu})^{T}\cdot 2(y_{pred} - y)$</p><p>隐藏层输出向量的Jacobian矩阵为$w2\cdot 2(y_{pred} - y)^{T}$，梯度矩阵为$2(y_{pred} - y)\cdot (w2)^{T}$</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grad_w2 = h_relu.T.dot(grad_y_pred)</span><br><span class="line">grad_h_relu = grad_y_pred.dot(w2.T)</span><br></pre></td></tr></table></figure><p><strong>第七步：迭代计算，反向传播，计算隐藏层输入向量梯度</strong></p><script type="math/tex;mode=display">
h_{relu}=max(0, h)
\Rightarrow 
dh_{relu}=\left\{\begin{matrix}
dh & h\geq 0\\ 
0 & h < 0
\end{matrix}\right.
=1(h\geq 0)*dh</script><p>激活函数是逐个元素操作，所以使用Hadamard积</p><script type="math/tex;mode=display">
dloss=tr(w2\cdot 2(y_{pred} - y)^{T}\cdot dh_{relu})\\
=tr(w2\cdot 2(y_{pred} - y)^{T}\cdot 1(h\geq 0)* dh)\\
=tr((2(y_{pred} - y)\cdot (w2)^{T})^{T}\cdot 1(h\geq 0)* dh)\\
=tr((2(y_{pred} - y)\cdot (w2)^{T})^{T}* 1(h\geq 0)^{T}\cdot dh)</script><p>所以Jacobian矩阵为$(2(y_{pred} - y)\cdot (w2)^{T})^{T}* 1(h\geq 0)^{T}$，梯度矩阵为</p><script type="math/tex;mode=display">
\bigtriangledown_{h}f(h)=1(h\geq 0)\cdot 2(y_{pred} - y)\cdot (w2)^{T}</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grad_h = grad_h_relu.copy()</span><br><span class="line">grad_h[h &lt; 0] = 0</span><br></pre></td></tr></table></figure><p><strong>第八步：迭代计算，反向传播，计算隐藏层权重向量梯度</strong></p><script type="math/tex;mode=display">
h=x\cdot w1
\Rightarrow 
dh=x\cdot dw1</script><script type="math/tex;mode=display">
dloss
=tr((2(y_{pred} - y)\cdot (w2)^{T})^{T}* 1(h\geq 0)^{T}\cdot dh)\\
=tr((2(y_{pred} - y)\cdot (w2)^{T})^{T}* 1(h\geq 0)^{T}\cdot x\cdot dw1)</script><p>所以Jacobian矩阵为$(2(y_{pred} - y)\cdot (w2)^{T})^{T}* 1(h\geq 0)^{T}\cdot x$，梯度矩阵为</p><script type="math/tex;mode=display">
\bigtriangledown_{w1}f(w1)=((2(y_{pred} - y)\cdot (w2)^{T})^{T}* 1(h\geq 0)^{T}\cdot x)^{T}\\
=x^{T}\cdot 1(h\geq 0)* 2(y_{pred} - y)\cdot (w2)^{T}</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grad_w1 = x.T.dot(grad_h)</span><br></pre></td></tr></table></figure><p>第九步：迭代计算，反向传播，更新权重矩阵</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Update weights</span><br><span class="line">w1 -= learning_rate * grad_w1</span><br><span class="line">w2 -= learning_rate * grad_w2</span><br></pre></td></tr></table></figure><h2 id="推导二"><a href="#推导二" class="headerlink" title="推导二"></a>推导二</h2><p><code>cs231n</code>课程<a href="http://cs231n.github.io/neural-networks-case-study/#net" target="_blank" rel="noopener">Putting it together: Minimal Neural Network Case Study</a>中实现了一个<code>2</code>层神经网络</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">N = 100 # number of points per class</span><br><span class="line">D = 2 # dimensionality</span><br><span class="line">K = 3 # number of classes</span><br><span class="line">X = np.zeros((N*K,D)) # data matrix (each row = single example)</span><br><span class="line">y = np.zeros(N*K, dtype=&apos;uint8&apos;) # class labels</span><br><span class="line">for j in xrange(K):</span><br><span class="line">  ix = range(N*j,N*(j+1))</span><br><span class="line">  r = np.linspace(0.0,1,N) # radius</span><br><span class="line">  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta</span><br><span class="line">  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]</span><br><span class="line">  y[ix] = j</span><br><span class="line"></span><br><span class="line"># initialize parameters randomly</span><br><span class="line">h = 100 # size of hidden layer</span><br><span class="line">W = 0.01 * np.random.randn(D,h)</span><br><span class="line">b = np.zeros((1,h))</span><br><span class="line">W2 = 0.01 * np.random.randn(h,K)</span><br><span class="line">b2 = np.zeros((1,K))</span><br><span class="line"></span><br><span class="line"># some hyperparameters</span><br><span class="line">step_size = 1e-0</span><br><span class="line">reg = 1e-3 # regularization strength</span><br><span class="line"></span><br><span class="line"># gradient descent loop</span><br><span class="line">num_examples = X.shape[0]</span><br><span class="line">for i in xrange(10000):</span><br><span class="line">  </span><br><span class="line">  # evaluate class scores, [N x K]</span><br><span class="line">  hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation</span><br><span class="line">  scores = np.dot(hidden_layer, W2) + b2</span><br><span class="line">  </span><br><span class="line">  # compute the class probabilities</span><br><span class="line">  exp_scores = np.exp(scores)</span><br><span class="line">  probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]</span><br><span class="line">  </span><br><span class="line">  # compute the loss: average cross-entropy loss and regularization</span><br><span class="line">  correct_logprobs = -np.log(probs[range(num_examples),y])</span><br><span class="line">  data_loss = np.sum(correct_logprobs)/num_examples</span><br><span class="line">  reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)</span><br><span class="line">  loss = data_loss + reg_loss</span><br><span class="line">  if i % 1000 == 0:</span><br><span class="line">    print &quot;iteration %d: loss %f&quot; % (i, loss)</span><br><span class="line">  </span><br><span class="line">  # compute the gradient on scores</span><br><span class="line">  dscores = probs</span><br><span class="line">  dscores[range(num_examples),y] -= 1</span><br><span class="line">  dscores /= num_examples</span><br><span class="line">  </span><br><span class="line">  # backpropate the gradient to the parameters</span><br><span class="line">  # first backprop into parameters W2 and b2</span><br><span class="line">  dW2 = np.dot(hidden_layer.T, dscores)</span><br><span class="line">  db2 = np.sum(dscores, axis=0, keepdims=True)</span><br><span class="line">  # next backprop into hidden layer</span><br><span class="line">  dhidden = np.dot(dscores, W2.T)</span><br><span class="line">  # backprop the ReLU non-linearity</span><br><span class="line">  dhidden[hidden_layer &lt;= 0] = 0</span><br><span class="line">  # finally into W,b</span><br><span class="line">  dW = np.dot(X.T, dhidden)</span><br><span class="line">  db = np.sum(dhidden, axis=0, keepdims=True)</span><br><span class="line">  </span><br><span class="line">  # add regularization gradient contribution</span><br><span class="line">  dW2 += reg * W2</span><br><span class="line">  dW += reg * W</span><br><span class="line">  </span><br><span class="line">  # perform a parameter update</span><br><span class="line">  W += -step_size * dW</span><br><span class="line">  b += -step_size * db</span><br><span class="line">  W2 += -step_size * dW2</span><br><span class="line">  b2 += -step_size * db2</span><br></pre></td></tr></table></figure><p>第一步：设置批量输入数据和输出数据</p><ul><li>批量数据大小$N=100$</li><li>数据维数$D=2$</li><li>类别数$K=3$</li><li>输入数据$X\in R^{N\times D}$</li><li>输出数据$y\in R^{N\times K}$</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">N = 100 # number of points per class</span><br><span class="line">D = 2 # dimensionality</span><br><span class="line">K = 3 # number of classes</span><br><span class="line">X = np.zeros((N*K,D)) # data matrix (each row = single example)</span><br><span class="line">y = np.zeros(N*K, dtype=&apos;uint8&apos;) # class labels</span><br><span class="line">for j in xrange(K):</span><br><span class="line">  ix = range(N*j,N*(j+1))</span><br><span class="line">  r = np.linspace(0.0,1,N) # radius</span><br><span class="line">  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta</span><br><span class="line">  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]</span><br><span class="line">  y[ix] = j</span><br></pre></td></tr></table></figure><p>第二步：初始化权重参数</p><ul><li>隐藏层神经元个数$h=100$</li><li>隐藏层权重矩阵$W\in R^{D\times h}$</li><li>隐藏层偏置向量$b\in R^{1\times h}$</li><li>输出层权重矩阵$W2\in R^{h\times K}$</li><li>输出层偏置向量$b2\in R^{1\times K}$</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># initialize parameters randomly</span><br><span class="line">h = 100 # size of hidden layer</span><br><span class="line">W = 0.01 * np.random.randn(D,h)</span><br><span class="line">b = np.zeros((1,h))</span><br><span class="line">W2 = 0.01 * np.random.randn(h,K)</span><br><span class="line">b2 = np.zeros((1,K))</span><br></pre></td></tr></table></figure><p>第三步：设置学习率和正则化强度</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># some hyperparameters</span><br><span class="line">step_size = 1e-0</span><br><span class="line">reg = 1e-3 # regularization strength</span><br></pre></td></tr></table></figure><p>第四步：迭代计算，输入批量数据到神经网络，进行前向传播</p><script type="math/tex;mode=display">
hiddenLayer = max(X\cdot W+b, 0)\\
scores = hiddenLayer\cdot W2+b2</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># evaluate class scores, [N x K]</span><br><span class="line">hidden_layer = np.maximum(0, np.dot(X, W) + b) # note, ReLU activation</span><br><span class="line">scores = np.dot(hidden_layer, W2) + b2</span><br></pre></td></tr></table></figure><p>第四步：迭代计算，计算损失值</p><script type="math/tex;mode=display">
expScores = exp(scores)\\
probs = \frac {expScores}{expScores\cdot 1}\\
correctLogProbs = -\ln probs_{y}\in R^{N\times 1}\\
dataLoss=\frac {1}{N} 1^{T}\cdot correctLogProbs\\
regLoss=0.5\cdot reg\cdot ||W||^{2}+0.5\cdot reg\cdot ||W2||^{2}\\
loss = dataLoss+regLoss</script><p><em>$1$表示求和向量：$[1,1,…]^{T}$</em></p><p><em>$probs_{y}$表示每行正确类别的概率</em></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># compute the class probabilities</span><br><span class="line">exp_scores = np.exp(scores)</span><br><span class="line">probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]</span><br><span class="line"></span><br><span class="line"># compute the loss: average cross-entropy loss and regularization</span><br><span class="line">correct_logprobs = -np.log(probs[range(num_examples),y])</span><br><span class="line">data_loss = np.sum(correct_logprobs)/num_examples</span><br><span class="line">reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)</span><br><span class="line">loss = data_loss + reg_loss</span><br><span class="line">if i % 1000 == 0:</span><br><span class="line">    print &quot;iteration %d: loss %f&quot; % (i, loss)</span><br></pre></td></tr></table></figure><p><strong>第五步：迭代计算，反向传播，计算输出层输入向量梯度</strong></p><script type="math/tex;mode=display">
scores_{y}=scores*Y\cdot 1\\
expscores_{y}=exp(scores*Y\cdot 1)\ \ \ \ 
expscores=exp(scores)\ \ \ \ 
expscores_{sum}=exp(scores)\cdot 1\\
probs_{y}=\frac {expscores_{y}}{expscores_{sum}}\ \ \ \ 
probs=\frac {expscores}{expscores_{sum}}</script><script type="math/tex;mode=display">
dataloss=-\frac {1}{N} 1^{T}\cdot \ln (probs_{y})
=-\frac {1}{N} 1^{T}\cdot \ln \frac {expscores_{y}}{expscores_{sum}}\\
=-\frac {1}{N} 1^{T}\cdot (\ln expscores_{y} -\ln expscores_{sum})\\
=-\frac {1}{N} 1^{T}\cdot (scores*Y\cdot 1 -\ln expscores_{sum})</script><script type="math/tex;mode=display">
d(dataloss)=tr(d(-\frac {1}{N} (1^{T}\cdot scores*Y\cdot 1 -1^{T}\cdot \ln expscores_{sum})))\\
=tr(d(-\frac {1}{N} (1^{T}\cdot scores*Y\cdot 1))) - tr(d(-\frac {1}{N} (1^{T}\cdot \ln expscores_{sum})))</script><script type="math/tex;mode=display">
tr(d(-\frac {1}{N} (1^{T}\cdot scores*Y\cdot 1)))=
tr(-\frac {1}{N} (1^{T}\cdot dscores*Y\cdot 1))\\
=tr(-\frac {1}{N} (dscores^{T}\cdot Y))
=tr(-\frac {1}{N} Y^{T}\cdot dscores)</script><script type="math/tex;mode=display">
tr(d(-\frac {1}{N} (1^{T}\cdot \ln expscores_{sum})))
=tr(-\frac {1}{N} (1^{T}\cdot expscores_{sum}^{-1}\cdot dexpscores_{sum}))\\
=tr(-\frac {1}{N} \frac {(1^{T}\cdot dexpscores_{sum})}{expscores_{sum}})
=tr(-\frac {1}{N} \frac {(1^{T}\cdot exp(scores)* dscores\cdot 1)}{expscores_{sum}})\\
=tr(-\frac {1}{N} \frac {exp(scores)^{T}\cdot dscores}{expscores_{sum}})
=tr(-\frac {1}{N} (\frac {exp(scores)}{expscores_{sum}})^{T}\cdot dscores)
=tr(-\frac {1}{N} probs^{T}\cdot dscores)</script><script type="math/tex;mode=display">
\Rightarrow d(dataloss)=
tr(-\frac {1}{N} Y^{T}\cdot dscores)-tr(-\frac {1}{N} probs^{T}\cdot dscores)\\
=tr(\frac {1}{N} (probs^{T} - Y^{T})\cdot dscores)</script><p>所以$Jacobian$矩阵为$D_{scores}f(scores)=probs^{T} - Y^{T}$，梯度矩阵为$\bigtriangledown_{scores}f(scores)=probs - Y$</p><ul><li>$Y$大小为$N\times K$，每行仅正确类别位置为1，其余为0</li><li>$1$是求和向量，$[1,1,…]^{T}$</li></ul><p>计算<code>softmax</code>分类的交叉熵损失关于输出层输入向量梯度，这一部分想了好久，主要问题是关于矩阵除法和逐元素除法（标量除法）的分别，感觉还是<strong>先对单个数据进行求梯度再泛化比较方便</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># compute the gradient on scores</span><br><span class="line">dscores = probs</span><br><span class="line">dscores[range(num_examples),y] -= 1</span><br><span class="line">dscores /= num_examples</span><br></pre></td></tr></table></figure><p><strong>第六步：迭代计算，反向传播，计算输出层权重矩阵、偏置向量以及隐藏层输出向量梯度</strong></p><script type="math/tex;mode=display">
scores = hiddenLayer\cdot W2+b2\\
dscores = dhiddenLayer\cdot W2 + hiddenLayer\cdot dW2 + db2</script><script type="math/tex;mode=display">
 d(dataloss)
=tr(\frac {1}{N} (probs^{T} - Y^{T})\cdot dscores)\\
=tr(\frac {1}{N} (probs^{T} - Y^{T})\cdot (dhiddenLayer\cdot W2 + hiddenLayer\cdot dW2 + db2))\\
=tr(\frac {1}{N} (probs^{T} - Y^{T})\cdot dhiddenLayer\cdot W2)\\
+tr(\frac {1}{N} (probs^{T} - Y^{T})\cdot hiddenLayer\cdot dW2)+
tr(\frac {1}{N} (probs^{T} - Y^{T})\cdot db2)</script><p>求输出层权重矩阵梯度</p><script type="math/tex;mode=display">
d(dataloss)=tr(\frac {1}{N} (probs^{T} - Y^{T})\cdot hiddenLayer\cdot dW2)</script><script type="math/tex;mode=display">
D_{W2}f(W2)=\frac {1}{N} (probs^{T} - Y^{T})\cdot hiddenLayer\\
\bigtriangledown_{W2}f(W2)=\frac {1}{N} hiddenLayer^{T}\cdot (probs - Y)</script><p>求输出层偏置向量梯度</p><script type="math/tex;mode=display">
d(dataloss)=tr(\frac {1}{N} \sum_{i=1}^{N}(probs_{i}^{T} - Y_{i}^{T})\cdot db2)</script><script type="math/tex;mode=display">
D_{b2}f(b2)=\frac {1}{N} \sum_{i=1}^{N}(probs_{i}^{T} - Y_{i}^{T})\\
\bigtriangledown_{b2}f(b2)=\frac {1}{N} \sum_{i=1}^{N}(probs_{i} - Y_{i})</script><p><strong>对偏置向量还需要注意维数，求和批量数据的偏置向量梯度</strong></p><p>求隐藏层输出向量梯度</p><script type="math/tex;mode=display">
d(dataloss)=tr(\frac {1}{N} (probs^{T} - Y^{T})\cdot dhiddenLayer\cdot W2)
=tr(\frac {1}{N} W2\cdot (probs^{T} - Y^{T})\cdot dhiddenLayer)</script><script type="math/tex;mode=display">
D_{hiddenLayer}f(hiddenLayer)=\frac {1}{N} W2\cdot (probs^{T} - Y^{T})\\
\bigtriangledown_{hiddenLayer}f(hiddenLayer)=\frac {1}{N} (probs - Y)\cdot (W2)^{T}</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># backpropate the gradient to the parameters</span><br><span class="line"># first backprop into parameters W2 and b2</span><br><span class="line">dW2 = np.dot(hidden_layer.T, dscores)</span><br><span class="line">db2 = np.sum(dscores, axis=0, keepdims=True)</span><br><span class="line"># next backprop into hidden layer</span><br><span class="line">dhidden = np.dot(dscores, W2.T)</span><br></pre></td></tr></table></figure><p><strong>第七步：迭代计算，反向传播，计算隐藏层输入向量梯度</strong></p><script type="math/tex;mode=display">
hiddenLayer_{in}=X\cdot W+b\\
hiddenLayer = max(0, hiddenLayer_{in})\\
dhiddenLayer = 1(hiddenLayer_{in}\geq 0)* dhiddenLayer_{in}</script><script type="math/tex;mode=display">
d(dataloss)=tr(\frac {1}{N} W2\cdot (probs^{T} - Y^{T})\cdot dhiddenLayer)\\
=tr(\frac {1}{N} W2\cdot (probs^{T} - Y^{T})\cdot 1(hiddenLayer_{in}\geq 0)* dhiddenLayer_{in})\\
=tr(\frac {1}{N} (W2\cdot (probs^{T} - Y^{T}))^{T} * 1(hiddenLayer_{in}\geq 0)^{T}\cdot dhiddenLayer_{in})</script><script type="math/tex;mode=display">
D_{hiddenLayer_{in}}f(hiddenLayer_{in})=\frac {1}{N} (W2\cdot (probs^{T} - Y^{T}))^{T} * 1(hiddenLayer_{in}\geq 0)^{T}\\
\bigtriangledown_{hiddenLayer_{in}}f(hiddenLayer_{in})=\frac {1}{N} ((probs - Y)\cdot (W2)^{T})* 1(hiddenLayer_{in}\geq 0)</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># backprop the ReLU non-linearity</span><br><span class="line">dhidden[hidden_layer &lt;= 0] = 0</span><br></pre></td></tr></table></figure><p><strong>第七步：迭代计算，反向传播，计算隐藏层权重向量和偏置向量梯度</strong></p><script type="math/tex;mode=display">
hiddenLayer_{in}=X\cdot W+b\\
dhiddenLayer_{in}=X\cdot dW + db</script><script type="math/tex;mode=display">
d(dataloss)=tr(\frac {1}{N} (W2\cdot (probs^{T} - Y^{T}))^{T} * 1(hiddenLayer_{in}\geq 0)^{T}\cdot dhiddenLayer_{in})\\
=tr(\frac {1}{N} (W2\cdot (probs^{T} - Y^{T}))^{T} * 1(hiddenLayer_{in}\geq 0)^{T}\cdot (X\cdot dW + db))</script><p>求隐藏层权重向量梯度</p><script type="math/tex;mode=display">
d(dataloss)=tr(\frac {1}{N} (W2\cdot (probs^{T} - Y^{T}))^{T} * 1(hiddenLayer_{in}\geq 0)^{T}\cdot X\cdot dW)</script><script type="math/tex;mode=display">
D_{W}f(W)=\frac {1}{N} (W2\cdot (probs^{T} - Y^{T}))^{T} * 1(hiddenLayer_{in}\geq 0)^{T}\cdot X\\
\bigtriangledown_{W}f(W)=\frac {1}{N} X^{T}\cdot ((probs - Y)\cdot (W2)^{T})* 1(hiddenLayer_{in}\geq 0)</script><p>求隐藏层偏置向量梯度</p><script type="math/tex;mode=display">
d(dataloss)=tr(\frac {1}{N} \sum_{i=1}^{N}(W2\cdot (probs^{T} - Y^{T}))^{T} * 1(hiddenLayer_{in}\geq 0)^{T}\cdot db)</script><script type="math/tex;mode=display">
D_{W}f(W)=\frac {1}{N} \sum_{i=1}^{N}(W2\cdot (probs^{T} - Y^{T}))^{T} * 1(hiddenLayer_{in}\geq 0)^{T}\\
\bigtriangledown_{W}f(W)=\frac {1}{N} \sum_{i=1}^{N}((probs - Y)\cdot (W2)^{T})* 1(hiddenLayer_{in}\geq 0)</script><p><strong>对偏置向量还需要注意维数，求和批量数据的偏置向量梯度</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># finally into W,b</span><br><span class="line">dW = np.dot(X.T, dhidden)</span><br><span class="line">db = np.sum(dhidden, axis=0, keepdims=True)</span><br></pre></td></tr></table></figure><p>第八步：迭代计算，反向传播，计算正则化梯度</p><script type="math/tex;mode=display">
regLoss=0.5\cdot reg\cdot ||W||^{2}+0.5\cdot reg\cdot ||W2||^{2}\\
d(regLoss)=reg\cdot W\cdot dW+reg\cdot W2\cdot dW2</script><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># add regularization gradient contribution</span><br><span class="line">dW2 += reg * W2</span><br><span class="line">dW += reg * W</span><br></pre></td></tr></table></figure><p>第九步：迭代计算，反向传播，更新权重矩阵和偏置向量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># perform a parameter update</span><br><span class="line">W += -step_size * dW</span><br><span class="line">b += -step_size * db</span><br><span class="line">W2 += -step_size * dW2</span><br><span class="line">b2 += -step_size * db2</span><br></pre></td></tr></table></figure><h2 id="相关资料"><a href="#相关资料" class="headerlink" title="相关资料"></a>相关资料</h2><ol><li><a href="https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf" target="_blank" rel="noopener">The Matrix Cookbook - Mathematics</a></li><li><a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener">Matrix calculus</a></li></ol></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.jpg" alt="zhujian 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="zhujian 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zhujian</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zhujian.tech/posts/1dd3ebad.html" title="神经网络推导-矩阵计算">https://www.zhujian.tech/posts/1dd3ebad.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/python/" rel="tag"># python</a> <a href="/tags/nerual-network/" rel="tag"># 神经网络</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/b9ab243b.html" rel="next" title="实值标量函数一阶微分和Jacobian矩阵辨识"><i class="fa fa-chevron-left"></i> 实值标量函数一阶微分和Jacobian矩阵辨识</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/posts/ba2ca878.html" rel="prev" title="神经网络实现-numpy">神经网络实现-numpy<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#推导一"><span class="nav-number">1.</span> <span class="nav-text">推导一</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#推导二"><span class="nav-number">2.</span> <span class="nav-text">推导二</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#相关资料"><span class="nav-number">3.</span> <span class="nav-text">相关资料</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zhujian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">zhujian</p><div class="site-description" itemprop="description">one bite at a time</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">169</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">129</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/./atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zjZSTU" title="Github &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;zjZSTU" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/u012005313" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;u012005313" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i> CSDN</a></span><span class="links-of-author-item"><a href="/mailto:zjzstu@gmail.com" title="Gmail &amp;rarr; mailto:zjzstu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> Gmail</a></span><span class="links-of-author-item"><a href="/mailto:505169307@gmail.com" title="QQmail &amp;rarr; mailto:505169307@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> QQmail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://cloud.tencent.com/" title="https:&#x2F;&#x2F;cloud.tencent.com" rel="noopener" target="_blank">腾讯云</a></li><li class="links-of-blogroll-item"> <a href="https://www.aliyun.com/" title="https:&#x2F;&#x2F;www.aliyun.com" rel="noopener" target="_blank">阿里云</a></li><li class="links-of-blogroll-item"> <a href="https://developer.android.com/" title="https:&#x2F;&#x2F;developer.android.com&#x2F;" rel="noopener" target="_blank">Android Dev</a></li><li class="links-of-blogroll-item"> <a href="https://jenkins.io/" title="https:&#x2F;&#x2F;jenkins.io&#x2F;" rel="noopener" target="_blank">Jenkins</a></li><li class="links-of-blogroll-item"> <a href="http://cs231n.github.io/" title="http:&#x2F;&#x2F;cs231n.github.io&#x2F;" rel="noopener" target="_blank">cs231n</a></li><li class="links-of-blogroll-item"> <a href="https://pytorch.org/docs/stable/index.html" title="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;index.html" rel="noopener" target="_blank">pytorch</a></li><li class="links-of-blogroll-item"> <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" title="https:&#x2F;&#x2F;docs.docker.com&#x2F;install&#x2F;linux&#x2F;docker-ce&#x2F;ubuntu&#x2F;" rel="noopener" target="_blank">docker</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">zhujian</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">948k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">26:20</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-bar-chart-o"></i></span> <a href="https://tongji.baidu.com/web/27249108/overview/index?siteId=13647183" rel="noopener" target="_blank">百度统计</a></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div><div class="beian"> <a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备 19026415号</a> <span class="post-meta-divider">|</span> <img src="/images/beian_icon.png" style="display:inline-block;text-decoration:none;height:13px"> <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001748" rel="noopener" target="_blank">浙公网安备 33011802001748号</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span> <span class="site-uv" title="总访客量">访客数：<span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="site-pv" title="总访问量">阅读量：<span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="17,63,61" opacity="1" zindex="-1" count="199" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '8f22595c6dd29c94467d',
      clientSecret: 'e66bd90ebb9aa66c562c753dec6c90ceecbd51f2',
      repo: 'guestbook',
      owner: 'zjZSTU',
      admin: ['zjZSTU'],
      id: 'a5689d0775ed887df6475285782e5011',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script></body></html>