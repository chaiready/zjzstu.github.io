<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/./atom.xml" title="做一个幸福的人" type="application/atom+xml"><meta name="google-site-verification" content="Qr_3yqLtyErDFKHW7mE8PJz4qDUX-bf_fMLpSRckQe4"><meta name="baidu-site-verification" content="zOwIvKMV7f"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"搜索文章",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet"><meta name="description" content="参考：Softmax回归softmax回归常用于多分类问题，其输出可直接看成对类别的预测概率假设对k类标签（[1, 2, ..., k]）进行分类，那么经过softmax回归计算后，输出一个k维向量，向量中每个值都代表对一个类别的预测概率"><meta name="keywords" content="python,微积分,numpy,matplotlib,sklearn,pandas,softmax"><meta property="og:type" content="article"><meta property="og:title" content="softmax回归"><meta property="og:url" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;posts&#x2F;2626bec3.html"><meta property="og:site_name" content="做一个幸福的人"><meta property="og:description" content="参考：Softmax回归softmax回归常用于多分类问题，其输出可直接看成对类别的预测概率假设对k类标签（[1, 2, ..., k]）进行分类，那么经过softmax回归计算后，输出一个k维向量，向量中每个值都代表对一个类别的预测概率"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;softmax回归&#x2F;iris_petal.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;softmax回归&#x2F;iris_sepal.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;softmax回归&#x2F;numpy_softmax_loss.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;softmax回归&#x2F;numpy_softmax_accuracy.png"><meta property="og:updated_time" content="2020-02-15T05:36:35.871Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;softmax回归&#x2F;iris_petal.png"><link rel="canonical" href="https://www.zhujian.tech/posts/2626bec3.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>softmax回归 | 做一个幸福的人</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e677aac1ac69b8826b9cfecb4e72e107";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">做一个幸福的人</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">面朝大海，春暖花开</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">129</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">48</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">169</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header> <a href="https://github.com/zjZSTU" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zhujian.tech/posts/2626bec3.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="zhujian"><meta itemprop="description" content="one bite at a time"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="做一个幸福的人"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> softmax回归</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-04-23 14:23:20" itemprop="dateCreated datePublished" datetime="2019-04-23T14:23:20+00:00">2019-04-23</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-15 05:36:35" itemprop="dateModified" datetime="2020-02-15T05:36:35+00:00">2020-02-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/" itemprop="url" rel="index"><span itemprop="name">算法</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">编程</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/math/" itemprop="url" rel="index"><span itemprop="name">数学</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/programming-language/" itemprop="url" rel="index"><span itemprop="name">编程语言</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/data-learning/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/codebase/" itemprop="url" rel="index"><span itemprop="name">代码库</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>19k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>32 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>参考：</p><p><a href="http://ufldl.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92" target="_blank" rel="noopener">Softmax回归</a></p><p><code>softmax</code>回归常用于多分类问题，其输出可直接看成对类别的预测概率</p><p>假设对<code>k</code>类标签（<code>[1, 2, ..., k]</code>）进行分类，那么经过<code>softmax</code>回归计算后，输出一个<code>k</code>维向量，向量中每个值都代表对一个类别的预测概率</p><a id="more"></a><p>下面先以单个输入数据为例，进行评分函数、损失函数的计算和求导，然后扩展到多个输入数据同步计算</p><h2 id="对数函数操作"><a href="#对数函数操作" class="headerlink" title="对数函数操作"></a>对数函数操作</h2><p>对数求和</p><script type="math/tex;mode=display">
log_{a}^{x}+log_{a}^{y} = log_{a}^{xy}</script><p>对数求差</p><script type="math/tex;mode=display">
log_{a}^{x}-log_{a}^{y} = log_{a}^{\frac{x}{y}}</script><p>指数乘法</p><script type="math/tex;mode=display">
e^{x}\cdot e^{y} = e^{x+y}</script><h2 id="求导公式"><a href="#求导公式" class="headerlink" title="求导公式"></a>求导公式</h2><p>若函数$u(x),v(x)均可导$，那么</p><script type="math/tex;mode=display">
\left(\frac{u(x)}{v(x)}\right)^{\prime}=\frac{u^{\prime}(x) v(x)-v^{\prime}(x) u(x)}{v^{2}(x)}</script><h2 id="单个输入数据进行softmax回归计算"><a href="#单个输入数据进行softmax回归计算" class="headerlink" title="单个输入数据进行softmax回归计算"></a>单个输入数据进行<code>softmax</code>回归计算</h2><h3 id="评分函数"><a href="#评分函数" class="headerlink" title="评分函数"></a>评分函数</h3><p>假设使用<code>softmax</code>回归分类数据$x$，共$k$个标签，首先进行线性回归操作</p><script type="math/tex;mode=display">
z_{\theta}(x)=\theta^T\cdot x
=\begin{bmatrix}
\theta_{1}^T\\ 
\theta_{2}^T\\ 
...\\ 
\theta_{k}^T
\end{bmatrix}\cdot x
=\begin{bmatrix}
\theta_{1}^T\cdot x\\ 
\theta_{2}^T\cdot x\\ 
...\\ 
\theta_{k}^T\cdot x
\end{bmatrix}</script><p>其中输入数据$x$大小为$(n+1)\times 1$，$\theta$大小为$(n+1)\times k$，$n$表示权重数量，$m$表示训练数据个数，$k$表示类别标签数量</p><p>输出结果$z$大小为$k\times 1$，然后对计算结果进行归一化操作，使得输出值能够表示类别概率，如下所示</p><script type="math/tex;mode=display">
h_{\theta}\left(x\right)=\left[ \begin{array}{c}{p\left(y=1 | x ; \theta\right)} \\ {p\left(y=2 | x ; \theta\right)} \\ {\vdots} \\ {p\left(y=k | x ; \theta\right)}\end{array}\right]
=\frac{1}{\sum_{j=1}^{k} e^{\theta_{j}^{T} x}} \left[ \begin{array}{c}{e^{\theta_{1}^{T} x}} \\ {e^{\theta_{2}^{T} x}} \\ {\vdots} \\ {e^{\theta_{k}^{T} x}}\end{array}\right]</script><p>其中$\theta_{1}、\theta_{2},…,\theta_{k}$的大小为$(n+1)\times 1$，输出结果是一个$k\times 1$大小向量，每列表示$k$类标签的预测概率</p><p>所以对于输入数据$x$而言，其属于标签$j$的概率是</p><script type="math/tex;mode=display">
p\left(y=j | x; \theta\right)=\frac{e^{\theta_{j}^{T} x}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x}}</script><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><p>利用交叉熵损失（<code>cross entropy loss</code>）作为<code>softmax</code>回归的损失函数，用于计算训练数据对应的真正标签的损失值</p><script type="math/tex;mode=display">
J(\theta)
= (-1)\cdot \sum_{j=1}^{k} 1\left\{y=j\right\} \ln p\left(y=j | x; \theta\right)
= (-1)\cdot \sum_{j=1}^{k} 1\left\{y=j\right\} \ln \frac{e^{\theta_{j}^{T} x}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x}}</script><p>其中函数$1\{\cdot\}$是一个示性函数（<code>indicator function</code>），其取值规则为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1&#123;a true statement&#125; = 1, and 1&#123;a false statement&#125; = 0</span><br></pre></td></tr></table></figure><p>也就是示性函数输入为<code>True</code>时，输出为<code>1</code>；否则，输出为<code>0</code></p><p>对权重向量$\theta_{s}$进行求导：</p><script type="math/tex;mode=display">
\frac{\varphi J(\theta)}{\varphi \theta_{s}}
=(-1)\cdot \frac{\varphi }{\varphi \theta_{s}}
\left[ \\
\sum_{j=1,j\neq s}^{k} 1\left\{y=j \right\} \ln p\left(y=j | x; \theta\right)
+1\left\{y=s \right\} \ln p\left(y=s | x; \theta\right) \\
\right]</script><script type="math/tex;mode=display">
=(-1)\cdot \sum_{j=1,j\neq s}^{k} 1\left\{y=j \right\} \frac{1}{p\left(y=j | x; \theta\right)}\frac{\varphi p\left(y=j | x; \theta\right)}{\varphi \theta_{s}}
+(-1)\cdot 1\left\{y=s \right\} \frac{1}{p\left(y=s | x; \theta\right)}\frac{\varphi p\left(y=s | x; \theta\right)}{\varphi \theta_{s}}</script><p>分为两种情况</p><ul><li>当计算结果正好由$\theta_{s}$计算得到，此时线性运算为$z=\theta_{s}^{T} x$，计算结果为$p\left(y=s | x; \theta\right)=\frac{e^{\theta_{s}^{T} x}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x}}$，求导如下</li></ul><script type="math/tex;mode=display">
\frac{\varphi p\left(y=s | x; \theta\right)}{\varphi \theta_{s}}
=\frac{u^{\prime}(x) v(x)-v^{\prime}(x) u(x)}{v^{2}(x)}</script><p>其中</p><script type="math/tex;mode=display">
u(x) = e^{\theta_{s}^{T} x}, v(x)=\sum_{l=1}^{k} e^{\theta_{l}^{T} x}</script><p>所以</p><script type="math/tex;mode=display">
\frac{\varphi u(x)}{\varphi \theta_s} = e^{\theta_{s}^{T} x}\cdot x=u(x)\cdot x,
\frac{\varphi v(x)}{\varphi \theta_s} = e^{\theta_{s}^{T} x}\cdot x=u(x)\cdot x \\
\frac{\varphi p\left(y=s | x; \theta\right)}{\varphi \theta_{s}} = p\left(y=s | x; \theta\right)\cdot x-p\left(y=s | x; \theta\right)^2\cdot x</script><ul><li>当计算结果不是由$\theta_{s}$计算得到，此时线性运算为$z=\theta_{j}^{T} x, j\neq s$，计算结果为$p\left(y=j | x; \theta\right)=\frac{e^{\theta_{j}^{T} x}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x}}$</li></ul><script type="math/tex;mode=display">
\frac{\varphi p\left(y=j | x; \theta\right)}{\varphi \theta_{s}}
=\frac{u^{\prime}(x) v(x)-v^{\prime}(x) u(x)}{v^{2}(x)}</script><p>其中</p><script type="math/tex;mode=display">
u(x) = e^{\theta_{j}^{T} x}, v(x)=\sum_{l=1}^{k} e^{\theta_{l}^{T} x}</script><p>所以</p><script type="math/tex;mode=display">
\frac{\varphi u(x)}{\varphi \theta_s} = e^{\theta_{j}^{T} x}\cdot x=0,
\frac{\varphi v(x)}{\varphi \theta_s} = e^{\theta_{s}^{T} x}\cdot x \\
\frac{\varphi p\left(y=s | x; \theta\right)}{\varphi \theta_{s}} = -p\left(y=s | x; \theta\right)p\left(y=j | x; \theta\right)\cdot x</script><p>综合上述两种情况可知，求导结果为</p><script type="math/tex;mode=display">
\frac{\varphi J(\theta)}{\varphi \theta_{s}}
=(-1)\cdot \sum_{j=1,j\neq s}^{k} 1\left\{y=j \right\} \frac{1}{p\left(y=j | x; \theta\right)}\frac{\varphi p\left(y=j | x; \theta\right)}{\varphi \theta_{s}}
+(-1)\cdot 1\left\{y=s \right\} \frac{1}{p\left(y=s | x; \theta\right)}\frac{\varphi p\left(y=s | x; \theta\right)}{\varphi \theta_{s}} \\
=(-1)\cdot \sum_{j=1,j\neq s}^{k} 1\left\{y=j \right\} \frac{1}{p\left(y=j | x; \theta\right)})\cdot (-1)\cdot p\left(y=s | x; \theta\right)p\left(y=j | x; \theta\right)\cdot x + (-1)\cdot 1\left\{y=s \right\} \frac{1}{p\left(y=s | x; \theta\right)}\left[p\left(y=s | x; \theta\right)\cdot x-p\left(y=s | x; \theta\right)^2\cdot x\right] \\
=(-1)\cdot \sum_{j=1,j\neq s}^{k} 1\left\{y=j \right\}\cdot (-1)\cdot p\left(y=s | x; \theta\right)\cdot x + (-1)\cdot 1\left\{y=s \right\} \left[x-p\left(y=s | x; \theta\right)\cdot x\right] \\
=(-1)\cdot 1\left\{y=s \right\} x - (-1)\cdot \sum_{j=1}^{k} 1\left\{y=j \right\} p\left(y=s | x; \theta\right)\cdot x</script><p>因为$\sum_{j=1}^{k} 1\left\{y=j \right\}=1$，所以最终结果为</p><script type="math/tex;mode=display">
\frac{\varphi J(\theta)}{\varphi \theta_{s}}
=(-1)\cdot \left[ 1\left\{y=s \right\} - p\left(y=s | x; \theta\right) \right]\cdot x</script><h2 id="批量数据进行softmax回归计算"><a href="#批量数据进行softmax回归计算" class="headerlink" title="批量数据进行softmax回归计算"></a>批量数据进行softmax回归计算</h2><p>上面实现了单个数据进行类别概率和损失函数的计算以及求导，进一步推导到批量数据进行操作</p><h3 id="评分函数-1"><a href="#评分函数-1" class="headerlink" title="评分函数"></a>评分函数</h3><p>假设使用softmax回归分类数据$x$，共$k$个标签，首先进行线性回归操作</p><script type="math/tex;mode=display">
z_{\theta}(x_{i})=\theta^T\cdot x_{i}
=\begin{bmatrix}
\theta_{1}^T\\ 
\theta_{2}^T\\ 
...\\ 
\theta_{k}^T
\end{bmatrix}\cdot x_{i}
=\begin{bmatrix}
\theta_{1}^T\cdot x_{i}\\ 
\theta_{2}^T\cdot x_{i}\\ 
...\\ 
\theta_{k}^T\cdot x_{i}
\end{bmatrix}</script><p>其中输入数据$x$大小为$(n+1)\times m$，$\theta$大小为$(n+1)\times k$，$n$表示权重数量，$m$表示训练数据个数，$k$表示类别标签数量</p><p>输出结果$z$大小为$k\times m$，然后对计算结果进行归一化操作，使得输出值能够表示类别概率，如下所示</p><script type="math/tex;mode=display">
h_{\theta}\left(x_{i}\right)=\left[ \begin{array}{c}{p\left(y=1 | x_{i} ; \theta\right)} \\ 
{p\left(y=2 | x_{i} ; \theta\right)} \\ 
{\vdots} \\
{p\left(y=k | x_{i} ; \theta\right)}\end{array}\right]
=\frac{1}{\sum_{j=1}^{k} e^{\theta_{j}^{T} x}} \left[ \begin{array}{c}{e^{\theta_{1}^{T} x_{i}}} \\ 
{e^{\theta_{2}^{T} x_{i}}} \\
 {\vdots} \\ 
 {e^{\theta_{k}^{T} x_{i}}}\end{array}\right]</script><p>其中$\theta_{1}、\theta_{2},…,\theta_{k}$的大小为$(n+1)\times 1$，输出结果是一个$k\times m$大小向量，每列表示$k$类标签的预测概率</p><p>所以对于输入数据$x_{i}$而言，其属于标签$j$的概率是</p><script type="math/tex;mode=display">
p\left(y_{i}=j | x_{i}; \theta\right)=\frac{e^{\theta_{j}^{T} x_{i}}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_{i}}}</script><h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><p>利用交叉熵损失（cross entropy loss）作为softmax回归的代价函数，用于计算训练数据对应的真正标签的损失值</p><script type="math/tex;mode=display">
J(\theta)
= (-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m} \sum_{j=1}^{k} 1\left\{y_{i}=j\right\} \ln p\left(y_{i}=j | x_{i}; \theta\right)
= (-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m} \sum_{j=1}^{k} 1\left\{y_{i}=j\right\} \ln \frac{e^{\theta_{j}^{T} x_{i}}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_{i}}}</script><p>其中函数$1\{\cdot\}$是一个示性函数（indicator function），其取值规则为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">1&#123;a true statement&#125; = 1, and 1&#123;a false statement&#125; = 0</span><br></pre></td></tr></table></figure><p>也就是示性函数输入为True时，输出为1；否则，输出为0</p><p>对权重向量$\theta_{s}$进行求导：</p><script type="math/tex;mode=display">
\frac{\varphi J(\theta)}{\varphi \theta_{s}}
=(-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m}\cdot \frac{\varphi }{\varphi \theta_{s}}
\left[ \sum_{j=1,j\neq s}^{k} 1\left\{y_{i}=j \right\} \ln p\left(y_{i}=j | x_{i}; \theta\right)+1\left\{y_{i}=s \right\} \ln p\left(y_{i}=s | x_{i}; \theta\right) \right]</script><script type="math/tex;mode=display">
=(-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m}\cdot \sum_{j=1,j\neq s}^{k} 1\left\{y_{i}=j \right\} \frac{1}{p\left(y_{i}=j | x_{i}; \theta\right)}\frac{\varphi p\left(y_{i}=j | x_{i}; \theta\right)}{\varphi \theta_{s}}
+(-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m}\cdot 1\left\{y_{i}=s \right\} \frac{1}{p\left(y_{i}=s | x_{i}; \theta\right)}\frac{\varphi p\left(y_{i}=s | x_{i}; \theta\right)}{\varphi \theta_{s}}</script><p>分为两种情况</p><ul><li>当计算结果正好由$\theta_{s}$计算得到，此时线性运算为$z=\theta_{s}^{T} x_{i}$，计算结果为$p\left(y_{i}=s | x_{i}; \theta\right)=\frac{e^{\theta_{s}^{T} x_{i}}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_{i}}}$，求导如下</li></ul><script type="math/tex;mode=display">
\frac{\varphi p\left(y_{i}=s | x_{i}; \theta\right)}{\varphi \theta_{s}}
=\frac{u^{\prime}(x) v(x)-v^{\prime}(x) u(x)}{v^{2}(x)}</script><p>其中</p><script type="math/tex;mode=display">
u(x) = e^{\theta_{s}^{T} x}, v(x)=\sum_{l=1}^{k} e^{\theta_{l}^{T} x}</script><p>所以</p><script type="math/tex;mode=display">
\frac{\varphi u(x)}{\varphi \theta_s} = e^{\theta_{s}^{T} x}\cdot x=u(x)\cdot x,
\frac{\varphi v(x)}{\varphi \theta_s} = e^{\theta_{s}^{T} x}\cdot x=u(x)\cdot x \\
\frac{\varphi p\left(y=s | x_{i}; \theta\right)}{\varphi \theta_{s}} = p\left(y=s | x_{i}; \theta\right)\cdot x_{i}-p\left(y=s | x_{i}; \theta\right)^2\cdot x_{i}</script><ul><li>当计算结果不是由$\theta_{s}$计算得到，此时线性运算为$z=\theta_{j}^{T} x_{i}, j\neq s$，计算结果为$p\left(y_{i}=j | x_{i}; \theta\right)=\frac{e^{\theta_{j}^{T} x_{i}}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_{i}}}$</li></ul><script type="math/tex;mode=display">
\frac{\varphi p\left(y_{i}=j | x_{i}; \theta\right)}{\varphi \theta_{s}}
=\frac{u^{\prime}(x) v(x)-v^{\prime}(x) u(x)}{v^{2}(x)}</script><p>其中</p><script type="math/tex;mode=display">
u(x) = e^{\theta_{j}^{T} x}, v(x)=\sum_{l=1}^{k} e^{\theta_{l}^{T} x}</script><p>所以</p><script type="math/tex;mode=display">
\frac{\varphi u(x)}{\varphi \theta_s} = e^{\theta_{j}^{T} x}\cdot x=0,
\frac{\varphi v(x)}{\varphi \theta_s} = e^{\theta_{s}^{T} x}\cdot x \\
\frac{\varphi p\left(y_{i}=s | x_{i}; \theta\right)}{\varphi \theta_{s}} = -p\left(y_{i}=s | x_{i}; \theta\right)p\left(y_{i}=j | x_{i}; \theta\right)\cdot x_{i}</script><p>综合上述两种情况可知，求导结果为</p><script type="math/tex;mode=display">
\frac{\varphi J(\theta)}{\varphi \theta_{s}}
=(-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m}\cdot \sum_{j=1,j\neq s}^{k} 1\left\{y_{i}=j \right\} \frac{1}{p\left(y_{i}=j | x_{i}; \theta\right)}\frac{\varphi p\left(y_{i}=j | x_{i}; \theta\right)}{\varphi \theta_{s}}
+(-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m}\cdot 1\left\{y_{i}=s \right\} \frac{1}{p\left(y_{i}=s | x_{i}; \theta\right)}\frac{\varphi p\left(y_{i}=s | x_{i}; \theta\right)}{\varphi \theta_{s}} \\
=(-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m}\cdot \sum_{j=1,j\neq s}^{k} 1\left\{y_{i}=j \right\} \frac{1}{p\left(y_{i}=j | x_{i}; \theta\right)})\cdot (-1)\cdot p\left(y_{i}=s | x_{i}; \theta\right)p\left(y_{i}=j | x_{i}; \theta\right)\cdot x_{i} + (-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m}\cdot 1\left\{y_{i}=s \right\} \frac{1}{p\left(y_{i}=s | x_{i}; \theta\right)}\left[p\left(y_{i}=s | x_{i}; \theta\right)\cdot x_{i}-p\left(y_{i}=s | x_{i}; \theta\right)^2\cdot x_{i}\right] \\
=(-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m}\cdot \sum_{j=1,j\neq s}^{k} 1\left\{y_{i}=j \right\}\cdot (-1)\cdot p\left(y_{i}=s | x_{i}; \theta\right)\cdot x_{i} + (-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m}\cdot 1\left\{y_{i}=s \right\} \left[x_{i}-p\left(y_{i}=s | x_{i}; \theta\right)\cdot x_{i}\right] \\
=(-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m}\cdot 1\left\{y_{i}=s \right\} x_{i} - (-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m}\cdot \sum_{j=1}^{k} 1\left\{y_{i}=j \right\} p\left(y_{i}=s | x_{i}; \theta\right)\cdot x_{i}</script><p>因为$\sum_{j=1}^{k} 1\left\{y_{i}=j \right\}=1$，所以最终结果为</p><script type="math/tex;mode=display">
\frac{\varphi J(\theta)}{\varphi \theta_{s}}
=(-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m}\cdot \left[ 1\left\{y_{i}=s \right\} - p\left(y_{i}=s | x_{i}; \theta\right) \right]\cdot x_{i}</script><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>权重$W$大小为$(n+1)\times k$，输入数据集大小为$m\times (n+1)$，输出数据集大小为$m\times k$</p><p>矩阵求导如下：</p><script type="math/tex;mode=display">
\frac{\varphi J(\theta)}{\varphi \theta}
=\frac{1}{m}\cdot \sum_{i=1}^{m}\cdot 
\begin{bmatrix}
(-1)\cdot\left[ 1\left\{y=1 \right\} - p\left(y=1 | x; \theta\right) \right]\cdot x\\ 
(-1)\cdot\left[ 1\left\{y=2 \right\} - p\left(y=2 | x; \theta\right) \right]\cdot x\\ 
...\\ 
(-1)\cdot\left[ 1\left\{y=k \right\} - p\left(y=k | x; \theta\right) \right]\cdot x
\end{bmatrix}
=(-1)\cdot \frac{1}{m}\cdot X_{m\times n+1}^T \cdot (I_{m\times k} - Y_{m\times k})</script><p>参考：</p><p><a href="https://www.kaggle.com/saksham219/softmax-regression-for-iris-classification" target="_blank" rel="noopener">Softmax regression for Iris classification</a></p><p><a href="https://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function" target="_blank" rel="noopener">Derivative of Softmax loss function</a></p><p>上述计算的是输入单个数据时的评分、损失和求导，所以使用随机梯度下降法进行权重更新，分类</p><h2 id="参数冗余和权重衰减"><a href="#参数冗余和权重衰减" class="headerlink" title="参数冗余和权重衰减"></a>参数冗余和权重衰减</h2><p><code>softmax</code>回归存在参数冗余现象，即对参数向量$\theta_{j}$减去向量$\varphi $不改变预测结果。证明如下：</p><script type="math/tex;mode=display">
\begin{aligned} p\left(y^{(i)}=j | x^{(i)} ; \theta\right) &=\frac{e^{\left(\theta_{j}-\psi\right)^{T} x^{(i)}}}{\sum_{l=1}^{k} e^{\left(\theta_{l}-\psi\right)^{T} x^{(i)}}} \\ &=\frac{e^{\theta_{j}^{T} x^{(i)}} e^{-\psi^{T} x^{(i)}}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x^{(i)}} e^{-\psi^{T} x^{(i)}}} \\ &=\frac{e^{\theta_{j}^{T} x^{(i)}}}{\sum_{l=1}^{k} e^{\theta_{t}^{T} x^{(i)}}} \end{aligned}</script><p>假设$(\theta_{1},\theta_{2},…,\theta_{k})$能得到$j(\theta)$的极小值点，那么$(\theta_{1}-\varphi,\theta_{2}-\varphi,…,\theta_{k}-\varphi)$同样能得到相同的极小值点</p><p>与此同时，因为损失函数是凸函数，局部最小值就是全局最小值，所以会导致权重在参数过大情况下就停止收敛，影响模型泛化能力</p><p><strong>在代价函数中加入权重衰减，能够避免过度参数化，得到泛化性能更强的模型</strong></p><p>在代价函数中加入<code>L2</code>正则化项，如下所示：</p><script type="math/tex;mode=display">
J(\theta)
= (-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m} \sum_{j=1}^{k} 1\left\{y_{i}=j\right\} \ln p\left(y_{i}=j | x_{i}; \theta\right) + \frac{\lambda}{2} \sum_{i=1}^{k} \sum_{j=0}^{n} \theta_{i j}^{2}
= (-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m} \sum_{j=1}^{k} 1\left\{y_{i}=j\right\} \ln \frac{e^{\theta_{j}^{T} x_{i}}}{\sum_{l=1}^{k} e^{\theta_{l}^{T} x_{i}}} + \frac{\lambda}{2} \sum_{i=1}^{k} \sum_{j=0}^{n} \theta_{i j}^{2}</script><p>求导结果如下：</p><script type="math/tex;mode=display">
\frac{\varphi J(\theta)}{\varphi \theta_{s}}
=(-1)\cdot \frac{1}{m}\cdot \sum_{i=1}^{m}\cdot \left[ 1\left\{y_{i}=s \right\} - p\left(y_{i}=s | x_{i}; \theta\right) \right]\cdot x_{i}+ \lambda \theta_{j}</script><p>代价实现如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">def compute_loss(scores, indicator, W):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    计算损失值</span><br><span class="line">    :param scores: 大小为(m, n)</span><br><span class="line">    :param indicator: 大小为(m, n)</span><br><span class="line">    :param W: (n, k)</span><br><span class="line">    :return: (m,1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return -1 * np.sum(np.log(scores) * indicator, axis=1) + 0.001/2*np.sum(W**2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_gradient(indicator, scores, x, W):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    计算梯度</span><br><span class="line">    :param indicator: 大小为(m,k)</span><br><span class="line">    :param scores: 大小为(m,k)</span><br><span class="line">    :param x: 大小为(m,n)</span><br><span class="line">    :param W: (n, k)</span><br><span class="line">    :return: (n,k)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return -1 * x.T.dot((indicator - scores)) + 0.001*W</span><br></pre></td></tr></table></figure><h2 id="鸢尾数据集"><a href="#鸢尾数据集" class="headerlink" title="鸢尾数据集"></a>鸢尾数据集</h2><p>使用鸢尾（iris）数据集，参考<a href="https://www.kaggle.com/uciml/iris" target="_blank" rel="noopener">Iris Species</a></p><p>共<code>4</code>个变量：</p><ul><li><code>SepalLengthCm</code> - 花萼长度</li><li><code>SepalWidthCm</code> - 花萼宽度</li><li><code>PetalLengthCm</code> - 花瓣长度</li><li><code>PetalWidthCm</code> - 花瓣宽度</li></ul><p>以及<code>3</code>个类别：</p><ul><li><code>Iris-setosa</code></li><li><code>Iris-versicolor</code></li><li><code>Iris-virginica</code></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">def load_data(shuffle=True, tsize=0.8):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    加载iris数据</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    data = pd.read_csv(data_path, header=0, delimiter=&apos;,&apos;)</span><br><span class="line">    # print(data.columns)</span><br><span class="line"></span><br><span class="line">    draw_data(data.values, data.columns)</span><br><span class="line"></span><br><span class="line">def draw_data(data, columns):</span><br><span class="line">    data_a = data[:50, 1:5]</span><br><span class="line">    data_b = data[50:100, 1:5]</span><br><span class="line">    data_c = data[100:150, 1:5]</span><br><span class="line"></span><br><span class="line">    fig = plt.figure(1)</span><br><span class="line">    plt.scatter(data_a[:, 0], data_a[:, 1], c=&apos;b&apos;, marker=&apos;8&apos;)</span><br><span class="line">    plt.scatter(data_b[:, 0], data_b[:, 1], c=&apos;r&apos;, marker=&apos;s&apos;)</span><br><span class="line">    plt.scatter(data_c[:, 0], data_c[:, 1], c=&apos;y&apos;, marker=&apos;*&apos;)</span><br><span class="line">    plt.xlabel(columns[1])</span><br><span class="line">    plt.ylabel(columns[2])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    fig = plt.figure(2)</span><br><span class="line">    plt.scatter(data_a[:, 2], data_a[:, 3], c=&apos;b&apos;, marker=&apos;8&apos;)</span><br><span class="line">    plt.scatter(data_b[:, 2], data_b[:, 3], c=&apos;r&apos;, marker=&apos;s&apos;)</span><br><span class="line">    plt.scatter(data_c[:, 2], data_c[:, 3], c=&apos;y&apos;, marker=&apos;*&apos;)</span><br><span class="line">    plt.xlabel(columns[3])</span><br><span class="line">    plt.ylabel(columns[4])</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line">    # 验证是否有重复数据</span><br><span class="line">    # for i in range(data_b.shape[0]):</span><br><span class="line">    #     res = list(filter(lambda x: x[0] == data_b[i][0] and x[1] == data_b[i][1], data_c[:, :2]))</span><br><span class="line">    #     if len(res) != 0:</span><br><span class="line">    #         res2 = list(filter(lambda x: x[2] == data_b[i][2] and x[3] == data_b[i][3], data_c[:, 2:4]))</span><br><span class="line">    #         if len(res2) != 0:</span><br><span class="line">    #             print(b[i])</span><br></pre></td></tr></table></figure><p><img src="/imgs/softmax回归/iris_petal.png" alt></p><p><img src="/imgs/softmax回归/iris_sepal.png" alt></p><h2 id="numpy实现"><a href="#numpy实现" class="headerlink" title="numpy实现"></a>numpy实现</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># @Time    : 19-4-25 上午10:30</span><br><span class="line"># @Author  : zj</span><br><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import pandas as pd</span><br><span class="line">from sklearn import utils</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line">import warnings</span><br><span class="line"></span><br><span class="line">warnings.filterwarnings(&apos;ignore&apos;)</span><br><span class="line"></span><br><span class="line">data_path = &apos;../data/iris-species/Iris.csv&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_data(shuffle=True, tsize=0.8):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    加载iris数据</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    data = pd.read_csv(data_path, header=0, delimiter=&apos;,&apos;)</span><br><span class="line"></span><br><span class="line">    if shuffle:</span><br><span class="line">        data = utils.shuffle(data)</span><br><span class="line"></span><br><span class="line">    # 示性函数</span><br><span class="line">    pd_indicator = pd.get_dummies(data[&apos;Species&apos;])</span><br><span class="line">    indicator = np.array(</span><br><span class="line">        [pd_indicator[&apos;Iris-setosa&apos;], pd_indicator[&apos;Iris-versicolor&apos;], pd_indicator[&apos;Iris-virginica&apos;]]).T</span><br><span class="line"></span><br><span class="line">    species_dict = &#123;</span><br><span class="line">        &apos;Iris-setosa&apos;: 0,</span><br><span class="line">        &apos;Iris-versicolor&apos;: 1,</span><br><span class="line">        &apos;Iris-virginica&apos;: 2</span><br><span class="line">    &#125;</span><br><span class="line">    data[&apos;Species&apos;] = data[&apos;Species&apos;].map(species_dict)</span><br><span class="line"></span><br><span class="line">    data_x = np.array(</span><br><span class="line">        [data[&apos;SepalLengthCm&apos;], data[&apos;SepalWidthCm&apos;], data[&apos;PetalLengthCm&apos;], data[&apos;PetalWidthCm&apos;]]).T</span><br><span class="line">    data_y = data[&apos;Species&apos;]</span><br><span class="line"></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, train_size=tsize, test_size=(1 - tsize),</span><br><span class="line">                                                        shuffle=False)</span><br><span class="line"></span><br><span class="line">    y_train = np.atleast_2d(y_train).T</span><br><span class="line">    y_test = np.atleast_2d(y_test).T</span><br><span class="line"></span><br><span class="line">    y_train_indicator = np.atleast_2d(indicator[:y_train.shape[0]])</span><br><span class="line">    y_test_indicator = indicator[y_train.shape[0]:]</span><br><span class="line"></span><br><span class="line">    return x_train, x_test, y_train, y_test, y_train_indicator, y_test_indicator</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def linear(x, w):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    线性操作</span><br><span class="line">    :param x: 大小为(m,n+1)</span><br><span class="line">    :param w: 大小为(n+1,k)</span><br><span class="line">    :return: 大小为(m,k)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return x.dot(w)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def softmax(x):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    softmax归一化计算</span><br><span class="line">    :param x: 大小为(m, k)</span><br><span class="line">    :return: 大小为(m, k)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x -= np.atleast_2d(np.max(x, axis=1)).T</span><br><span class="line">    exps = np.exp(x)</span><br><span class="line">    return exps / np.atleast_2d(np.sum(exps, axis=1)).T</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_scores(X, W):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    计算精度</span><br><span class="line">    :param X: 大小为(m,n+1)</span><br><span class="line">    :param W: 大小为(n+1,k)</span><br><span class="line">    :return: (m,k)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return softmax(linear(X, W))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_loss(scores, indicator, W, la=2e-4):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    计算损失值</span><br><span class="line">    :param scores: 大小为(m, k)</span><br><span class="line">    :param indicator: 大小为(m, k)</span><br><span class="line">    :param W: (n+1, k)</span><br><span class="line">    :return: (1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    cost = -1 / scores.shape[0] * np.sum(np.log(scores) * indicator)</span><br><span class="line">    reg = la / 2 * np.sum(W ** 2)</span><br><span class="line">    return cost + reg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_gradient(scores, indicator, x, W, la=2e-4):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    计算梯度</span><br><span class="line">    :param scores: 大小为(m,k)</span><br><span class="line">    :param indicator: 大小为(m,k)</span><br><span class="line">    :param x: 大小为(m,n+1)</span><br><span class="line">    :param W: (n+1, k)</span><br><span class="line">    :return: (n+1,k)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return -1 / scores.shape[0] * x.T.dot((indicator - scores)) + la * W</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_accuracy(scores, Y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    计算精度</span><br><span class="line">    :param scores: (m,k)</span><br><span class="line">    :param Y: (m,1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    res = np.dstack((np.argmax(scores, axis=1), Y.squeeze())).squeeze()</span><br><span class="line"></span><br><span class="line">    return len(list(filter(lambda x: x[0] == x[1], res[:]))) / len(res)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def draw(res_list, title=None, xlabel=None):</span><br><span class="line">    if title is not None:</span><br><span class="line">        plt.title(title)</span><br><span class="line">    if xlabel is not None:</span><br><span class="line">        plt.xlabel(xlabel)</span><br><span class="line">    plt.plot(res_list)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_gradient_descent(batch_size=8, epoches=2000, alpha=2e-4):</span><br><span class="line">    x_train, x_test, y_train, y_test, y_train_indicator, y_test_indicator = load_data()</span><br><span class="line"></span><br><span class="line">    m, n = x_train.shape[:2]</span><br><span class="line">    k = y_train_indicator.shape[1]</span><br><span class="line">    # 初始化权重(n+1,k)</span><br><span class="line">    W = 0.01 * np.random.normal(loc=0.0, scale=1.0, size=(n + 1, k))</span><br><span class="line">    x_train = np.insert(x_train, 0, np.ones(m), axis=1)</span><br><span class="line">    x_test = np.insert(x_test, 0, np.ones(x_test.shape[0]), axis=1)</span><br><span class="line"></span><br><span class="line">    loss_list = []</span><br><span class="line">    accuracy_list = []</span><br><span class="line">    bestW = None</span><br><span class="line">    bestA = 0</span><br><span class="line">    range_list = np.arange(0, x_train.shape[0] - batch_size, step=batch_size)</span><br><span class="line">    for i in range(epoches):</span><br><span class="line">        for j in range_list:</span><br><span class="line">            data = x_train[j:j + batch_size]</span><br><span class="line">            labels = y_train_indicator[j:j + batch_size]</span><br><span class="line"></span><br><span class="line">            # 计算分类概率</span><br><span class="line">            scores = np.atleast_2d(compute_scores(data, W))</span><br><span class="line">            # 更新梯度</span><br><span class="line">            tempW = W - alpha * compute_gradient(scores, labels, data, W)</span><br><span class="line">            W = tempW</span><br><span class="line"></span><br><span class="line">            if j == range_list[-1]:</span><br><span class="line">                loss = compute_loss(scores, labels, W)</span><br><span class="line">                loss_list.append(loss)</span><br><span class="line"></span><br><span class="line">                accuracy = compute_accuracy(compute_scores(x_train, W), y_train)</span><br><span class="line">                accuracy_list.append(accuracy)</span><br><span class="line">                if accuracy &gt;= bestA:</span><br><span class="line">                    bestA = accuracy</span><br><span class="line">                    bestW = W.copy()</span><br><span class="line">                break</span><br><span class="line"></span><br><span class="line">    draw(loss_list, title=&apos;损失值&apos;)</span><br><span class="line">    draw(accuracy_list, title=&apos;训练精度&apos;)</span><br><span class="line"></span><br><span class="line">    print(bestA)</span><br><span class="line">    print(compute_accuracy(compute_scores(x_test, bestW), y_test))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    compute_gradient_descent(batch_size=8, epoches=100000)</span><br></pre></td></tr></table></figure><p>训练10万次的最好训练结果以及对应的测试结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 测试集精度</span><br><span class="line">0.9916666666666667</span><br><span class="line"># 验证集精度</span><br><span class="line">0.9666666666666667</span><br></pre></td></tr></table></figure><p><img src="/imgs/softmax回归/numpy_softmax_loss.png" alt></p><p><img src="/imgs/softmax回归/numpy_softmax_accuracy.png" alt></p><h3 id="指数计算-数值稳定性考虑"><a href="#指数计算-数值稳定性考虑" class="headerlink" title="指数计算 - 数值稳定性考虑"></a>指数计算 - 数值稳定性考虑</h3><p>参考：<a href="http://cs231n.github.io/linear-classify/" target="_blank" rel="noopener">Practical issues: Numeric stability.</a></p><p>在<code>softmax</code>回归中，需要利用指数函数$e^x$对线性操作的结果进行归一化，这有可能会造成数值溢出，常用的做法是对分数上下同乘以一个常数$C$</p><script type="math/tex;mode=display">
\frac{e^{f_{i_{i}}}}{\sum_{j} e^{f_{j}}}=\frac{C e^{f_{y_{i}}}}{C \sum_{j} e^{f_{j}}}=\frac{e^{f_{i_{i}}+\log C}}{\sum_{j} e^{f_{j}+\log C}}</script><p>这个操作不改变结果，如果取值$C$为线性操作结果最大值负数$\log C=-\max _{j} f_{j}$，就能够将向量$f$的取值范围降低，最大值为$0$，避免数值不稳定</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def softmax(x):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    softmax归一化计算</span><br><span class="line">    :param x: 大小为(m, k)</span><br><span class="line">    :return: 大小为(m, k)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    x -= np.atleast_2d(np.max(x, axis=1)).T</span><br><span class="line">    exps = np.exp(x)</span><br><span class="line">    return exps / np.atleast_2d(np.sum(exps, axis=1)).T</span><br></pre></td></tr></table></figure><h2 id="softmax回归和logistic回归"><a href="#softmax回归和logistic回归" class="headerlink" title="softmax回归和logistic回归"></a>softmax回归和logistic回归</h2><p><code>softmax</code>回归是<code>logistic</code>回归在多分类任务上的扩展，将$k=2$时，<code>softmax</code>回归模型可转换成<code>logistic</code>回归模型</p><script type="math/tex;mode=display">
h_{\theta}(x)=\frac{1}{e^{\theta_{1}^{T} x}+e^{\theta_{2}^{T} x^{(i)}}} \left[ \begin{array}{c}{e^{\theta_{1}^{T} x}} \\ {e^{\theta_{2}^{T} x}}\end{array}\right] 
=\frac{1}{e^{\vec{0}^{T} x}+e^{(\theta_{2}-\theta_{1})^{T} x^{(i)}}} \left[ \begin{array}{c}{e^{\vec{0}^{T} x}} \\ {e^{(\theta_{2}-\theta_{1})^{T} x}}\end{array}\right] \\
=\frac{1}{1+e^{(\theta_{2}-\theta_{1})^{T} x^{(i)}}} \left[ \begin{array}{c}{1} \\ {e^{(\theta_{2}-\theta_{1})^{T} x}}\end{array}\right]
= \left[ \begin{array}{c}{\frac{1}{1+e^{(\theta_{2}-\theta_{1})^{T} x^{(i)}}}} \\ {\frac{e^{(\theta_{2}-\theta_{1})^{T} x}}{1+e^{(\theta_{2}-\theta_{1})^{T} x^{(i)}}}}\end{array}\right]
=\left[ \begin{array}{c}{\frac{1}{1+e^{(\theta_{2}-\theta_{1})^{T} x^{(i)}}}} \\ {1- \frac{1}{1+e^{(\theta_{2}-\theta_{1})^{T} x^{(i)}}}}\end{array}\right]</script><p>针对多分类任务，可以选择<code>softmax</code>回归模型进行多分类，也可以选择<code>logistic</code>回归模型进行若干个二分类</p><p>区别在于选择的类别是否<strong>互斥</strong>，如果类别互斥，使用<code>softmax</code>回归分类更为合适；如果类别不互斥，使用<code>logistic</code>回归分类更为合适</p></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.jpg" alt="zhujian 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="zhujian 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zhujian</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zhujian.tech/posts/2626bec3.html" title="softmax回归">https://www.zhujian.tech/posts/2626bec3.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/python/" rel="tag"># python</a> <a href="/tags/calculus/" rel="tag"># 微积分</a> <a href="/tags/numpy/" rel="tag"># numpy</a> <a href="/tags/matplotlib/" rel="tag"># matplotlib</a> <a href="/tags/sklearn/" rel="tag"># sklearn</a> <a href="/tags/pandas/" rel="tag"># pandas</a> <a href="/tags/softmax/" rel="tag"># softmax</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/5d2f01d1.html" rel="next" title="成绩函数、目标函数、代价函数和损失函数"><i class="fa fa-chevron-left"></i> 成绩函数、目标函数、代价函数和损失函数</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/posts/1c195604.html" rel="prev" title="从numpy到pytorch实现softmax回归">从numpy到pytorch实现softmax回归<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#对数函数操作"><span class="nav-number">1.</span> <span class="nav-text">对数函数操作</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#求导公式"><span class="nav-number">2.</span> <span class="nav-text">求导公式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#单个输入数据进行softmax回归计算"><span class="nav-number">3.</span> <span class="nav-text">单个输入数据进行softmax回归计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#评分函数"><span class="nav-number">3.1.</span> <span class="nav-text">评分函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数"><span class="nav-number">3.2.</span> <span class="nav-text">损失函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#批量数据进行softmax回归计算"><span class="nav-number">4.</span> <span class="nav-text">批量数据进行softmax回归计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#评分函数-1"><span class="nav-number">4.1.</span> <span class="nav-text">评分函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代价函数"><span class="nav-number">4.2.</span> <span class="nav-text">代价函数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度下降"><span class="nav-number">5.</span> <span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#参数冗余和权重衰减"><span class="nav-number">6.</span> <span class="nav-text">参数冗余和权重衰减</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#鸢尾数据集"><span class="nav-number">7.</span> <span class="nav-text">鸢尾数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#numpy实现"><span class="nav-number">8.</span> <span class="nav-text">numpy实现</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#指数计算-数值稳定性考虑"><span class="nav-number">8.1.</span> <span class="nav-text">指数计算 - 数值稳定性考虑</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#softmax回归和logistic回归"><span class="nav-number">9.</span> <span class="nav-text">softmax回归和logistic回归</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zhujian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">zhujian</p><div class="site-description" itemprop="description">one bite at a time</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">169</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">129</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/./atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zjZSTU" title="Github &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;zjZSTU" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/u012005313" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;u012005313" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i> CSDN</a></span><span class="links-of-author-item"><a href="/mailto:zjzstu@gmail.com" title="Gmail &amp;rarr; mailto:zjzstu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> Gmail</a></span><span class="links-of-author-item"><a href="/mailto:505169307@gmail.com" title="QQmail &amp;rarr; mailto:505169307@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> QQmail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://cloud.tencent.com/" title="https:&#x2F;&#x2F;cloud.tencent.com" rel="noopener" target="_blank">腾讯云</a></li><li class="links-of-blogroll-item"> <a href="https://www.aliyun.com/" title="https:&#x2F;&#x2F;www.aliyun.com" rel="noopener" target="_blank">阿里云</a></li><li class="links-of-blogroll-item"> <a href="https://developer.android.com/" title="https:&#x2F;&#x2F;developer.android.com&#x2F;" rel="noopener" target="_blank">Android Dev</a></li><li class="links-of-blogroll-item"> <a href="https://jenkins.io/" title="https:&#x2F;&#x2F;jenkins.io&#x2F;" rel="noopener" target="_blank">Jenkins</a></li><li class="links-of-blogroll-item"> <a href="http://cs231n.github.io/" title="http:&#x2F;&#x2F;cs231n.github.io&#x2F;" rel="noopener" target="_blank">cs231n</a></li><li class="links-of-blogroll-item"> <a href="https://pytorch.org/docs/stable/index.html" title="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;index.html" rel="noopener" target="_blank">pytorch</a></li><li class="links-of-blogroll-item"> <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" title="https:&#x2F;&#x2F;docs.docker.com&#x2F;install&#x2F;linux&#x2F;docker-ce&#x2F;ubuntu&#x2F;" rel="noopener" target="_blank">docker</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">zhujian</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">948k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">26:20</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-bar-chart-o"></i></span> <a href="https://tongji.baidu.com/web/27249108/overview/index?siteId=13647183" rel="noopener" target="_blank">百度统计</a></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div><div class="beian"> <a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备 19026415号</a> <span class="post-meta-divider">|</span> <img src="/images/beian_icon.png" style="display:inline-block;text-decoration:none;height:13px"> <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001748" rel="noopener" target="_blank">浙公网安备 33011802001748号</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span> <span class="site-uv" title="总访客量">访客数：<span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="site-pv" title="总访问量">阅读量：<span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="17,63,61" opacity="1" zindex="-1" count="199" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '8f22595c6dd29c94467d',
      clientSecret: 'e66bd90ebb9aa66c562c753dec6c90ceecbd51f2',
      repo: 'guestbook',
      owner: 'zjZSTU',
      admin: ['zjZSTU'],
      id: '27b634f5a5c2c90b10d46bb9df16378d',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script></body></html>