<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/./atom.xml" title="做一个幸福的人" type="application/atom+xml"><meta name="google-site-verification" content="Qr_3yqLtyErDFKHW7mE8PJz4qDUX-bf_fMLpSRckQe4"><meta name="baidu-site-verification" content="zOwIvKMV7f"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"搜索文章",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet"><meta name="description" content="学习边框回归的概念时，发现一篇自定义检测器的文章 原文：Getting Started With Bounding Box Regression In TensorFlow 中文：目标检测之边框回归入门【Tensorflow】"><meta name="keywords" content="AlexNet,python,pytorch,LeNet-5,image localization dataset"><meta property="og:type" content="article"><meta property="og:title" content="[pytorch]训练一个简单的检测器"><meta property="og:url" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;posts&#x2F;5bfa4e56.html"><meta property="og:site_name" content="做一个幸福的人"><meta property="og:description" content="学习边框回归的概念时，发现一篇自定义检测器的文章 原文：Getting Started With Bounding Box Regression In TensorFlow 中文：目标检测之边框回归入门【Tensorflow】"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;detector-location&#x2F;box_detector.png"><meta property="og:updated_time" content="2020-02-15T05:36:35.871Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;detector-location&#x2F;box_detector.png"><link rel="canonical" href="https://www.zhujian.tech/posts/5bfa4e56.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>[pytorch]训练一个简单的检测器 | 做一个幸福的人</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e677aac1ac69b8826b9cfecb4e72e107";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">做一个幸福的人</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">面朝大海，春暖花开</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">129</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">48</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">169</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header> <a href="https://github.com/zjZSTU" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zhujian.tech/posts/5bfa4e56.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="zhujian"><meta itemprop="description" content="one bite at a time"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="做一个幸福的人"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> [pytorch]训练一个简单的检测器</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2020-01-18 20:30:40" itemprop="dateCreated datePublished" datetime="2020-01-18T20:30:40+00:00">2020-01-18</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-15 05:36:35" itemprop="dateModified" datetime="2020-02-15T05:36:35+00:00">2020-02-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/" itemprop="url" rel="index"><span itemprop="name">算法</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">编程</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/deep-learning/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/programming-language/" itemprop="url" rel="index"><span itemprop="name">编程语言</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/data/" itemprop="url" rel="index"><span itemprop="name">数据</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/deep-learning/convolutional-neural-network/" itemprop="url" rel="index"><span itemprop="name">卷积神经网络</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/codebase/" itemprop="url" rel="index"><span itemprop="name">代码库</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/data/dataset/" itemprop="url" rel="index"><span itemprop="name">数据集</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>10k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>17 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>学习边框回归的概念时，发现一篇自定义检测器的文章</p><ul><li>原文：<a href="https://towardsdatascience.com/getting-started-with-bounding-box-regression-in-tensorflow-743e22d0ccb3" target="_blank" rel="noopener">Getting Started With Bounding Box Regression In TensorFlow</a></li><li>中文：<a href="http://blog.hubwiz.com/2019/09/16/bounding-box-regression/" target="_blank" rel="noopener">目标检测之边框回归入门【Tensorflow】</a></li></ul><a id="more"></a><p>虽然题目写的是边框回归，但是里面没有讲解相关的概念，而是自定义了一个边框检测器，实现原理比较简单。看完之后感觉挺有趣的，之前也没有自己实现过检测器，原文使用<code>TensorFlow</code>实现，当前使用<code>PyTorch</code>进行复现</p><h2 id="操作流程"><a href="#操作流程" class="headerlink" title="操作流程"></a>操作流程</h2><ol><li>定位数据集</li><li>自定义损失函数</li><li>训练</li><li>检测</li></ol><h2 id="定位数据集"><a href="#定位数据集" class="headerlink" title="定位数据集"></a>定位数据集</h2><p>使用一个简单的<a href="https://zhujian.tech/posts/a2d65e1.html" target="_blank" rel="noopener">图像定位数据集</a>：</p><blockquote><ul><li>包含<code>3</code>类：<code>Cucumber</code>（黄瓜）、<code>Eggplant</code>（茄子）、<code>Mushroom</code>（蘑菇）</li><li>每类共有超过<code>60</code>张的图像，大小固定为<code>(227, 277, 3)</code>，每张图像里有一个物体</li><li>每张图像有一个对应的<code>xml</code>文件，格式和<code>PASCAL VOC</code>数据集一致，包含图像信息以及标注信息</li></ul></blockquote><h2 id="自定义损失函数"><a href="#自定义损失函数" class="headerlink" title="自定义损失函数"></a>自定义损失函数</h2><p>使用<code>MSE(Mean Squared Error)</code>和<a href="https://zhujian.tech/posts/796ebd4e.html" target="_blank" rel="noopener">IoU(Intersection over Union)</a>作为损失函数</p><script type="math/tex;mode=display">
loss(x, x^{'}) = MSE(x, x^{'}) + (1 - IoU(x, x^{'}))</script><p><code>MSE</code>计算的是预测标签和真值标签之间的损失，<code>1- IoU</code>计算的是预测边框和真值边框之间的损失，这样能够保证模型计算结果能够检测指定类别以及得到相应的标注信息。所以训练标签包含了边框标注数据和类别信息（<code>one-hot</code>编码形式）</p><h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><ul><li>自定义了数据集类<code>LocationDataSet</code>用于图像定位数据集的加载</li><li>自定义了损失函数类<code>MSE_IoU</code>用于损失函数的计算和求导</li><li>分别使用<code>LeNet-5</code>和<code>AlexNet</code>进行训练</li><li>使用随机梯度下降进行模型优化，初始学习率为<code>1e-3</code>，动量大小为<code>0.9</code>，使用<code>Nesterov</code>加速，共训练<code>100</code>轮</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">@author: zj</span><br><span class="line">@file:   bounding.py</span><br><span class="line">@time:   2020-01-15</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">import os</span><br><span class="line">import logging</span><br><span class="line">import cv2</span><br><span class="line">import numpy as np</span><br><span class="line">import xmltodict</span><br><span class="line">import torch</span><br><span class="line">from torch.utils.data import Dataset</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.optim as optim</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">import torchvision.transforms as transforms</span><br><span class="line">from torchvision.models import alexnet</span><br><span class="line"></span><br><span class="line">logging.basicConfig(format=&apos;%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s&apos;, level=logging.DEBUG)</span><br><span class="line"></span><br><span class="line"># LeNet-5</span><br><span class="line"># input_dim = 32</span><br><span class="line"># AlexNet</span><br><span class="line">input_dim = 227</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class LocationDataSet(Dataset):</span><br><span class="line"></span><br><span class="line">    def __init__(self, root_dir, train=True, transform=None, input_dim=1):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        自定义数据集类，加载定位数据集</span><br><span class="line">        1. 训练部分，加载编码前50图像和标记数据</span><br><span class="line">        2. 测试部分，加载编码50之后图像和标记数据</span><br><span class="line">        :param root_dir:</span><br><span class="line">        :param train:</span><br><span class="line">        :param transform:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        cates = [&apos;cucumber&apos;, &apos;eggplant&apos;, &apos;mushroom&apos;]</span><br><span class="line">        class_binary_label = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]</span><br><span class="line">        self.train = train</span><br><span class="line">        self.transform = transform</span><br><span class="line"></span><br><span class="line">        self.imgs = []</span><br><span class="line">        self.bboxes = []</span><br><span class="line">        self.classes = []</span><br><span class="line"></span><br><span class="line">        for cate_idx in range(3):</span><br><span class="line">            if self.train:</span><br><span class="line">                for i in range(1, 51):</span><br><span class="line">                    img, bndbox, class_name = self._get_item(root_dir, cates[cate_idx], i)</span><br><span class="line">                    bndbox = bndbox / input_dim</span><br><span class="line"></span><br><span class="line">                    self.imgs.append(img)</span><br><span class="line">                    self.bboxes.append(np.hstack((bndbox, class_binary_label[cate_idx])))</span><br><span class="line">                    self.classes.append(class_name)</span><br><span class="line">            else:</span><br><span class="line">                for i in range(51, 61):</span><br><span class="line">                    img, bndbox, class_name = self._get_item(root_dir, cates[cate_idx], i)</span><br><span class="line">                    bndbox = bndbox / input_dim</span><br><span class="line"></span><br><span class="line">                    self.imgs.append(img)</span><br><span class="line">                    self.bboxes.append(np.hstack((bndbox, class_binary_label[cate_idx])))</span><br><span class="line">                    self.classes.append(class_name)</span><br><span class="line"></span><br><span class="line">    def __getitem__(self, idx):</span><br><span class="line">        img = self.imgs[idx]</span><br><span class="line">        if self.transform:</span><br><span class="line">            sample = self.transform(img)</span><br><span class="line">        else:</span><br><span class="line">            sample = img</span><br><span class="line">        return sample, torch.Tensor(self.bboxes[idx]).float()</span><br><span class="line"></span><br><span class="line">    def __len__(self):</span><br><span class="line">        return len(self.imgs)</span><br><span class="line"></span><br><span class="line">    def _get_item(self, root_dir, cate, i):</span><br><span class="line">        img_path = os.path.join(root_dir, &apos;%s_%d.jpg&apos; % (cate, i))</span><br><span class="line">        img = cv2.imread(img_path)</span><br><span class="line"></span><br><span class="line">        xml_path = os.path.join(root_dir, &apos;%s_%d.xml&apos; % (cate, i))</span><br><span class="line">        x = xmltodict.parse(open(xml_path, &apos;rb&apos;))</span><br><span class="line">        bndbox = x[&apos;annotation&apos;][&apos;object&apos;][&apos;bndbox&apos;]</span><br><span class="line">        bndbox = np.array(</span><br><span class="line">            [float(bndbox[&apos;xmin&apos;]), float(bndbox[&apos;ymin&apos;]), float(bndbox[&apos;xmax&apos;]), float(bndbox[&apos;ymax&apos;])])</span><br><span class="line"></span><br><span class="line">        return img, bndbox, x[&apos;annotation&apos;][&apos;object&apos;][&apos;name&apos;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_data():</span><br><span class="line">    transform = transforms.Compose([</span><br><span class="line">        transforms.ToPILImage(),</span><br><span class="line">        transforms.Resize(input_dim),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    root_dir = &apos;./data/image-localization-dataset/training_images/&apos;</span><br><span class="line">    train_dataset = LocationDataSet(root_dir, train=True, transform=transform, input_dim=input_dim)</span><br><span class="line">    test_dataset = LocationDataSet(root_dir, train=False, transform=transform, input_dim=input_dim)</span><br><span class="line"></span><br><span class="line">    train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4)</span><br><span class="line">    test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=True, num_workers=4)</span><br><span class="line"></span><br><span class="line">    return train_dataloader, test_dataloader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class MSE_IoU(nn.Module):</span><br><span class="line"></span><br><span class="line">    def calculate_iou(self, target_boxes, pred_boxes):</span><br><span class="line">        # 计算重叠区域的左上角和右下角坐标</span><br><span class="line">        x_min = torch.max(target_boxes[:, 0], pred_boxes[:, 0])</span><br><span class="line">        y_min = torch.max(target_boxes[:, 1], pred_boxes[:, 1])</span><br><span class="line">        x_max = torch.min(target_boxes[:, 2], pred_boxes[:, 2])</span><br><span class="line">        y_max = torch.min(target_boxes[:, 3], pred_boxes[:, 3])</span><br><span class="line">        # 计算交集面积</span><br><span class="line">        intersection = torch.max(torch.zeros(x_max.shape).cuda(), x_max - x_min) \</span><br><span class="line">                       * torch.max(torch.zeros(y_max.shape).cuda(), y_max - y_min)</span><br><span class="line"></span><br><span class="line">        # 计算两个边界框面积</span><br><span class="line">        boxAArea = (target_boxes[:, 2] - target_boxes[:, 0]) * (target_boxes[:, 3] - target_boxes[:, 1])</span><br><span class="line">        boxBArea = (pred_boxes[:, 2] - pred_boxes[:, 0]) * (pred_boxes[:, 3] - pred_boxes[:, 1])</span><br><span class="line"></span><br><span class="line">        iou = intersection / (boxAArea + boxBArea - intersection)</span><br><span class="line">        return iou</span><br><span class="line"></span><br><span class="line">    def forward(self, target_boxes, pred_boxes):</span><br><span class="line">        mseloss = nn.MSELoss().forward(target_boxes, pred_boxes)</span><br><span class="line">        iouloss = torch.mean(1 - self.calculate_iou(target_boxes, pred_boxes))</span><br><span class="line"></span><br><span class="line">        return mseloss + iouloss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class LeNet5(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self, in_channels=1, num_classes=10):</span><br><span class="line">        super(LeNet5, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=6, kernel_size=5, stride=1, padding=0, bias=True)</span><br><span class="line">        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True)</span><br><span class="line">        self.conv3 = nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1, padding=0, bias=True)</span><br><span class="line"></span><br><span class="line">        self.pool = nn.MaxPool2d((2, 2), stride=2)</span><br><span class="line"></span><br><span class="line">        self.fc1 = nn.Linear(in_features=120, out_features=84, bias=True)</span><br><span class="line">        self.fc2 = nn.Linear(84, num_classes, bias=True)</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        x = self.pool(F.relu(self.conv1(input)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = self.conv3(x)</span><br><span class="line"></span><br><span class="line">        x = x.view(-1, self.num_flat_features(x))</span><br><span class="line"></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        return self.fc2(x)</span><br><span class="line"></span><br><span class="line">    def num_flat_features(self, x):</span><br><span class="line">        size = x.size()[1:]  # all dimensions except the batch dimension</span><br><span class="line">        num_features = 1</span><br><span class="line">        for s in size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        return num_features</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_accuracy(loader, net, device):</span><br><span class="line">    total_accuracy = 0</span><br><span class="line">    num = 0</span><br><span class="line">    for item in loader:</span><br><span class="line">        data, labels = item</span><br><span class="line">        data = data.to(device)</span><br><span class="line">        labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">        scores = net.forward(data)</span><br><span class="line">        predicted = torch.nn.functional.one_hot(torch.argmax(scores[:, 4:7], dim=1), num_classes=3)</span><br><span class="line">        total_accuracy += torch.mean((predicted == labels[:, 4:7]).float()).item()</span><br><span class="line">        num += 1</span><br><span class="line">    return total_accuracy / num</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    train_dataloader, test_dataloader = load_data()</span><br><span class="line"></span><br><span class="line">    device = &quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;</span><br><span class="line"></span><br><span class="line">    num_classes = 7</span><br><span class="line">    # net = LeNet5(in_channels=3, num_classes=num_classes).to(device)</span><br><span class="line">    net = alexnet(num_classes=num_classes).to(device)</span><br><span class="line">    criterion = MSE_IoU().to(device)</span><br><span class="line">    # optimer = optim.Adam(net.parameters(), lr=1e-3)</span><br><span class="line">    optimer = optim.SGD(net.parameters(), lr=1e-3, momentum=0.9, nesterov=True)</span><br><span class="line"></span><br><span class="line">    logging.info(&quot;开始训练&quot;)</span><br><span class="line">    epoches = 100</span><br><span class="line">    for i in range(epoches):</span><br><span class="line">        num = 0</span><br><span class="line">        total_loss = 0</span><br><span class="line">        for j, item in enumerate(train_dataloader, 0):</span><br><span class="line">            data, labels = item</span><br><span class="line">            data = data.to(device)</span><br><span class="line">            labels = labels.to(device)</span><br><span class="line"></span><br><span class="line">            scores = net.forward(data)</span><br><span class="line">            loss = criterion.forward(scores, labels)</span><br><span class="line">            total_loss += loss.item()</span><br><span class="line"></span><br><span class="line">            optimer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            optimer.step()</span><br><span class="line">            num += 1</span><br><span class="line">        avg_loss = total_loss / num</span><br><span class="line">        logging.info(&apos;epoch: %d loss: %.6f&apos; % (i + 1, total_loss / num))</span><br><span class="line">        train_accuracy = compute_accuracy(train_dataloader, net, device)</span><br><span class="line">        test_accuracy = compute_accuracy(test_dataloader, net, device)</span><br><span class="line">        logging.info(&apos;train accuracy: %f test accuracy: %f&apos; % (train_accuracy, test_accuracy))</span><br><span class="line"></span><br><span class="line">    # torch.save(net.state_dict(), &apos;./model/LeNet-5.pth&apos;)</span><br><span class="line">    torch.save(net.state_dict(), &apos;./model/AlexNet.pth&apos;)</span><br><span class="line"></span><br><span class="line">    img, label = test_dataloader.dataset.__getitem__(10)</span><br><span class="line">    img = img.unsqueeze(0).to(device)</span><br><span class="line">    print(img.shape)</span><br><span class="line">    print(label)</span><br><span class="line">    scores = net.forward(img)</span><br><span class="line">    print(scores)</span><br></pre></td></tr></table></figure><p>使用<code>AlexNet</code>训练日志如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">2020-01-18 21:03:25,738 box-detector.py[line:230] INFO epoch: 98 loss: 0.225710</span><br><span class="line">2020-01-18 21:03:26,068 box-detector.py[line:234] INFO train accuracy: 0.894737 test accuracy: 0.854167</span><br><span class="line">2020-01-18 21:03:26,737 box-detector.py[line:230] INFO epoch: 99 loss: 0.224704</span><br><span class="line">2020-01-18 21:03:27,065 box-detector.py[line:234] INFO train accuracy: 0.903509 test accuracy: 0.833333</span><br><span class="line">2020-01-18 21:03:27,747 box-detector.py[line:230] INFO epoch: 100 loss: 0.230456</span><br><span class="line">2020-01-18 21:03:28,076 box-detector.py[line:234] INFO train accuracy: 0.899123 test accuracy: 0.854167</span><br><span class="line">torch.Size([1, 3, 227, 227])</span><br><span class="line">tensor([0.2247, 0.2599, 0.8414, 0.7225, 0.0000, 1.0000, 0.0000])</span><br><span class="line">tensor([[0.2072, 0.2580, 0.7714, 0.6773, 0.3503, 0.4326, 0.1010]],</span><br><span class="line">       device=&apos;cuda:0&apos;, grad_fn=&lt;AddmmBackward&gt;)</span><br></pre></td></tr></table></figure><p>模型保存在<code>./model/AlexNet.pth</code>中</p><h2 id="检测"><a href="#检测" class="headerlink" title="检测"></a>检测</h2><p>调用保存的模型进行标注检测，并比较相应的检测类别</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">@author: zj</span><br><span class="line">@file:   draw_box.py</span><br><span class="line">@time:   2020-01-18</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">import cv2</span><br><span class="line">import numpy as np</span><br><span class="line">import torch</span><br><span class="line">import torchvision.transforms as transforms</span><br><span class="line">from torchvision.models import alexnet</span><br><span class="line"></span><br><span class="line">from box_detector import load_data</span><br><span class="line">from box_detector import input_dim</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    train_loader, test_loader = load_data()</span><br><span class="line">    device = &quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;</span><br><span class="line"></span><br><span class="line">    dataset = test_loader.dataset</span><br><span class="line">    img, label = dataset.__getitem__(0)</span><br><span class="line">    image = img.unsqueeze(0).to(device)</span><br><span class="line">    label = label.unsqueeze(0)</span><br><span class="line"></span><br><span class="line">    num_classes = 7</span><br><span class="line">    # net = LeNet5(in_channels=3, num_classes=num_classes).to(device)</span><br><span class="line">    net = alexnet(num_classes=num_classes).to(device)</span><br><span class="line">    # net.load_state_dict(torch.load(&apos;./model/LeNet-5.pth&apos;))</span><br><span class="line">    net.load_state_dict(torch.load(&apos;./model/AlexNet.pth&apos;))</span><br><span class="line">    net.eval()</span><br><span class="line"></span><br><span class="line">    scores = net.forward(image)</span><br><span class="line">    print(scores)</span><br><span class="line">    print(label)</span><br><span class="line"></span><br><span class="line">    predict_cate = torch.argmax(scores[:, 4:7], dim=1)</span><br><span class="line">    truth_cate = torch.argmax(label[:, 4:7], dim=1)</span><br><span class="line">    print(&apos;predict: &apos; + str(predict_cate) + &apos; truth: &apos; + str(truth_cate))</span><br><span class="line"></span><br><span class="line">    img = img * 0.5 + 0.5</span><br><span class="line">    transform = transforms.Compose([</span><br><span class="line">        transforms.ToPILImage(),</span><br><span class="line">        transforms.Resize(227)</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    truth_rect = label[:, :4] * input_dim</span><br><span class="line">    predict_rect = scores[:, :4] * input_dim</span><br><span class="line">    print(truth_rect)</span><br><span class="line">    print(predict_rect)</span><br><span class="line"></span><br><span class="line">    origin = np.array(transform(img), dtype=np.uint8)</span><br><span class="line"></span><br><span class="line">    x_min, y_min, x_max, y_max = truth_rect.squeeze()[:4]</span><br><span class="line">    cv2.rectangle(origin, (x_min, y_min), (x_max, y_max),</span><br><span class="line">                  (0, 255, 0), thickness=2)</span><br><span class="line">    x_min, y_min, x_max, y_max = predict_rect.squeeze()[:4]</span><br><span class="line">    cv2.rectangle(origin, (x_min, y_min), (x_max, y_max),</span><br><span class="line">                  (0, 0, 255), thickness=2)</span><br><span class="line">    cv2.imwrite(&apos;box_detector.png&apos;, origin)</span><br><span class="line">    # cv2.imshow(&apos;img&apos;, origin)</span><br><span class="line">    # cv2.waitKey(0)</span><br></pre></td></tr></table></figure><p>输出结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tensor([[  7.,  17., 220., 199.]])</span><br><span class="line">tensor([[  2.9428,  16.3437, 221.9320, 199.8142]], device=&apos;cuda:0&apos;,</span><br><span class="line">       grad_fn=&lt;MulBackward0&gt;)</span><br></pre></td></tr></table></figure><p><img src="/imgs/detector-location/box_detector.png" alt></p></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.jpg" alt="zhujian 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="zhujian 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zhujian</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zhujian.tech/posts/5bfa4e56.html" title="[pytorch]训练一个简单的检测器">https://www.zhujian.tech/posts/5bfa4e56.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/AlexNet/" rel="tag"># AlexNet</a> <a href="/tags/python/" rel="tag"># python</a> <a href="/tags/pytorch/" rel="tag"># pytorch</a> <a href="/tags/LeNet-5/" rel="tag"># LeNet-5</a> <a href="/tags/image-localization-dataset/" rel="tag"># image localization dataset</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/a2d65e1.html" rel="next" title="[数据集]Image Localization Dataset"><i class="fa fa-chevron-left"></i> [数据集]Image Localization Dataset</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/posts/921c65be.html" rel="prev" title="[OKR]2020年2月份">[OKR]2020年2月份<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#操作流程"><span class="nav-number">1.</span> <span class="nav-text">操作流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#定位数据集"><span class="nav-number">2.</span> <span class="nav-text">定位数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#自定义损失函数"><span class="nav-number">3.</span> <span class="nav-text">自定义损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#训练"><span class="nav-number">4.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#检测"><span class="nav-number">5.</span> <span class="nav-text">检测</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zhujian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">zhujian</p><div class="site-description" itemprop="description">one bite at a time</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">169</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">129</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/./atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zjZSTU" title="Github &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;zjZSTU" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/u012005313" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;u012005313" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i> CSDN</a></span><span class="links-of-author-item"><a href="/mailto:zjzstu@gmail.com" title="Gmail &amp;rarr; mailto:zjzstu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> Gmail</a></span><span class="links-of-author-item"><a href="/mailto:505169307@gmail.com" title="QQmail &amp;rarr; mailto:505169307@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> QQmail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://cloud.tencent.com/" title="https:&#x2F;&#x2F;cloud.tencent.com" rel="noopener" target="_blank">腾讯云</a></li><li class="links-of-blogroll-item"> <a href="https://www.aliyun.com/" title="https:&#x2F;&#x2F;www.aliyun.com" rel="noopener" target="_blank">阿里云</a></li><li class="links-of-blogroll-item"> <a href="https://developer.android.com/" title="https:&#x2F;&#x2F;developer.android.com&#x2F;" rel="noopener" target="_blank">Android Dev</a></li><li class="links-of-blogroll-item"> <a href="https://jenkins.io/" title="https:&#x2F;&#x2F;jenkins.io&#x2F;" rel="noopener" target="_blank">Jenkins</a></li><li class="links-of-blogroll-item"> <a href="http://cs231n.github.io/" title="http:&#x2F;&#x2F;cs231n.github.io&#x2F;" rel="noopener" target="_blank">cs231n</a></li><li class="links-of-blogroll-item"> <a href="https://pytorch.org/docs/stable/index.html" title="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;index.html" rel="noopener" target="_blank">pytorch</a></li><li class="links-of-blogroll-item"> <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" title="https:&#x2F;&#x2F;docs.docker.com&#x2F;install&#x2F;linux&#x2F;docker-ce&#x2F;ubuntu&#x2F;" rel="noopener" target="_blank">docker</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">zhujian</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">948k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">26:20</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-bar-chart-o"></i></span> <a href="https://tongji.baidu.com/web/27249108/overview/index?siteId=13647183" rel="noopener" target="_blank">百度统计</a></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div><div class="beian"> <a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备 19026415号</a> <span class="post-meta-divider">|</span> <img src="/images/beian_icon.png" style="display:inline-block;text-decoration:none;height:13px"> <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001748" rel="noopener" target="_blank">浙公网安备 33011802001748号</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span> <span class="site-uv" title="总访客量">访客数：<span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="site-pv" title="总访问量">阅读量：<span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="17,63,61" opacity="1" zindex="-1" count="199" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '8f22595c6dd29c94467d',
      clientSecret: 'e66bd90ebb9aa66c562c753dec6c90ceecbd51f2',
      repo: 'guestbook',
      owner: 'zjZSTU',
      admin: ['zjZSTU'],
      id: 'd700fb4cd50ed16790718015a7796595',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script></body></html>