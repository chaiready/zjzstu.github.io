<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/./atom.xml" title="做一个幸福的人" type="application/atom+xml"><meta name="google-site-verification" content="Qr_3yqLtyErDFKHW7mE8PJz4qDUX-bf_fMLpSRckQe4"><meta name="baidu-site-verification" content="zOwIvKMV7f"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"搜索文章",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet"><meta name="description" content="参考：神经网络实现-numpy使用softmax回归进行mnist分类PyTorch 进阶之路（四）：在 GPU 上训练深度神经网络"><meta name="keywords" content="python,pytorch,numpy,matplotlib,神经网络"><meta property="og:type" content="article"><meta property="og:title" content="神经网络实现-pytorch"><meta property="og:url" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;posts&#x2F;5a77dbca.html"><meta property="og:site_name" content="做一个幸福的人"><meta property="og:description" content="参考：神经网络实现-numpy使用softmax回归进行mnist分类PyTorch 进阶之路（四）：在 GPU 上训练深度神经网络"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;神经网络实现-pytorch&#x2F;mnist_loss.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;神经网络实现-pytorch&#x2F;mnist_accuracy.png"><meta property="og:updated_time" content="2020-02-15T05:36:35.875Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;神经网络实现-pytorch&#x2F;mnist_loss.png"><link rel="canonical" href="https://www.zhujian.tech/posts/5a77dbca.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>神经网络实现-pytorch | 做一个幸福的人</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e677aac1ac69b8826b9cfecb4e72e107";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">做一个幸福的人</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">面朝大海，春暖花开</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">129</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">48</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">169</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header> <a href="https://github.com/zjZSTU" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zhujian.tech/posts/5a77dbca.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="zhujian"><meta itemprop="description" content="one bite at a time"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="做一个幸福的人"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 神经网络实现-pytorch</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-05-18 15:01:30" itemprop="dateCreated datePublished" datetime="2019-05-18T15:01:30+00:00">2019-05-18</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-15 05:36:35" itemprop="dateModified" datetime="2020-02-15T05:36:35+00:00">2020-02-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/" itemprop="url" rel="index"><span itemprop="name">算法</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">编程</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/deep-learning/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/programming-language/" itemprop="url" rel="index"><span itemprop="name">编程语言</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/data-learning/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/codebase/" itemprop="url" rel="index"><span itemprop="name">代码库</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>7.2k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>12 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>参考：</p><p><a href="https://www.zhujian.tech/posts/ba2ca878.html#more">神经网络实现-numpy</a></p><p><a href="https://www.zhujian.tech/posts/dd673751.html#more">使用softmax回归进行mnist分类</a></p><p><a href="https://www.jiqizhixin.com/articles/2019-04-09-9" target="_blank" rel="noopener">PyTorch 进阶之路（四）：在 GPU 上训练深度神经网络</a></p><a id="more"></a><p>使用<code>pytorch</code>实现<code>3</code>层神经网络模型<code>ThreeNet</code></p><h2 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h2><p><code>ThreeNet</code>是一个<code>3</code>层神经网络，输入层大小为<code>784</code>，隐藏层大小分别为<code>200</code>和<code>60</code>，输出层大小为<code>10</code>，激活函数使用<code>relu</code>，评分函数使用<code>softmax</code></p><p>网络参数如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 批量大小</span><br><span class="line">batch_size = 256</span><br><span class="line"># 输入维数</span><br><span class="line">D = 784</span><br><span class="line"># 隐藏层大小</span><br><span class="line">H1 = 200</span><br><span class="line">H2 = 60</span><br><span class="line"># 输出类别</span><br><span class="line">K = 10</span><br><span class="line"></span><br><span class="line"># 学习率</span><br><span class="line">learning_rate = 1e-2</span><br><span class="line"></span><br><span class="line"># 迭代次数</span><br><span class="line">epoches = 500</span><br></pre></td></tr></table></figure><p>网络定义如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">class NNModule(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(NNModule, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(D, H1)</span><br><span class="line">        self.fc2 = nn.Linear(H1, H2)</span><br><span class="line">        self.fc3 = nn.Linear(H2, K)</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        x = F.relu(self.fc1(input))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = F.log_softmax(self.fc3(x))</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">    def __copy__(self, device):</span><br><span class="line">        module = NNModule().to(device=device)</span><br><span class="line">        module.fc1.weight = copy.deepcopy(self.fc1.weight)</span><br><span class="line">        module.fc1.bias = copy.deepcopy(self.fc1.bias)</span><br><span class="line"></span><br><span class="line">        module.fc2.weight = copy.deepcopy(self.fc2.weight)</span><br><span class="line">        module.fc2.bias = copy.deepcopy(self.fc2.bias)</span><br><span class="line"></span><br><span class="line">        module.fc3.weight = copy.deepcopy(self.fc3.weight)</span><br><span class="line">        module.fc3.bias = copy.deepcopy(self.fc3.bias)</span><br><span class="line"></span><br><span class="line">        return module</span><br></pre></td></tr></table></figure><h2 id="mnist数据"><a href="#mnist数据" class="headerlink" title="mnist数据"></a>mnist数据</h2><p><code>pytorch</code>的<code>torchvision</code>模块内置了<code>MNIST</code>类，用于下载<code>mnist</code>数据集`</p><p>利用<code>torchvision.transforms</code>将数据转换为<code>Tensor</code>结构并进行初始化</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Grayscale(),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    transforms.Normalize(mean=(0.5,), std=(0.5,))</span><br><span class="line">])</span><br></pre></td></tr></table></figure><ul><li><p><code>Grayscale()</code>将<code>PIL</code>图像转换为灰度图像</p></li><li><p><code>ToTensor()</code>将<code>PIL</code>图像或<code>numpy.ndarray</code>数据转换为<code>tensor</code>，将原先$(H\times W\times C)$通道转换成$(C\times H\times W)$，同时将取值范围<code>[0,255]</code>压缩到<code>[0.0, 1.0]</code></p></li><li><p><code>Normalize()</code>对数据进行归一化，均值为<code>0.5</code>，标准差为<code>0.5</code></p></li></ul><script type="math/tex;mode=display">
y = \frac {x-mean}{std}</script><p>利用<code>torch.utils.data.DataLoader</code>保存数据，方便打乱和批量加载</p><p>代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def load_data(batch_size=128, shuffle=False):</span><br><span class="line">    data_dir = &apos;../data/&apos;</span><br><span class="line"></span><br><span class="line">    transform = transforms.Compose([</span><br><span class="line">        transforms.Grayscale(),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(mean=(0.5,), std=(0.5,))</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    train_data_set = datasets.MNIST(root=data_dir, train=True, download=True, transform=transform)</span><br><span class="line">    test_data_set = datasets.MNIST(root=data_dir, train=False, download=True, transform=transform)</span><br><span class="line"></span><br><span class="line">    train_loader = DataLoader(train_data_set, batch_size=batch_size, shuffle=shuffle)</span><br><span class="line">    test_loader = DataLoader(test_data_set, batch_size=batch_size, shuffle=shuffle)</span><br><span class="line"></span><br><span class="line">    return train_loader, test_loader</span><br></pre></td></tr></table></figure><h2 id="pytorch实现"><a href="#pytorch实现" class="headerlink" title="pytorch实现"></a>pytorch实现</h2><p>完整代码如下，实现<code>pytorch gpu</code>训练</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># @Time    : 19-5-18 下午3:03</span><br><span class="line"># @Author  : zj</span><br><span class="line"></span><br><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">import torch.nn.functional as F</span><br><span class="line">import torch.optim as optim</span><br><span class="line">from torchvision import datasets</span><br><span class="line">import torchvision.transforms as transforms</span><br><span class="line">from torch.utils.data import DataLoader</span><br><span class="line">import time</span><br><span class="line">import copy</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line"></span><br><span class="line">import warnings</span><br><span class="line"></span><br><span class="line">warnings.filterwarnings(&apos;ignore&apos;)</span><br><span class="line"></span><br><span class="line"># 批量大小</span><br><span class="line">batch_size = 256</span><br><span class="line"># 输入维数</span><br><span class="line">D = 784</span><br><span class="line"># 隐藏层大小</span><br><span class="line">H1 = 200</span><br><span class="line">H2 = 60</span><br><span class="line"># 输出类别</span><br><span class="line">K = 10</span><br><span class="line"></span><br><span class="line"># 学习率</span><br><span class="line">learning_rate = 1e-2</span><br><span class="line"></span><br><span class="line"># 迭代次数</span><br><span class="line">epoches = 500</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_data(batch_size=128, shuffle=False):</span><br><span class="line">    data_dir = &apos;../data/&apos;</span><br><span class="line"></span><br><span class="line">    transform = transforms.Compose([</span><br><span class="line">        transforms.Grayscale(),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">        transforms.Normalize(mean=(0.5,), std=(0.5,))</span><br><span class="line">    ])</span><br><span class="line"></span><br><span class="line">    train_data_set = datasets.MNIST(root=data_dir, train=True, download=True, transform=transform)</span><br><span class="line">    test_data_set = datasets.MNIST(root=data_dir, train=False, download=True, transform=transform)</span><br><span class="line"></span><br><span class="line">    train_loader = DataLoader(train_data_set, batch_size=batch_size, shuffle=shuffle)</span><br><span class="line">    test_loader = DataLoader(test_data_set, batch_size=batch_size, shuffle=shuffle)</span><br><span class="line"></span><br><span class="line">    return train_loader, test_loader</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_accuracy(module, dataLoader, device=torch.device(&apos;cpu&apos;)):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    计算精度</span><br><span class="line">    :param module: 计算模型</span><br><span class="line">    :param dataLoader: 数据加载器</span><br><span class="line"></span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    accuracy = 0</span><br><span class="line">    for i, items in enumerate(dataLoader, 0):</span><br><span class="line">        data, labels = items</span><br><span class="line">        data = data.reshape((data.size()[0], -1))</span><br><span class="line">        data, labels = data.to(device=device), labels.to(device=device)</span><br><span class="line"></span><br><span class="line">        scores = module.forward(data)</span><br><span class="line">        predictions = torch.argmax(scores, dim=1)</span><br><span class="line">        res = (predictions == labels.squeeze())</span><br><span class="line">        accuracy += 1.0 * torch.sum(res).item() / scores.size()[0]</span><br><span class="line">    return accuracy / dataLoader.__len__()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def draw(loss_list, title=&apos;损失图&apos;, ylabel=&apos;损失值&apos;, xlabel=&apos;迭代/20次&apos;):</span><br><span class="line">    plt.title(title)</span><br><span class="line">    plt.ylabel(ylabel)</span><br><span class="line">    plt.xlabel(xlabel)</span><br><span class="line">    plt.plot(loss_list)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class NNModule(nn.Module):</span><br><span class="line"></span><br><span class="line">    def __init__(self):</span><br><span class="line">        super(NNModule, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(D, H1)</span><br><span class="line">        self.fc2 = nn.Linear(H1, H2)</span><br><span class="line">        self.fc3 = nn.Linear(H2, K)</span><br><span class="line"></span><br><span class="line">    def forward(self, input):</span><br><span class="line">        x = F.relu(self.fc1(input))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = F.log_softmax(self.fc3(x))</span><br><span class="line">        return x</span><br><span class="line"></span><br><span class="line">    def __copy__(self, device):</span><br><span class="line">        module = NNModule().to(device=device)</span><br><span class="line">        module.fc1.weight = copy.deepcopy(self.fc1.weight)</span><br><span class="line">        module.fc1.bias = copy.deepcopy(self.fc1.bias)</span><br><span class="line"></span><br><span class="line">        module.fc2.weight = copy.deepcopy(self.fc2.weight)</span><br><span class="line">        module.fc2.bias = copy.deepcopy(self.fc2.bias)</span><br><span class="line"></span><br><span class="line">        module.fc3.weight = copy.deepcopy(self.fc3.weight)</span><br><span class="line">        module.fc3.bias = copy.deepcopy(self.fc3.bias)</span><br><span class="line"></span><br><span class="line">        return module</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_gradient_descent(batch_size=8, epoches=2000):</span><br><span class="line">    train_loader, test_loader = load_data(batch_size=batch_size, shuffle=True)</span><br><span class="line"></span><br><span class="line">    device = torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;)</span><br><span class="line"></span><br><span class="line">    # softmax模型</span><br><span class="line">    module = NNModule().to(device=device)</span><br><span class="line">    # 损失函数</span><br><span class="line">    criterion = nn.NLLLoss().to(device=device)</span><br><span class="line">    # 优化器</span><br><span class="line">    optimizer = optim.SGD(module.parameters(), lr=learning_rate)</span><br><span class="line"></span><br><span class="line">    loss_list = []</span><br><span class="line">    accuracy_list = []</span><br><span class="line">    bestA = 0</span><br><span class="line">    bestModule = None</span><br><span class="line"></span><br><span class="line">    batch_len = train_loader.__len__()</span><br><span class="line">    for i in range(epoches):</span><br><span class="line">        start = time.time()</span><br><span class="line">        for j, items in enumerate(train_loader, 0):</span><br><span class="line">            data, labels = items</span><br><span class="line">            data = data.reshape((data.size()[0], -1))</span><br><span class="line">            data, labels = data.to(device=device), labels.to(device=device)</span><br><span class="line"></span><br><span class="line">            scores = module.forward(data)</span><br><span class="line">            loss = criterion(scores, labels.squeeze())</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            # 反向传播</span><br><span class="line">            loss.backward()</span><br><span class="line">            # 梯度更新</span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line">            if j == (batch_len - 1):</span><br><span class="line">                loss_list.append(loss.item())</span><br><span class="line">        end = time.time()</span><br><span class="line">        print(&apos;epoch： %d time: %.2f s&apos; % (i + 1, end - start))</span><br><span class="line">        if i % 20 == 19:  # 每个20次进行一次检测</span><br><span class="line">            start = time.time()</span><br><span class="line">            accuracy = compute_accuracy(module, train_loader, device)</span><br><span class="line">            accuracy_list.append(accuracy)</span><br><span class="line">            if accuracy &gt;= bestA:</span><br><span class="line">                bestA = accuracy</span><br><span class="line">                bestModule = module.__copy__(device)</span><br><span class="line">            end = time.time()</span><br><span class="line">            print(&apos;epoch: %d time: %.2f s accuracy: %.3f %%&apos; % (i + 1, end - start, accuracy * 100))</span><br><span class="line"></span><br><span class="line">    draw(loss_list, title=&apos;mnist&apos;, xlabel=&apos;迭代/次&apos;)</span><br><span class="line">    draw(accuracy_list, title=&apos;训练精度&apos;, ylabel=&apos;检测精度&apos;, xlabel=&apos;迭代/20次&apos;)</span><br><span class="line"></span><br><span class="line">    test_accuracy = compute_accuracy(bestModule, test_loader, device)</span><br><span class="line"></span><br><span class="line">    print(&apos;best train accuracy is %.3f %%&apos; % (bestA * 100))</span><br><span class="line">    print(&apos;test accuracy is %.3f %%&apos; % (test_accuracy * 100))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    start = time.time()</span><br><span class="line">    compute_gradient_descent(batch_size=batch_size, epoches=epoches)</span><br><span class="line">    end = time.time()</span><br><span class="line">    print(&apos;all train and test need time: %.2f minutes&apos; % ((end - start) / 60.0))</span><br></pre></td></tr></table></figure><p>训练<code>500</code>次精度如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">best train accuracy is 99.997 %</span><br><span class="line">test accuracy is 97.959 %</span><br><span class="line">all train and test need time: 71.90 minutes</span><br></pre></td></tr></table></figure><p><img src="/imgs/神经网络实现-pytorch/mnist_loss.png" alt></p><p><img src="/imgs/神经网络实现-pytorch/mnist_accuracy.png" alt></p><h2 id="softmax-log-softmax和NLLLoss-CrossEntropyLoss"><a href="#softmax-log-softmax和NLLLoss-CrossEntropyLoss" class="headerlink" title="softmax/log_softmax和NLLLoss/CrossEntropyLoss"></a>softmax/log_softmax和NLLLoss/CrossEntropyLoss</h2><p><code>pytorch</code>提供了多种<code>softmax</code>评分以及损失函数</p><h3 id="评分函数"><a href="#评分函数" class="headerlink" title="评分函数"></a>评分函数</h3><ol><li><p><a href="https://pytorch.org/docs/stable/nn.html?highlight=softmax#torch.nn.Softmax" target="_blank" rel="noopener">torch.nn.Softmax</a>：标准的<code>softmax</code>评分函数，在官网中提示了<code>LogSoftmax</code>有更快和更好的数值属性</p><script type="math/tex;mode=display">
 \text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}</script><pre><code> This module doesn’t work directly with NLLLoss, which expects the Log to be computed between the Softmax and itself. 
 Use LogSoftmax instead (it’s faster and has better numerical properties).
</code></pre></li><li><p><a href="https://pytorch.org/docs/stable/nn.html#logsoftmax" target="_blank" rel="noopener">torch.nn.LogSoftmax</a>：在$Softmax(x)$的基础上添加了对数运算</p><script type="math/tex;mode=display">
 \text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)</script></li></ol><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><ol><li><p><a href="https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss" target="_blank" rel="noopener">torch.nn.CrossEntropyLoss</a>：结合了$nn.LogSoftmax()$和$nn.NLLLoss()$操作</p></li><li><p><a href="https://pytorch.org/docs/stable/nn.html#nllloss" target="_blank" rel="noopener">torch.nn.NLLLoss</a>：负对数似然损失（<code>negative log likelihood loss</code>）</p></li></ol><h3 id="组合使用"><a href="#组合使用" class="headerlink" title="组合使用"></a>组合使用</h3><p>所以得到隐藏层输出向量后，使用$nn.CrossEntropyLoss$或者$nn.LogSoftmax+nn.NLLLoss$就能够实现损失值的计算，如果想要输出评分值，那么使用$nn.Softmax$</p></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.jpg" alt="zhujian 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="zhujian 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zhujian</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zhujian.tech/posts/5a77dbca.html" title="神经网络实现-pytorch">https://www.zhujian.tech/posts/5a77dbca.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/python/" rel="tag"># python</a> <a href="/tags/pytorch/" rel="tag"># pytorch</a> <a href="/tags/numpy/" rel="tag"># numpy</a> <a href="/tags/matplotlib/" rel="tag"># matplotlib</a> <a href="/tags/nerual-network/" rel="tag"># 神经网络</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/ba2ca878.html" rel="next" title="神经网络实现-numpy"><i class="fa fa-chevron-left"></i> 神经网络实现-numpy</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/posts/3b660279.html" rel="prev" title="卷积神经网络概述">卷积神经网络概述<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#网络结构"><span class="nav-number">1.</span> <span class="nav-text">网络结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mnist数据"><span class="nav-number">2.</span> <span class="nav-text">mnist数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#pytorch实现"><span class="nav-number">3.</span> <span class="nav-text">pytorch实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#softmax-log-softmax和NLLLoss-CrossEntropyLoss"><span class="nav-number">4.</span> <span class="nav-text">softmax/log_softmax和NLLLoss/CrossEntropyLoss</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#评分函数"><span class="nav-number">4.1.</span> <span class="nav-text">评分函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#损失函数"><span class="nav-number">4.2.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#组合使用"><span class="nav-number">4.3.</span> <span class="nav-text">组合使用</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zhujian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">zhujian</p><div class="site-description" itemprop="description">one bite at a time</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">169</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">129</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/./atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zjZSTU" title="Github &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;zjZSTU" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/u012005313" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;u012005313" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i> CSDN</a></span><span class="links-of-author-item"><a href="/mailto:zjzstu@gmail.com" title="Gmail &amp;rarr; mailto:zjzstu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> Gmail</a></span><span class="links-of-author-item"><a href="/mailto:505169307@gmail.com" title="QQmail &amp;rarr; mailto:505169307@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> QQmail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://cloud.tencent.com/" title="https:&#x2F;&#x2F;cloud.tencent.com" rel="noopener" target="_blank">腾讯云</a></li><li class="links-of-blogroll-item"> <a href="https://www.aliyun.com/" title="https:&#x2F;&#x2F;www.aliyun.com" rel="noopener" target="_blank">阿里云</a></li><li class="links-of-blogroll-item"> <a href="https://developer.android.com/" title="https:&#x2F;&#x2F;developer.android.com&#x2F;" rel="noopener" target="_blank">Android Dev</a></li><li class="links-of-blogroll-item"> <a href="https://jenkins.io/" title="https:&#x2F;&#x2F;jenkins.io&#x2F;" rel="noopener" target="_blank">Jenkins</a></li><li class="links-of-blogroll-item"> <a href="http://cs231n.github.io/" title="http:&#x2F;&#x2F;cs231n.github.io&#x2F;" rel="noopener" target="_blank">cs231n</a></li><li class="links-of-blogroll-item"> <a href="https://pytorch.org/docs/stable/index.html" title="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;index.html" rel="noopener" target="_blank">pytorch</a></li><li class="links-of-blogroll-item"> <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" title="https:&#x2F;&#x2F;docs.docker.com&#x2F;install&#x2F;linux&#x2F;docker-ce&#x2F;ubuntu&#x2F;" rel="noopener" target="_blank">docker</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">zhujian</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">948k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">26:20</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-bar-chart-o"></i></span> <a href="https://tongji.baidu.com/web/27249108/overview/index?siteId=13647183" rel="noopener" target="_blank">百度统计</a></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div><div class="beian"> <a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备 19026415号</a> <span class="post-meta-divider">|</span> <img src="/images/beian_icon.png" style="display:inline-block;text-decoration:none;height:13px"> <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001748" rel="noopener" target="_blank">浙公网安备 33011802001748号</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span> <span class="site-uv" title="总访客量">访客数：<span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="site-pv" title="总访问量">阅读量：<span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="17,63,61" opacity="1" zindex="-1" count="199" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '8f22595c6dd29c94467d',
      clientSecret: 'e66bd90ebb9aa66c562c753dec6c90ceecbd51f2',
      repo: 'guestbook',
      owner: 'zjZSTU',
      admin: ['zjZSTU'],
      id: 'bd82c88b51550b94d181c928ea334b19',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script></body></html>