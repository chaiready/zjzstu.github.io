<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/./atom.xml" title="做一个幸福的人" type="application/atom+xml"><meta name="google-site-verification" content="Qr_3yqLtyErDFKHW7mE8PJz4qDUX-bf_fMLpSRckQe4"><meta name="baidu-site-verification" content="zOwIvKMV7f"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"搜索文章",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet"><meta name="description" content="参考：《机器学习基础 原理、算法与实践》第二章 线性回归Python 机器学习：线性回归主要内容如下："><meta name="keywords" content="python,numpy,matplotlib,线性回归"><meta property="og:type" content="article"><meta property="og:title" content="线性回归"><meta property="og:url" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;posts&#x2F;ec419bd2.html"><meta property="og:site_name" content="做一个幸福的人"><meta property="og:description" content="参考：《机器学习基础 原理、算法与实践》第二章 线性回归Python 机器学习：线性回归主要内容如下："><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;线性回归&#x2F;sweden.png"><meta property="og:updated_time" content="2020-02-15T05:36:35.875Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;线性回归&#x2F;sweden.png"><link rel="canonical" href="https://www.zhujian.tech/posts/ec419bd2.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>线性回归 | 做一个幸福的人</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e677aac1ac69b8826b9cfecb4e72e107";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">做一个幸福的人</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">面朝大海，春暖花开</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">129</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">48</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">169</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header> <a href="https://github.com/zjZSTU" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zhujian.tech/posts/ec419bd2.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="zhujian"><meta itemprop="description" content="one bite at a time"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="做一个幸福的人"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 线性回归</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-04-09 19:36:36" itemprop="dateCreated datePublished" datetime="2019-04-09T19:36:36+00:00">2019-04-09</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-15 05:36:35" itemprop="dateModified" datetime="2020-02-15T05:36:35+00:00">2020-02-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/" itemprop="url" rel="index"><span itemprop="name">算法</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">编程</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/programming-language/" itemprop="url" rel="index"><span itemprop="name">编程语言</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/data-learning/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/codebase/" itemprop="url" rel="index"><span itemprop="name">代码库</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>7.2k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>12 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>参考：</p><p>《机器学习基础 原理、算法与实践》第二章 线性回归</p><p><a href="http://baijiahao.baidu.com/s?id=1602127602901158968&amp;wfr=spider&amp;for=pc" target="_blank" rel="noopener">Python 机器学习：线性回归</a></p><p>主要内容如下：</p><a id="more"></a><ol><li>回归和分类的区别</li><li>线性回归</li><li>最小二乘法</li><li>梯度下降法</li></ol><h2 id="回归和分类"><a href="#回归和分类" class="headerlink" title="回归和分类"></a>回归和分类</h2><p>参考：</p><p><a href="https://www.jiqizhixin.com/articles/2017-12-15-2" target="_blank" rel="noopener">区分识别机器学习中的分类与回归</a></p><p><a href="https://www.zhihu.com/question/21329754" target="_blank" rel="noopener">分类与回归区别是什么？</a></p><p>回归和分类一样，都是对变量进行预测</p><p>回归是对<strong>连续型变量</strong>进行预测，回归预测建模是指建立输入变量<code>X</code>映射到<strong>连续输出变量Y</strong>的映射函数<code>f</code></p><p>分类是对<em>离散型或连续型变量</em>进行预测，分类预测建模是指建立输入变量<code>X</code>映射到<strong>离散输出变量Y</strong>的映射函数<code>f</code></p><p>比如，预测天气温度是回归问题，预测天气是下雨还是晴天就是分类问题</p><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>线性回归（<code>linear regression</code>）是以线性模型来建模自变量和因变量之间关系的方法</p><script type="math/tex;mode=display">
y=h(x;\theta)</script><p>其中$x$是自变量，$y$是因变量，$\theta$是模型参数</p><p>如果自变量$x$只有一个，那么这种问题称为单变量线性回归（或称为一元线性回归）；如果自变量$x$表示多个，那么成为多变量线性回归（或称为多元线性回归）</p><h3 id="单变量线性回归"><a href="#单变量线性回归" class="headerlink" title="单变量线性回归"></a>单变量线性回归</h3><p>单变量线性问题可转换为求解二维平面上的直线问题</p><p>模型计算公式如下：</p><script type="math/tex;mode=display">
y=w_{1}x+w_{0}</script><p>参数集合$\theta = \left \{ w_{0},w_{1} \right \}$</p><p>在学习过程中，需要判断参数$w_{0}$和$w_{1}$是否满足要求，即是否和所有数据点接近。使用<code>均方误差（mean square error，简称MSE）</code>来评估预测值和实际数据点的接近程度，模型评估公式如下：</p><script type="math/tex;mode=display">
J(w_{0},w_{1})=\frac{1}{N}\sum_{i=1}^{N}(h(x_{i};\theta) - y_{i})^{2}</script><p>其中$y_{i}$表示真实数据，$h$表示估计值，$J$表示损失值</p><h3 id="多变量线性回归"><a href="#多变量线性回归" class="headerlink" title="多变量线性回归"></a>多变量线性回归</h3><p>多变量线性回归计算公式如下：</p><script type="math/tex;mode=display">
\left\{\begin{matrix}
y_{1}=w_{0}+w_{1}\cdot x_{11}+...+w_{n}\cdot x_{1n}\\ 
y_{2}=w_{0}+w_{1}\cdot x_{21}+...+w_{n}\cdot x_{2n}\\ 
...\\ 
y_{m}=w_{0}+w_{1}\cdot x_{m1}+...+w_{n}\cdot x_{mn}
\end{matrix}\right.</script><p>参数$m$表示有$m$个等式，参数$n$表示每一组变量有$n$个参数。设$x_{0}=1$，计算公式如下：</p><script type="math/tex;mode=display">
\left\{\begin{matrix}
y_{1}=w_{0}\cdot x_{10}+w_{1}\cdot x_{11}+...+w_{n}\cdot x_{1n}\\ 
y_{2}=w_{0}\cdot x_{20}+w_{1}\cdot x_{21}+...+w_{n}\cdot x_{2n}\\ 
...\\ 
y_{m}=w_{0}\cdot x_{m0}+w_{1}\cdot x_{m1}+...+w_{n}\cdot x_{mn}
\end{matrix}\right.</script><p>此时每组参数个数增加为$n+1$，其向量化公式如下：</p><script type="math/tex;mode=display">
Y=X\cdot W</script><p>其中</p><script type="math/tex;mode=display">
Y_{m\times 1}=\begin{bmatrix}
y_{1}\\ 
y_{2}\\ 
...\\ 
y_{m}
\end{bmatrix}</script><script type="math/tex;mode=display">
X_{m\times (n+1)}=\begin{bmatrix}
x_{10} & x_{11} & ... & x_{1n}\\ 
x_{20} & x_{21} & ... & x_{2n}\\ 
... & ... & ... & ...\\ 
x_{m0} & x_{m1} & ... & x_{mn}
\end{bmatrix}
=\begin{bmatrix}
1 & x_{11} & ... & x_{1n}\\ 
1 & x_{21} & ... & x_{2n}\\ 
... & ... & ... & ...\\ 
1 & x_{m1} & ... & x_{mn}
\end{bmatrix}</script><script type="math/tex;mode=display">
W_{(n+1)\times 1}=
\begin{bmatrix}
w_{0} \\ w_{1} \\ ... \\ w_{n}
\end{bmatrix}</script><p>同样使用均方误差作为损失函数</p><script type="math/tex;mode=display">
J(W)=\frac{1}{N}\sum_{i=1}^{N}(h(x_{i};W) - y_{i})^{2}</script><h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><p>参考：</p><p><a href="https://zhuanlan.zhihu.com/p/31146740" target="_blank" rel="noopener">机器学习数学：最小二乘法</a></p><p><a href="https://baike.baidu.com/item/%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98%E6%B3%95" target="_blank" rel="noopener">最小二乘法</a></p><p>利用最小二乘法（<code>least square method</code>）计算线性回归问题的参数，它通过<strong>最小化误差的平方和</strong>来求取目标函数的最优值，这样进一步转换为求取损失函数$J$的最小值，当$J$得到最小值时，参数偏导数一定为<code>0</code></p><script type="math/tex;mode=display">
loss=N*J=\sum_{i=1}^{N}(h(x_{i}:\theta) - y_{i})^{2}</script><p>有两种方式进行最小二乘法的计算，使用<strong>几何方式</strong>计算单变量线性回归问题，使用<strong>矩阵方式</strong>计算多变量线性回归问题</p><h3 id="几何计算"><a href="#几何计算" class="headerlink" title="几何计算"></a>几何计算</h3><p>当$J$得到最小值时，$w_{0}$和$w_{1}$的偏导数一定为<code>0</code>，所以参数$w_{0}$和$w_{1}$的计算公式如下：</p><script type="math/tex;mode=display">
\frac{\varphi J}{\varphi w_{0}}
=\frac{\varphi }{\varphi w_{0}}\frac{1}{N} \sum_{i=1}^{N}(h(x_{i})-y_{i})^{2}
=\frac{\varphi }{\varphi w_{0}}\frac{1}{N} \sum_{i=1}^{N}(w_{0}+w_{1}\cdot x_{i}-y_{i})^{2}</script><script type="math/tex;mode=display">
=\frac{2}{N} \sum_{i=1}^{N}(w_{0}+w_{1}\cdot x_{i}-y_{i})
=2\cdot w_{0}+\frac{2}{N} \sum_{i=1}^{N}(w_{1}\cdot x_{i}-y_{i})</script><script type="math/tex;mode=display">
\frac{\varphi J}{\varphi w_{0}}=0 
\Rightarrow 
w_{0}=-\frac{1}{N}\sum_{i=1}^{N}(w_{1}\cdot x_{i}-y_{i})
=\frac{1}{N}(\sum_{i=1}^{N}y_{i}-\sum_{i=1}^{N}w_{1}\cdot x_{i})
=\bar{y}-w_{1}\cdot \bar{x}</script><script type="math/tex;mode=display">
\frac{\varphi J}{\varphi w_{1}}
=\frac{\varphi }{\varphi w_{1}}\frac{1}{N} \sum_{i=1}^{N}(h(x_{i})-y_{i})^{2}
=\frac{\varphi }{\varphi w_{1}}\frac{1}{N} \sum_{i=1}^{N}(w_{0}+w_{1}\cdot x_{i}-y_{i})^{2}</script><script type="math/tex;mode=display">
=\frac{2}{N} \sum_{i=1}^{N}(w_{0}+w_{1}\cdot x_{i}-y_{i})\cdot x_{i}
=\frac{2\cdot w_{0}}{N}\sum_{i=1}^{N}x_{i}+\frac{2\cdot w_{1}}{N}\sum_{i=1}^{N}x_{i}\cdot x_{i}-\frac{2}{N}\sum_{i=1}^{N}x_{i}\cdot y_{i}</script><script type="math/tex;mode=display">
=2\cdot w_{0}\cdot \bar{x}+2\cdot w_{1}\cdot \bar{x^{2}}-2\cdot \bar{x\cdot y}</script><script type="math/tex;mode=display">
\frac{\varphi J}{\varphi w_{1}}=0,  w_{0}=\bar{y}-w_{1}\cdot \bar{x}
\Rightarrow
\bar{x}\cdot \bar{y}-w_{1}\cdot \bar{x}^{2}+w_{1}\cdot \bar{x^{2}}-\bar{x\cdot y}=0
\Rightarrow
w_{1}=\frac{\bar{x\cdot y} - \bar{x}\cdot \bar{y}}{\bar{x^{2}}-\bar{x}^{2}}</script><p>最终得到的$w_{0}$和$w_{1}$的计算公式如下：</p><script type="math/tex;mode=display">
w_{0}=\bar{y}-w_{1}\cdot \bar{x}</script><script type="math/tex;mode=display">
w_{1}=\frac{\bar{x\cdot y} - \bar{x}\cdot \bar{y}}{\bar{x^{2}}-\bar{x}^{2}}</script><ul><li>参数$\bar{y}$表示真实结果的均值</li><li>参数$\bar{x}$表示输入变量的均值</li><li>参数$\bar{x\cdot y}$表示输入变量和真实结果的乘积的均值</li><li>其他变量以此类推</li></ul><h3 id="矩阵计算"><a href="#矩阵计算" class="headerlink" title="矩阵计算"></a>矩阵计算</h3><p>参考：<a href="https://zhuanlan.zhihu.com/p/33899560" target="_blank" rel="noopener">最小二乘法线性回归：矩阵视角</a></p><p>基本矩阵运算如下：</p><script type="math/tex;mode=display">
(X\pm Y)^T = X^T\pm Y^T</script><script type="math/tex;mode=display">
(X\cdot Y)^{T}=Y^{T}\cdot X^{T}</script><script type="math/tex;mode=display">
(A^T)^T=A</script><script type="math/tex;mode=display">
\left | A^{T} \right |=\left | A \right |</script><p>矩阵求导如下：</p><script type="math/tex;mode=display">
\frac{\varphi (\theta ^{T}\cdot X)}{\varphi \theta}=X</script><script type="math/tex;mode=display">
\frac{\varphi (X^{T}\cdot \theta )}{\varphi \theta}=X</script><script type="math/tex;mode=display">
\frac{\varphi (\theta ^{T}\cdot \theta )}{\varphi \theta}=\theta</script><script type="math/tex;mode=display">
\frac{\varphi (\theta ^{T}\cdot C\cdot \theta )}{\varphi \theta}=2\cdot C\cdot \theta</script><p>对多变量线性线性回归问题进行计算，</p><script type="math/tex;mode=display">
J(W)
=\frac{1}{N}\cdot \sum_{i=1}^{N}(h(x_{i};W)-y_{i})^2
=\frac{1}{N}(X\cdot W-Y)^{T}\cdot (X\cdot W -Y)</script><script type="math/tex;mode=display">
=\frac{1}{N}((X\cdot W)^T-Y^T)\cdot (X\cdot W-Y))
=\frac{1}{N}(W^T\cdot X^T-Y^T)\cdot (X\cdot W-Y))</script><script type="math/tex;mode=display">
=\frac{1}{N}(W^T\cdot X^{T}\cdot X\cdot W-W^T\cdot X^{T}\cdot Y-Y^{T}\cdot X\cdot W+Y^{T}\cdot Y)</script><p>其中，$W^T\cdot X^{T}\cdot Y$是$Y^{T}\cdot X\cdot W$的转置，计算结果均为$1\times 1$的标量，所以大小相等，上式计算如下：</p><script type="math/tex;mode=display">
J(W)=\frac{1}{N}(W^T\cdot X^{T}\cdot X\cdot W-2\cdot W^T\cdot X^{T}\cdot Y+Y^{T}\cdot Y)</script><p>求解$\frac{\varphi J(W)}{\varphi W}=0$</p><script type="math/tex;mode=display">
\frac{\varphi J(W)}{\varphi W}=\frac{1}{N}\cdot X^{T}\cdot X\cdot W-\frac{1}{N}\cdot X^{T}\cdot Y=0</script><script type="math/tex;mode=display">
\Rightarrow X^{T}\cdot X\cdot W=X^{T}\cdot Y</script><script type="math/tex;mode=display">
\Rightarrow W=(X^{T}\cdot X)^{-1}\cdot X^{T}\cdot Y</script><p><strong>$X^{T}\cdot X$必须是非奇异矩阵，满足$\left | X^{T}\cdot X \right |\neq 0$，才能保证可逆</strong></p><p>对于矩阵的秩，有以下定理</p><ol><li>对于$n$阶矩阵$A$，当且仅当$\left | A \right | \neq 0$时，$R(A_{n})=n$，称$A$为满秩矩阵</li><li>$R(A^T)=R(A)$</li><li>$R(AB)\leq min \left \{ R(A), R(B)\right \}$</li><li>设$A$为$m\times n$矩阵，则$0\leq (A)\leq min \left \{ m,n \right \}$</li></ol><p>所以矩阵$X$的秩$R(X)$需要为$n+1$（通常样本数量$m$大于变量数量$n+1$）时，才能保证能够使用最小二乘法的矩阵方式求解线性回归问题</p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>单边量线性回归测试数据参考[线性回归最小二乘法和梯度下降法]的<a href="https://www.math.muni.cz/~kolacek/docs/frvs/M7222/data/AutoInsurSweden.txt" target="_blank" rel="noopener">瑞典汽车保险数据集</a></p><p>多变量线性回归测试数据参考<code>coursera</code>的<a href="https://github.com/peedeep/Coursera/blob/master/ex1/ex1data2.txt" target="_blank" rel="noopener">ex1data2.txt</a></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># @Time    : 19-4-16 上午10:04</span><br><span class="line"># @Author  : zj</span><br><span class="line"></span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">最小二乘法计算线性回归问题</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line"></span><br><span class="line">from mpl_toolkits.mplot3d import Axes3D</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_sweden_data():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    加载单变量数据</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    path = &apos;../data/sweden.txt&apos;</span><br><span class="line">    res = None</span><br><span class="line">    with open(path, &apos;r&apos;) as f:</span><br><span class="line">        line = f.readline()</span><br><span class="line">        res = np.array(line.strip().split(&apos; &apos;)).reshape((-1, 2))</span><br><span class="line">        # print(res)</span><br><span class="line">    x = []</span><br><span class="line">    y = []</span><br><span class="line">    for i, item in enumerate(res, 0):</span><br><span class="line">        item[1] = str(item[1]).replace(&apos;,&apos;, &apos;.&apos;)</span><br><span class="line">        # print(&apos;%d %.3f&apos; % (int(item[0]), float(item[1])))</span><br><span class="line">        x.append(int(item[0]))</span><br><span class="line">        y.append(float(item[1]))</span><br><span class="line">    return np.array(x), np.array(y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_ex1_multi_data():</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    加载多变量数据</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    path = &apos;../data/coursera2.txt&apos;</span><br><span class="line">    datas = []</span><br><span class="line">    with open(path, &apos;r&apos;) as f:</span><br><span class="line">        lines = f.readlines()</span><br><span class="line">        for line in lines:</span><br><span class="line">            datas.append(line.strip().split(&apos;,&apos;))</span><br><span class="line">    data_arr = np.array(datas)</span><br><span class="line">    data_arr = data_arr.astype(np.float)</span><br><span class="line"></span><br><span class="line">    X = data_arr[:, :2]</span><br><span class="line">    Y = data_arr[:, 2]</span><br><span class="line">    return X, Y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def least_square_loss_v1(x, y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    最小二乘法，几何运算</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    X = np.array(x)</span><br><span class="line">    Y = np.array(y)</span><br><span class="line">    muX = np.mean(X)</span><br><span class="line">    muY = np.mean(Y)</span><br><span class="line">    muXY = np.mean(X * Y)</span><br><span class="line">    muXX = np.mean(X * X)</span><br><span class="line"></span><br><span class="line">    w1 = (muXY - muX * muY) / (muXX - muX ** 2)</span><br><span class="line">    w0 = muY - w1 * muX</span><br><span class="line">    return w0, w1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def least_square_loss_v2(x, y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    最小二乘法，矩阵运算</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    extend_x = np.insert(x, 0, values=np.ones(x.shape[0]), axis=1)</span><br><span class="line">    w = np.linalg.inv(extend_x.T.dot(extend_x)).dot(extend_x.T).dot(y)</span><br><span class="line">    return w</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_single_variable_linear_regression():</span><br><span class="line">    x, y = load_sweden_data()</span><br><span class="line">    w0, w1 = least_square_loss_v1(x, y)</span><br><span class="line"></span><br><span class="line">    y2 = w1 * x + w0</span><br><span class="line"></span><br><span class="line">    plt.scatter(x, y)</span><br><span class="line">    plt.plot(x, y2)</span><br><span class="line"></span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_multi_variable_linear_regression():</span><br><span class="line">    x, y = load_ex1_multi_data()</span><br><span class="line">    # 计算权重</span><br><span class="line">    w = least_square_loss_v2(x, y)</span><br><span class="line">    print(w)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    # compute_single_variable_linear_regression()</span><br><span class="line">    compute_multi_variable_linear_regression()</span><br></pre></td></tr></table></figure><p><img src="/imgs/线性回归/sweden.png" alt></p><h3 id="适用范围"><a href="#适用范围" class="headerlink" title="适用范围"></a>适用范围</h3><p>参考：</p><p><a href="https://zhuanlan.zhihu.com/p/38128785" target="_blank" rel="noopener">最小二乘法（least sqaure method）</a></p><p><a href="https://www.zhihu.com/question/24095027" target="_blank" rel="noopener">在进行线性回归时，为什么最小二乘法是最优方法？</a></p><p>最小二乘法直接进行计算就能求出解，操作简洁，最适用于计算单变量线性回归问题</p><p>而对于多变量线性回归问题，使用最小二乘法计算需要考虑计算效率，因为$X^T\cdot X$的逆矩阵计算代价很大，同时需要考虑可逆问题，所以更推荐梯度下降算法来解决多变量线性回归问题</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本文学习了线性回归模型，利用最小二乘法（<em>最小化误差的平方和</em>）实现单边量/多变量线性数据的训练和预测</p><p>在训练过程中，线性回归模型使用线性映射进行前向计算，利用均方误差方法进行损失值的计算</p><ul><li>对于单变量线性回归问题，适用于最小二乘法的几何计算</li><li>对于多变量线性回归问题，如果变量维数不大同时满足$\left | X^{T}\cdot X \right |\neq 0$且$R(X) = n+1$的情况，使用最小二乘法的矩阵计算；否则，利用梯度下降方式进行权重更新</li></ul><p>线性回归模型更适用于回归问题，可以使用逻辑回归模型进行分类</p></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.jpg" alt="zhujian 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="zhujian 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zhujian</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zhujian.tech/posts/ec419bd2.html" title="线性回归">https://www.zhujian.tech/posts/ec419bd2.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/python/" rel="tag"># python</a> <a href="/tags/numpy/" rel="tag"># numpy</a> <a href="/tags/matplotlib/" rel="tag"># matplotlib</a> <a href="/tags/linear-regression/" rel="tag"># 线性回归</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/adb6e880.html" rel="next" title="[数据集]cifar-100"><i class="fa fa-chevron-left"></i> [数据集]cifar-100</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/posts/dea583b1.html" rel="prev" title="特征缩放">特征缩放<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#回归和分类"><span class="nav-number">1.</span> <span class="nav-text">回归和分类</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#线性回归"><span class="nav-number">2.</span> <span class="nav-text">线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#单变量线性回归"><span class="nav-number">2.1.</span> <span class="nav-text">单变量线性回归</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#多变量线性回归"><span class="nav-number">2.2.</span> <span class="nav-text">多变量线性回归</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#最小二乘法"><span class="nav-number">3.</span> <span class="nav-text">最小二乘法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#几何计算"><span class="nav-number">3.1.</span> <span class="nav-text">几何计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#矩阵计算"><span class="nav-number">3.2.</span> <span class="nav-text">矩阵计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#示例"><span class="nav-number">3.3.</span> <span class="nav-text">示例</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#适用范围"><span class="nav-number">3.4.</span> <span class="nav-text">适用范围</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#小结"><span class="nav-number">4.</span> <span class="nav-text">小结</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zhujian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">zhujian</p><div class="site-description" itemprop="description">one bite at a time</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">169</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">129</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/./atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zjZSTU" title="Github &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;zjZSTU" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/u012005313" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;u012005313" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i> CSDN</a></span><span class="links-of-author-item"><a href="/mailto:zjzstu@gmail.com" title="Gmail &amp;rarr; mailto:zjzstu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> Gmail</a></span><span class="links-of-author-item"><a href="/mailto:505169307@gmail.com" title="QQmail &amp;rarr; mailto:505169307@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> QQmail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://cloud.tencent.com/" title="https:&#x2F;&#x2F;cloud.tencent.com" rel="noopener" target="_blank">腾讯云</a></li><li class="links-of-blogroll-item"> <a href="https://www.aliyun.com/" title="https:&#x2F;&#x2F;www.aliyun.com" rel="noopener" target="_blank">阿里云</a></li><li class="links-of-blogroll-item"> <a href="https://developer.android.com/" title="https:&#x2F;&#x2F;developer.android.com&#x2F;" rel="noopener" target="_blank">Android Dev</a></li><li class="links-of-blogroll-item"> <a href="https://jenkins.io/" title="https:&#x2F;&#x2F;jenkins.io&#x2F;" rel="noopener" target="_blank">Jenkins</a></li><li class="links-of-blogroll-item"> <a href="http://cs231n.github.io/" title="http:&#x2F;&#x2F;cs231n.github.io&#x2F;" rel="noopener" target="_blank">cs231n</a></li><li class="links-of-blogroll-item"> <a href="https://pytorch.org/docs/stable/index.html" title="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;index.html" rel="noopener" target="_blank">pytorch</a></li><li class="links-of-blogroll-item"> <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" title="https:&#x2F;&#x2F;docs.docker.com&#x2F;install&#x2F;linux&#x2F;docker-ce&#x2F;ubuntu&#x2F;" rel="noopener" target="_blank">docker</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">zhujian</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">948k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">26:20</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-bar-chart-o"></i></span> <a href="https://tongji.baidu.com/web/27249108/overview/index?siteId=13647183" rel="noopener" target="_blank">百度统计</a></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div><div class="beian"> <a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备 19026415号</a> <span class="post-meta-divider">|</span> <img src="/images/beian_icon.png" style="display:inline-block;text-decoration:none;height:13px"> <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001748" rel="noopener" target="_blank">浙公网安备 33011802001748号</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span> <span class="site-uv" title="总访客量">访客数：<span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="site-pv" title="总访问量">阅读量：<span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="17,63,61" opacity="1" zindex="-1" count="199" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '8f22595c6dd29c94467d',
      clientSecret: 'e66bd90ebb9aa66c562c753dec6c90ceecbd51f2',
      repo: 'guestbook',
      owner: 'zjZSTU',
      admin: ['zjZSTU'],
      id: '6cb485816fdaf304d0be0f746fb1225d',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script></body></html>