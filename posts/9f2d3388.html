<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 4.0.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="alternate" href="/./atom.xml" title="做一个幸福的人" type="application/atom+xml"><meta name="google-site-verification" content="Qr_3yqLtyErDFKHW7mE8PJz4qDUX-bf_fMLpSRckQe4"><meta name="baidu-site-verification" content="zOwIvKMV7f"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="https://fonts.loli.net/css?family=Roboto:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="/lib/pace/pace-theme-center-atom.min.css"><script src="/lib/pace/pace.min.js"></script><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={root:"/",scheme:"Gemini",version:"7.5.0",exturl:!1,sidebar:{position:"left",display:"post",offset:12,onmobile:!1},copycode:{enable:!0,show_result:!0,style:null},back2top:{enable:!0,sidebar:!0,scrollpercent:!0},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!0,mediumzoom:!1,lazyload:!1,pangu:!1,algolia:{appID:"",apiKey:"",indexName:"",hits:{per_page:10},labels:{input_placeholder:"搜索文章",hits_empty:"我们没有找到任何搜索结果: ${query}",hits_stats:"找到约${hits}条结果（用时${time}ms）"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},path:"search.xml",motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},translation:{copy_button:"复制",copy_success:"复制成功",copy_failure:"复制失败"},sidebarPadding:40}</script><link href="https://fonts.googleapis.com/css?family=Noto+Serif+SC|Roboto&display=swap" rel="stylesheet"><meta name="description" content="参考：《机器学习基础 原理、算法与实践》第3章Basic Logistic Regression With NumPy"><meta name="keywords" content="python,微积分,逻辑回归"><meta property="og:type" content="article"><meta property="og:title" content="逻辑回归"><meta property="og:url" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;posts&#x2F;9f2d3388.html"><meta property="og:site_name" content="做一个幸福的人"><meta property="og:description" content="参考：《机器学习基础 原理、算法与实践》第3章Basic Logistic Regression With NumPy"><meta property="og:locale" content="zh-CN"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;逻辑回归&#x2F;sigmoid.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;逻辑回归&#x2F;cost.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;逻辑回归&#x2F;lr_batch_loss.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;逻辑回归&#x2F;lr_batch_accu.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;逻辑回归&#x2F;lr_stochastic_loss.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;逻辑回归&#x2F;lr_stochastic_accu.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;逻辑回归&#x2F;lr_small_loss.png"><meta property="og:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;逻辑回归&#x2F;lr_small_accu.png"><meta property="og:updated_time" content="2020-02-15T05:36:35.879Z"><meta name="twitter:card" content="summary"><meta name="twitter:image" content="https:&#x2F;&#x2F;www.zhujian.tech&#x2F;imgs&#x2F;逻辑回归&#x2F;sigmoid.png"><link rel="canonical" href="https://www.zhujian.tech/posts/9f2d3388.html"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,isPage:!1,isArchive:!1}</script><title>逻辑回归 | 做一个幸福的人</title><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="https://hm.baidu.com/hm.js?e677aac1ac69b8826b9cfecb4e72e107";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-meta"><div><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span> <span class="site-title">做一个幸福的人</span><span class="logo-line-after"><i></i></span></a></div><p class="site-subtitle">面朝大海，春暖花开</p></div><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div></div></div><nav class="site-nav"><ul id="menu" class="menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-fw fa-home"></i> 首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i> 关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i> 标签<span class="badge">129</span></a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i> 分类<span class="badge">48</span></a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i> 归档<span class="badge">169</span></a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i> 搜索</a></li></ul></nav><div class="site-search"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"> <input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="搜索..." spellcheck="false" type="text" id="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"></div></div><div class="search-pop-overlay"></div></div></div></header> <a href="https://github.com/zjZSTU" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0 0 115 115 130 115 142 142 250 250 250 0Z"></path><path d="M128.3 109C113.8 99.7 119 89.6 119 89.6 122 82.7 120.5 78.6 120.5 78.6 119.2 72 123.4 76.3 123.4 76.3 127.3 80.9 125.5 87.3 125.5 87.3 122.9 97.6 130.6 101.9 134.4 103.2" fill="currentColor" style="transform-origin:130px 106px" class="octo-arm"></path><path d="M115 115C114.9 115.1 118.7 116.5 119.8 115.4L133.7 101.6C136.9 99.2 139.9 98.4 142.2 98.6 133.8 88 127.5 74.4 143.8 58 148.5 53.4 154 51.2 159.7 51 160.3 49.4 163.2 43.6 171.4 40.1 171.4 40.1 176.1 42.5 178.8 56.2 183.1 58.6 187.2 61.8 190.9 65.4 194.5 69 197.7 73.2 200.1 77.6 213.8 80.2 216.3 84.9 216.3 84.9 212.7 93.1 206.9 96 205.4 96.6 205.1 102.4 203 107.8 198.3 112.5 181.9 128.9 168.3 122.5 157.7 114.1 157.9 116.9 156.7 120.9 152.7 124.9L141 136.5C139.8 137.7 141.6 141.9 141.8 141.8Z" fill="currentColor" class="octo-body"></path></svg></a><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content"><div class="posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="https://www.zhujian.tech/posts/9f2d3388.html"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.png"><meta itemprop="name" content="zhujian"><meta itemprop="description" content="one bite at a time"></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="做一个幸福的人"></span><header class="post-header"><h1 class="post-title" itemprop="name headline"> 逻辑回归</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-o"></i></span> <span class="post-meta-item-text">发表于</span> <time title="创建时间：2019-04-17 14:06:17" itemprop="dateCreated datePublished" datetime="2019-04-17T14:06:17+00:00">2019-04-17</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-calendar-check-o"></i></span> <span class="post-meta-item-text">更新于</span> <time title="修改时间：2020-02-15 05:36:35" itemprop="dateModified" datetime="2020-02-15T05:36:35+00:00">2020-02-15</time></span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="fa fa-folder-o"></i></span> <span class="post-meta-item-text">分类于</span> <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/" itemprop="url" rel="index"><span itemprop="name">算法</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/programming/" itemprop="url" rel="index"><span itemprop="name">编程</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/math/" itemprop="url" rel="index"><span itemprop="name">数学</span></a></span> ， <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/algorithm/data-learning/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a></span></span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="post-meta-item-text">阅读次数：</span><span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="fa fa-file-word-o"></i></span> <span class="post-meta-item-text">本文字数：</span> <span>9.8k</span></span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="fa fa-clock-o"></i></span> <span class="post-meta-item-text">阅读时长 &asymp;</span> <span>16 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><p>参考：</p><p>《机器学习基础 原理、算法与实践》第3章</p><p><a href="https://www.kaggle.com/emilyhorsman/basic-logistic-regression-with-numpy" target="_blank" rel="noopener">Basic Logistic Regression With NumPy</a></p><a id="more"></a><p><a href="https://www.kaggle.com/mtax687/logistic-regression-using-numpy" target="_blank" rel="noopener">Logistic Regression using Numpy</a></p><p>逻辑回归（<code>logistic regression</code>）是分类算法，常用于二元分类</p><h2 id="基本求导公式"><a href="#基本求导公式" class="headerlink" title="基本求导公式"></a>基本求导公式</h2><p>参考：<a href="https://baike.baidu.com/item/%E5%AF%BC%E6%95%B0%E8%A1%A8/10889755" target="_blank" rel="noopener">导数表</a></p><p>对数函数求导</p><script type="math/tex;mode=display">
y=log_{a}^{x} \Rightarrow {y}'=\frac{1}{xln(a)} \Rightarrow {ln(x)}'=\frac{1}{x}</script><p>幂函数求导</p><script type="math/tex;mode=display">
y=\frac{1}{x^{n}} \Rightarrow {y}'=-\frac{n}{x^{n+1}}</script><p>指数函数求导</p><script type="math/tex;mode=display">
y=n^x \Rightarrow {y}'=n^x \cdot ln(n) \Rightarrow {(e^x)}'=e^x</script><h2 id="sigmoid函数"><a href="#sigmoid函数" class="headerlink" title="sigmoid函数"></a>sigmoid函数</h2><p><code>sigmoid</code>函数简称为<code>S</code>型函数，也称为<code>logistic</code>函数，公式如下：</p><script type="math/tex;mode=display">
g(z)=\frac{1}{1+e^{-z}}</script><p><em>$e^{-z}$常写为$exp(-z)$</em>，求导如下</p><script type="math/tex;mode=display">
\frac{\varphi }{\varphi z}g(z)=\frac{-1}{(1+e^{-z})^2}\cdot {(e^{-z})}' = \frac{-e^{-z}}{(1+e^{-z})^2}\cdot {(-z)}' =  \frac{e^{-z}}{(1+e^{-z})^2}</script><script type="math/tex;mode=display">
=\frac{1}{1+e^{-z}}\cdot \frac{e^{-z}}{1+e^{-z}}
=\frac{1}{1+e^{-z}}\cdot (1-\frac{1}{1+e^{-z}})
=g(z)\cdot (1-g(z))</script><p>其实现特性如下：</p><ul><li>当输入值大于<code>0</code>时，输出趋近于<code>1</code></li><li>当输入值小于<code>0</code>时，输出趋近于<code>-1</code></li><li>当输入值等于<code>0</code>时，输出为<code>0.5</code></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def sigmoid(x):</span><br><span class="line">    return 1 / (1 + np.exp(-1 * x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def draw_sigmoid():</span><br><span class="line">    x = np.linspace(-10, 10)</span><br><span class="line">    y = sigmoid(np.array(x))</span><br><span class="line"></span><br><span class="line">    plt.plot(x, y)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    draw_sigmoid()</span><br></pre></td></tr></table></figure><p><img src="/imgs/逻辑回归/sigmoid.png" alt></p><h2 id="负对数似然代价函数"><a href="#负对数似然代价函数" class="headerlink" title="负对数似然代价函数"></a>负对数似然代价函数</h2><p>负对数似然代价函数计算公式如下</p><script type="math/tex;mode=display">
cost(h(x;\theta),y)=\left\{\begin{matrix}
-ln(h(x;\theta)),y=1
\\ 
-ln(1-h(x;\theta)),y=0
\end{matrix}\right.</script><p>分两种情况</p><ol><li>判定计算结果是否为<code>1</code>。当计算结果为<code>1</code>时，代价为<code>0</code>，否则代价随$h(x;\theta)$减少而增大</li><li>判定计算结果是否为<code>0</code>。当计算结果为<code>0</code>时，代价为<code>0</code>，否则代价随$h(x;\theta)$增大而增大</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">def cost(x):</span><br><span class="line">    y1 = -1 * np.log(x)</span><br><span class="line">    y2 = -1 * np.log(1 - x)</span><br><span class="line">    return y1, y2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def draw_cost():</span><br><span class="line">    x = np.linspace(0, 1)</span><br><span class="line">    y1, y2 = cost(x)</span><br><span class="line"></span><br><span class="line">    plt.plot(x, y1)</span><br><span class="line">    plt.plot(x, y2)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    draw_cost()</span><br></pre></td></tr></table></figure><p><img src="/imgs/逻辑回归/cost.png" alt></p><h2 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h2><p>在二元分类中，结果<code>y</code>取值为<code>0</code>或<code>1</code>，将负对数似然代价函数的两种情况合并在一起得到交叉熵损失函数（<code>cross entropy loss function</code>）</p><script type="math/tex;mode=display">
loss(h(x;\theta),y)=-yln(h(x;\theta))-(1-y)ln(1-h(x;\theta))</script><h2 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h2><p>逻辑回归的实现就是线性回归加上<code>sigmoid</code>操作，其线性操作如下：</p><script type="math/tex;mode=display">
z_{\theta}(x)=\theta^T\cdot x=\theta_{0}\cdot x_{0}+\theta_{1}\cdot x_{1}+...+\theta_{n}\cdot x_{n}</script><p>逻辑回归模型实现公式如下：</p><script type="math/tex;mode=display">
h(x;\theta)=g(z_{\theta}(x))=g(\theta^T\cdot x)=\frac{1}{1+e^{-\theta^T\cdot x}}</script><p>对逻辑回归模型求导如下：</p><script type="math/tex;mode=display">
\frac{\varphi }{\varphi \theta_{i}}h(x;\theta)={h(\theta^T\cdot x)}'={g(\theta^T\cdot x)}'
=g(\theta^T\cdot x)\cdot (1-g(\theta^T\cdot x))\cdot {(\theta^T\cdot x)}'</script><script type="math/tex;mode=display">
=g(\theta^T\cdot x)\cdot (1-g(\theta^T\cdot x))\cdot x_{i}
=h(x;\theta)\cdot (1-h(x;\theta))\cdot x_{i}</script><p>逻辑回归利用<code>sigmoid</code>函数进行二元分类，首先对输入数据进行线性运算$\theta^T\cdot x$，再将结果输入<code>sigmoid</code>函数，压缩到$[0,1]$范围内，输出结果作为判别概率，表示输出结果为1的可能性，即$h(x;\theta)=P(y=1|x;\theta)$，相对应的输出结果为0的概率为$P(y=0|x;\theta)=1-h(x;\theta)$</p><p>逻辑回归利用交叉熵损失函数作为二元分类损失函数，公式如下：</p><script type="math/tex;mode=display">
J(\theta)=-\frac{1}{N}
\sum_{j=1}^{N}
\begin{bmatrix}
 y_{j}ln(h(x_{j};\theta))+(1-y_{j})ln(1-h(x_{j};\theta))
\end{bmatrix}</script><p>矩阵运算如下：</p><script type="math/tex;mode=display">
\Rightarrow 
J(\theta)=
-\frac{1}{N} (Y^T\cdot ln(g(X\cdot \theta))+(1-Y^T)\cdot ln(1-g(X\cdot \theta))</script><p>其中$X$大小为$m\times (n+1)$，$\theta$大小为$(n+1)\times 1$，$Y$大小为$m\times 1$，$m$表示样本数量，$n$表示权重数量</p><h2 id="测试数据"><a href="#测试数据" class="headerlink" title="测试数据"></a>测试数据</h2><p>使用<code>numeric</code>类型的德国信用数据，其包含<code>24</code>个变量和一个<code>2</code>类标签 - <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/" target="_blank" rel="noopener">german.data-numeric</a></p><h2 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h2><p>参考：<a href="https://www.cnblogs.com/zhongmiaozhimen/p/6155093.html" target="_blank" rel="noopener">第三周：逻辑回归代价函数求导过程</a></p><p>使用批量训练数据进行梯度计算，对损失函数求导如下：</p><script type="math/tex;mode=display">
\frac{\varphi }{\varphi \theta_{i}}J(\theta)=
-\frac{1}{N}
\sum_{j=1}^{N}
\begin{bmatrix}
 y_{j}ln(h(x_{j};\theta))+(1-y_{j})ln(1-h(x_{j};\theta))
\end{bmatrix}'</script><script type="math/tex;mode=display">
=-\frac{1}{N}
\sum_{j=1}^{N}
\begin{bmatrix}
 y_{j}\frac{1}{h(x_{j};\theta)}\cdot {h(\theta^T\cdot x)}'+(1-y_{j})\frac{1}{1-h(x_{j};\theta)}\cdot {(1-h(\theta^T\cdot x))}'
\end{bmatrix}</script><script type="math/tex;mode=display">
=-\frac{1}{N}
\sum_{j=1}^{N}
\begin{bmatrix}
 y_{j}\frac{1}{h(x_{j};\theta)}\cdot h(x_{j};\theta)\cdot (1-h(x_{j};\theta))\cdot x_{j,i}+(1-y_{j})\frac{1}{1-h(x_{j};\theta)}\cdot -h(x_{j};\theta)\cdot (1-h(x_{j};\theta))\cdot x_{j,i}
 \end{bmatrix}</script><script type="math/tex;mode=display">
=-\frac{1}{N}
\sum_{j=1}^{N}
\begin{bmatrix}
 y_{j}\cdot (1-h(x_{j};\theta))\cdot x_{j,i} - (1-y_{j})\cdot h(x_{j};\theta)\cdot x_{j,i}
 \end{bmatrix}</script><script type="math/tex;mode=display">
=-\frac{1}{N}
\sum_{j=1}^{N}
\begin{bmatrix}
 y_{j}\cdot x_{j,i} -  h(x_{j};\theta)\cdot x_{j,i}
 \end{bmatrix}
=\frac{1}{N}
\sum_{j=1}^{N}
\begin{bmatrix}
 (h(x_{j};\theta)-y_{j})\cdot x_{j,i}
 \end{bmatrix}</script><p>其中$x_{j,i}$表示第$j$行第$i$列，矩阵运算如下：</p><script type="math/tex;mode=display">
\Rightarrow 
\frac{\varphi }{\varphi \theta}J(\theta)=
\frac{1}{N} X^T\cdot (g(X\cdot \theta)-Y)</script><p>其中$X$大小为$m\times (n+1)$，$\theta$大小为$(n+1)\times 1$，$Y$大小为$m\times 1$，$m$表示样本数量，$n$表示权重数量</p><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-</span><br><span class="line"></span><br><span class="line"># @Time    : 19-4-18 上午9:22</span><br><span class="line"># @Author  : zj</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import numpy as np</span><br><span class="line">import pandas as pd</span><br><span class="line">from sklearn.model_selection import train_test_split</span><br><span class="line"></span><br><span class="line">data_path = &apos;../data/german.data-numeric&apos;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def load_data(tsize=0.8, shuffle=True):</span><br><span class="line">    data_list = pd.read_csv(data_path, header=None, sep=&apos;\s+&apos;)</span><br><span class="line"></span><br><span class="line">    data_array = data_list.values</span><br><span class="line">    height, width = data_array.shape[:2]</span><br><span class="line">    data_x = data_array[:, :(width - 1)]</span><br><span class="line">    data_y = data_array[:, (width - 1)]</span><br><span class="line"></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(data_x, data_y, train_size=tsize, test_size=(1 - tsize),</span><br><span class="line">                                                        shuffle=shuffle)</span><br><span class="line"></span><br><span class="line">    y_train = np.atleast_2d(np.array(list(map(lambda x: 1 if x == 2 else 0, y_train)))).T</span><br><span class="line">    y_test = np.atleast_2d(np.array(list(map(lambda x: 1 if x == 2 else 0, y_test)))).T</span><br><span class="line"></span><br><span class="line">    return x_train, y_train, x_test, y_test</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def init_weights(inputs):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    初始化权重，符合标准正态分布</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    return np.atleast_2d(np.random.uniform(size=inputs)).T</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def sigmoid(x):</span><br><span class="line">    return 1 / (1 + np.exp(-1 * x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def logistic_regression(w, x):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    w大小为(n+1)x1</span><br><span class="line">    x大小为mx(n+1)</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    z = x.dot(w)</span><br><span class="line">    return sigmoid(z)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_loss(w, x, y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    w大小为(n+1)x1</span><br><span class="line">    x大小为mx(n+1)</span><br><span class="line">    y大小为mx1</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    lr_value = logistic_regression(w, x)</span><br><span class="line">    n = y.shape[0]</span><br><span class="line">    res = -1.0 / n * (y.T.dot(np.log(lr_value)) + (1 - y.T).dot(np.log(1 - lr_value)))</span><br><span class="line">    return res[0][0]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_gradient(w, x, y):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    梯度计算</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    n = y.shape[0]</span><br><span class="line">    lr_value = logistic_regression(w, x)</span><br><span class="line">    return 1.0 / n * x.T.dot(lr_value - y)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_predict_accuracy(predictions, y):</span><br><span class="line">    results = predictions &gt; 0.5</span><br><span class="line">    res = len(list(filter(lambda x: x[0] == x[1], np.dstack((results, y))[:, 0]))) / len(results)</span><br><span class="line">    return res</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def draw(res_list, title=None, xlabel=None, ylabel=None):</span><br><span class="line">    if title is not None:</span><br><span class="line">        plt.title(title)</span><br><span class="line">    if xlabel is not None:</span><br><span class="line">        plt.xlabel(xlabel)</span><br><span class="line">    plt.plot(res_list)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    # 加载训练和测试数据</span><br><span class="line">    # train_data, train_label, test_data, test_label = load_german_numeric(tsize=0.85, shuffle=False)</span><br><span class="line">    train_data, train_label, test_data, test_label = load_data()</span><br><span class="line"></span><br><span class="line">    # 根据训练数据计算均值和标准差</span><br><span class="line">    mu = np.mean(train_data, axis=0)</span><br><span class="line">    std = np.std(train_data, axis=0)</span><br><span class="line"></span><br><span class="line">    # 标准化训练和测试数据</span><br><span class="line">    train_data = (train_data - mu) / std</span><br><span class="line">    test_data = (test_data - mu) / std</span><br><span class="line"></span><br><span class="line">    # 添加偏置值</span><br><span class="line">    train_data = np.insert(train_data, 0, np.ones(train_data.shape[0]), axis=1)</span><br><span class="line">    test_data = np.insert(test_data, 0, np.ones(test_data.shape[0]), axis=1)</span><br><span class="line"></span><br><span class="line">    # 定义步长、权重和偏置值</span><br><span class="line">    lr = 0.01</span><br><span class="line">    w = init_weights(train_data.shape[1])</span><br><span class="line"></span><br><span class="line">    # 计算目标函数/损失函数以及梯度更新</span><br><span class="line">    epoches = 20000</span><br><span class="line"></span><br><span class="line">    loss_list = []</span><br><span class="line">    accuracy_list = []</span><br><span class="line">    loss = 0</span><br><span class="line">    best_accuracy = 0</span><br><span class="line">    best_w = None</span><br><span class="line">    for i in range(epoches):</span><br><span class="line">        loss += compute_loss(w, train_data, train_label)</span><br><span class="line">        # 计算梯度</span><br><span class="line">        gradient = compute_gradient(w, train_data, train_label)</span><br><span class="line">        # 权重更新</span><br><span class="line">        tempW = w - lr * gradient</span><br><span class="line">        w = tempW</span><br><span class="line"></span><br><span class="line">        if i % 50 == 49:</span><br><span class="line">            # 每个50次记录一次</span><br><span class="line">            # 计算精度</span><br><span class="line">            accuracy = compute_predict_accuracy(logistic_regression(w, train_data), train_label)</span><br><span class="line">            accuracy_list.append(accuracy)</span><br><span class="line"></span><br><span class="line">            if accuracy &gt; best_accuracy:</span><br><span class="line">                best_accuracy = accuracy</span><br><span class="line">                best_w = w.copy()</span><br><span class="line">            # 计算损失</span><br><span class="line">            loss_list.append(loss / 50)</span><br><span class="line">            loss = 0</span><br><span class="line"></span><br><span class="line">    draw(loss_list, title=&apos;损失值/50&apos;)</span><br><span class="line">    draw(accuracy_list, title=&apos;训练集检测精度/50&apos;)</span><br><span class="line">    print(max(accuracy_list))</span><br><span class="line">    print(compute_predict_accuracy(logistic_regression(best_w, test_data), test_label))</span><br></pre></td></tr></table></figure><p><img src="/imgs/逻辑回归/lr_batch_loss.png" alt></p><p><img src="/imgs/逻辑回归/lr_batch_accu.png" alt></p><p>随机梯度下降实现如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">import warnings</span><br><span class="line"></span><br><span class="line">warnings.filterwarnings(&apos;ignore&apos;)</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">def compute_loss(w, x, y, isBatch=True):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    w大小为(n+1)x1</span><br><span class="line">    x大小为mx(n+1)</span><br><span class="line">    y大小为mx1</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    lr_value = logistic_regression(w, x)</span><br><span class="line">    if isBatch:</span><br><span class="line">        n = y.shape[0]</span><br><span class="line">        res = -1.0 / n * (y.T.dot(np.log(lr_value)) + (1 - y.T).dot(np.log(1 - lr_value)))</span><br><span class="line">        return res[0][0]</span><br><span class="line">    else:</span><br><span class="line">        res = -1.0 * (y * (np.log(lr_value)) + (1 - y) * (np.log(1 - lr_value)))</span><br><span class="line">        return res[0]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def compute_gradient(w, x, y, isBatch=True):</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    梯度计算</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    lr_value = logistic_regression(w, x)</span><br><span class="line">    if isBatch:</span><br><span class="line">        n = y.shape[0]</span><br><span class="line">        return 1.0 / n * x.T.dot(lr_value - y)</span><br><span class="line">    else:</span><br><span class="line">        return np.atleast_2d(1.0 * x.T * (lr_value - y)).T</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    ...</span><br><span class="line">    ...</span><br><span class="line">    # 计算目标函数/损失函数以及梯度更新</span><br><span class="line">    epoches = 20</span><br><span class="line">    num = train_label.shape[0]</span><br><span class="line"></span><br><span class="line">    loss_list = []</span><br><span class="line">    accuracy_list = []</span><br><span class="line">    loss = 0</span><br><span class="line">    best_accuracy = 0</span><br><span class="line">    best_w = None</span><br><span class="line">    for i in range(epoches):</span><br><span class="line">        for j in range(num):</span><br><span class="line">            loss += compute_loss(w, train_data[j], train_label[j], isBatch=False)</span><br><span class="line">            # 计算梯度</span><br><span class="line">            gradient = compute_gradient(w, train_data[j], train_label[j], isBatch=False)</span><br><span class="line">            # 权重更新</span><br><span class="line">            tempW = w - lr * gradient</span><br><span class="line">            w = tempW</span><br><span class="line"></span><br><span class="line">            if j % 50 == 49:</span><br><span class="line">                # 每个50次记录一次</span><br><span class="line">                # 计算精度</span><br><span class="line">                accuracy = compute_predict_accuracy(logistic_regression(w, train_data), train_label)</span><br><span class="line">                accuracy_list.append(accuracy)</span><br><span class="line"></span><br><span class="line">                if accuracy &gt; best_accuracy:</span><br><span class="line">                    best_accuracy = accuracy</span><br><span class="line">                    best_w = w.copy()</span><br><span class="line">                # 计算损失</span><br><span class="line">                loss_list.append(loss / 50)</span><br><span class="line">                loss = 0</span><br><span class="line"></span><br><span class="line">    draw(loss_list, title=&apos;损失值/50&apos;)</span><br><span class="line">    draw(accuracy_list, title=&apos;训练集检测精度/50&apos;)</span><br><span class="line">    print(max(accuracy_list))</span><br><span class="line">    print(compute_predict_accuracy(logistic_regression(best_w, test_data), test_label))</span><br></pre></td></tr></table></figure><p><img src="/imgs/逻辑回归/lr_stochastic_loss.png" alt></p><p><img src="/imgs/逻辑回归/lr_stochastic_accu.png" alt></p><p>小批量梯度下降结果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"># 计算目标函数/损失函数以及梯度更新</span><br><span class="line">epoches = 200</span><br><span class="line">batch_size = 128</span><br><span class="line">num = train_label.shape[0]</span><br><span class="line"></span><br><span class="line">loss_list = []</span><br><span class="line">accuracy_list = []</span><br><span class="line">loss = 0</span><br><span class="line">best_accuracy = 0</span><br><span class="line">best_w = None</span><br><span class="line">for i in range(epoches):</span><br><span class="line">    for j in range(0, num, 16):</span><br><span class="line">        loss_list.append(compute_loss(w, train_data[j:j + batch_size], train_label[j:j + batch_size], isBatch=True))</span><br><span class="line">        # 计算梯度</span><br><span class="line">        gradient = compute_gradient(w, train_data[j:j + batch_size], train_label[j:j + batch_size], isBatch=True)</span><br><span class="line">        # 权重更新</span><br><span class="line">        tempW = w - lr * gradient</span><br><span class="line">        w = tempW</span><br><span class="line"></span><br><span class="line">        # 每个小批次记录一次</span><br><span class="line">        # 计算精度</span><br><span class="line">        accuracy = compute_predict_accuracy(logistic_regression(w, train_data), train_label)</span><br><span class="line">        accuracy_list.append(accuracy)</span><br><span class="line"></span><br><span class="line">        if accuracy &gt; best_accuracy:</span><br><span class="line">            best_accuracy = accuracy</span><br><span class="line">            best_w = w.copy()</span><br><span class="line"></span><br><span class="line">draw(loss_list, title=&apos;损失值&apos;)</span><br><span class="line">draw(accuracy_list, title=&apos;训练集检测精度&apos;)</span><br><span class="line">print(max(accuracy_list))</span><br><span class="line">print(compute_predict_accuracy(logistic_regression(best_w, test_data), test_label))</span><br></pre></td></tr></table></figure><p><img src="/imgs/逻辑回归/lr_small_loss.png" alt></p><p><img src="/imgs/逻辑回归/lr_small_accu.png" alt></p><h3 id="RuntimeWarning-divide-by-zero-encountered-in-log"><a href="#RuntimeWarning-divide-by-zero-encountered-in-log" class="headerlink" title="RuntimeWarning: divide by zero encountered in log"></a>RuntimeWarning: divide by zero encountered in log</h3><p>计算损失过程中可能会出现精度错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_loss = -1.0 / num_train * np.sum(y_batch * np.log(scores) + (1 - y_batch) * np.log(1 - scores))</span><br></pre></td></tr></table></figure><p>修改如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">eplison = 1e-5</span><br><span class="line"></span><br><span class="line">scores = self.logistic_regression(X_batch)</span><br><span class="line">data_loss = -1.0 / num_train * np.sum(y_batch * np.log(np.maximum(scores, eplison)) + (1 - y_batch) * np.log(np.maximum(1 - scores, eplison)))</span><br></pre></td></tr></table></figure><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>逻辑回归二元分类需要注意：</p><ol><li>标签值的转换（将两类标签转换成<code>0/1</code>数值）</li><li>预测值的计算（计算单个预测值就能判断类别）</li></ol></div><div class="reward-container"><div>坚持原创技术分享，您的支持将鼓励我继续创作！</div> <button disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';"> 打赏</button><div id="qr" style="display:none"><div style="display:inline-block"> <img src="/images/wechatpay.jpg" alt="zhujian 微信支付"><p>微信支付</p></div><div style="display:inline-block"> <img src="/images/alipay.jpg" alt="zhujian 支付宝"><p>支付宝</p></div></div></div><div><ul class="post-copyright"><li class="post-copyright-author"> <strong>本文作者：</strong> zhujian</li><li class="post-copyright-link"> <strong>本文链接：</strong> <a href="https://www.zhujian.tech/posts/9f2d3388.html" title="逻辑回归">https://www.zhujian.tech/posts/9f2d3388.html</a></li><li class="post-copyright-license"> <strong>版权声明：</strong> 本博客所有文章除特别声明外，均采用<a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i> BY-NC-SA</a> 许可协议。转载请注明出处！</li></ul></div><footer class="post-footer"><div class="post-tags"> <a href="/tags/python/" rel="tag"># python</a> <a href="/tags/calculus/" rel="tag"># 微积分</a> <a href="/tags/logistic-regression/" rel="tag"># 逻辑回归</a></div><div class="post-nav"><div class="post-nav-next post-nav-item"><a href="/posts/ca2079f0.html" rel="next" title="从numpy到pytorch实现线性回归"><i class="fa fa-chevron-left"></i> 从numpy到pytorch实现线性回归</a></div><span class="post-nav-divider"></span><div class="post-nav-prev post-nav-item"> <a href="/posts/730913b9.html" rel="prev" title="从numpy到pytorch实现逻辑回归">从numpy到pytorch实现逻辑回归<i class="fa fa-chevron-right"></i></a></div></div></footer></article></div></div><div class="comments" id="gitalk-container"></div></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span><span class="toggle-line toggle-line-middle"></span><span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc"> 文章目录</li><li class="sidebar-nav-overview"> 站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#基本求导公式"><span class="nav-number">1.</span> <span class="nav-text">基本求导公式</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#sigmoid函数"><span class="nav-number">2.</span> <span class="nav-text">sigmoid函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#负对数似然代价函数"><span class="nav-number">3.</span> <span class="nav-text">负对数似然代价函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#交叉熵损失函数"><span class="nav-number">4.</span> <span class="nav-text">交叉熵损失函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#逻辑回归"><span class="nav-number">5.</span> <span class="nav-text">逻辑回归</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#测试数据"><span class="nav-number">6.</span> <span class="nav-text">测试数据</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#梯度下降"><span class="nav-number">7.</span> <span class="nav-text">梯度下降</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#实现"><span class="nav-number">7.1.</span> <span class="nav-text">实现</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RuntimeWarning-divide-by-zero-encountered-in-log"><span class="nav-number">7.2.</span> <span class="nav-text">RuntimeWarning: divide by zero encountered in log</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#小结"><span class="nav-number">8.</span> <span class="nav-text">小结</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"> <img class="site-author-image" itemprop="image" alt="zhujian" src="/images/avatar.png"><p class="site-author-name" itemprop="name">zhujian</p><div class="site-description" itemprop="description">one bite at a time</div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"> <a href="/archives/"><span class="site-state-item-count">169</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"> <a href="/categories/"><span class="site-state-item-count">48</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"> <a href="/tags/"><span class="site-state-item-count">129</span> <span class="site-state-item-name">标签</span></a></div></nav></div><div class="feed-link motion-element"><a href="/./atom.xml" rel="alternate"><i class="fa fa-rss"></i> RSS</a></div><div class="links-of-author motion-element"><span class="links-of-author-item"><a href="https://github.com/zjZSTU" title="Github &amp;rarr; https:&#x2F;&#x2F;github.com&#x2F;zjZSTU" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i> Github</a></span><span class="links-of-author-item"><a href="https://blog.csdn.net/u012005313" title="CSDN &amp;rarr; https:&#x2F;&#x2F;blog.csdn.net&#x2F;u012005313" rel="noopener" target="_blank"><i class="fa fa-fw fa-book"></i> CSDN</a></span><span class="links-of-author-item"><a href="/mailto:zjzstu@gmail.com" title="Gmail &amp;rarr; mailto:zjzstu@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> Gmail</a></span><span class="links-of-author-item"><a href="/mailto:505169307@gmail.com" title="QQmail &amp;rarr; mailto:505169307@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i> QQmail</a></span></div><div class="cc-license motion-element" itemprop="license"> <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/null" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a></div><div class="links-of-blogroll motion-element"><div class="links-of-blogroll-title"><i class="fa fa-fw fa-link"></i> Links</div><ul class="links-of-blogroll-list"><li class="links-of-blogroll-item"> <a href="https://cloud.tencent.com/" title="https:&#x2F;&#x2F;cloud.tencent.com" rel="noopener" target="_blank">腾讯云</a></li><li class="links-of-blogroll-item"> <a href="https://www.aliyun.com/" title="https:&#x2F;&#x2F;www.aliyun.com" rel="noopener" target="_blank">阿里云</a></li><li class="links-of-blogroll-item"> <a href="https://developer.android.com/" title="https:&#x2F;&#x2F;developer.android.com&#x2F;" rel="noopener" target="_blank">Android Dev</a></li><li class="links-of-blogroll-item"> <a href="https://jenkins.io/" title="https:&#x2F;&#x2F;jenkins.io&#x2F;" rel="noopener" target="_blank">Jenkins</a></li><li class="links-of-blogroll-item"> <a href="http://cs231n.github.io/" title="http:&#x2F;&#x2F;cs231n.github.io&#x2F;" rel="noopener" target="_blank">cs231n</a></li><li class="links-of-blogroll-item"> <a href="https://pytorch.org/docs/stable/index.html" title="https:&#x2F;&#x2F;pytorch.org&#x2F;docs&#x2F;stable&#x2F;index.html" rel="noopener" target="_blank">pytorch</a></li><li class="links-of-blogroll-item"> <a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/" title="https:&#x2F;&#x2F;docs.docker.com&#x2F;install&#x2F;linux&#x2F;docker-ce&#x2F;ubuntu&#x2F;" rel="noopener" target="_blank">docker</a></li></ul></div></div><div class="back-to-top motion-element"><i class="fa fa-arrow-up"></i> <span>0%</span></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright"> &copy; <span itemprop="copyrightYear">2020</span><span class="with-love"><i class="fa fa-user"></i></span> <span class="author" itemprop="copyrightHolder">zhujian</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-area-chart"></i></span> <span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">948k</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-coffee"></i></span> <span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">26:20</span> <span class="post-meta-divider">|</span><span class="post-meta-item-icon"><i class="fa fa-bar-chart-o"></i></span> <a href="https://tongji.baidu.com/web/27249108/overview/index?siteId=13647183" rel="noopener" target="_blank">百度统计</a></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.0.0</div> <span class="post-meta-divider">|</span><div class="theme-info">主题 – <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> v7.5.0</div><div class="beian"> <a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">浙ICP备 19026415号</a> <span class="post-meta-divider">|</span> <img src="/images/beian_icon.png" style="display:inline-block;text-decoration:none;height:13px"> <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=33011802001748" rel="noopener" target="_blank">浙公网安备 33011802001748号</a></div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i></span> <span class="site-uv" title="总访客量">访客数：<span id="busuanzi_value_site_uv"></span></span></span> <span class="post-meta-divider">|</span><span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i></span> <span class="site-pv" title="总访问量">阅读量：<span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script color="17,63,61" opacity="1" zindex="-1" count="199" src="/lib/canvas-nest/canvas-nest.min.js"></script><script src="/lib/anime.min.js"></script><script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script><script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/pisces.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><script type="text/x-mathjax-config">

  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    },
    TeX: {
      equationNumbers: {
        autoNumber: 'AMS'
      }
    }
  });

  MathJax.Hub.Register.StartupHook('TeX Jax Ready', function() {
    MathJax.InputJax.TeX.prefilterHooks.Add(function(data) {
      if (data.display) {
        var next = data.script.nextSibling;
        while (next && next.nodeName.toLowerCase() === '#text') {
          next = next.nextSibling;
        }
        if (next && next.nodeName.toLowerCase() === 'br') {
          next.parentNode.removeChild(next);
        }
      }
    });
  });

  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i = 0; i < all.length; i += 1) {
      element = document.getElementById(all[i].inputID + '-Frame').parentNode;
      if (element.nodeName.toLowerCase() == 'li') {
        element = element.parentNode;
      }
      element.classList.add('has-jax');
    }
  });
</script><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML', () => {
    MathJax.Hub.Typeset();
  }, window.MathJax);
</script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID: '8f22595c6dd29c94467d',
      clientSecret: 'e66bd90ebb9aa66c562c753dec6c90ceecbd51f2',
      repo: 'guestbook',
      owner: 'zjZSTU',
      admin: ['zjZSTU'],
      id: '4c8f8fea896c0ebea5ddb4f8a6e10872',
        language: '',
      distractionFreeMode: 'true'
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
</script></body></html>