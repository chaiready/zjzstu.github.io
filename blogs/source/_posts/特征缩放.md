---
title: 特征缩放
categories:
  - 编程
tags:
  - 机器学习
  - 深度学习
abbrlink: dea583b1
date: 2019-04-10 13:49:35
---

参考：[数据标准化/归一化normalization](https://blog.csdn.net/pipisorry/article/details/52247379)

在多变量回归或分类问题上，需要保证这些变量的取值范围具有同一尺度

原因一：**确保大尺度变量不会左右分类器的分类结果**。如果分类器利用结果变量的距离来计算损失函数，那么小尺度变量的变化会被忽略，大尺度变量会决定分类效果
原因二：**帮助梯度下降算法收敛更快**。参考[机器学习--特征缩放/均值归一化](https://blog.csdn.net/runnerxin/article/details/78551025)，从损失函数等值线图可知，变量在同一尺度下能够更快的通过梯度下降算法收敛

常用的特征缩放方法包括标准化（或称为规范化）和区间缩放

### 标准化

标准化方法就是将数据变换为均值为`0`，方差为`1`的标准正态分布。标准化公式如下

$$
x_{i}'=\frac{x_{i}-\mu_{i}}{s_{i}}
$$

其中$x_{i}$是第$i$个属性的特征向量，$x_{i}'$是变换后的特征向量，$\mu_{i}$是第$i$个属性的均值，$s_{i}$是第$i$个属性的标准差

**要求：变量服从正态分布**

### 区间缩放

将特征值缩放到某个特定大小的区间，比如`[0,1]`，计算公式如下：

$$
x_{i}'=\frac{x_{i}-min(x_{i})}{max_{x_{i}}-min_{x_{i}}}
$$

其中$x_{i}$是第$i$个属性的特征向量，$x_{i}'$是变换后的特征向量，函数$max()$和$min()$用于求该属性的最大最小值