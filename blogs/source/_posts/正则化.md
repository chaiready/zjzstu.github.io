---
title: 正则化
categories:
  - [算法, 最优化]
  - [数学]
tags:
  - 正则化
  - 权重惩罚
  - 微积分
abbrlink: ce0afb50
date: 2019-04-22 15:35:43
---

[Setting up the data and the model](http://cs231n.github.io/neural-networks-2/#init)

[机器学习中常常提到的正则化到底是什么意思？](https://www.zhihu.com/question/20924039)

## 泰勒公式

参考：[泰勒公式](https://zh.wikipedia.org/wiki/%E6%B3%B0%E5%8B%92%E5%85%AC%E5%BC%8F)

设$n$是一个正整数，如果定义一个包含$a$的区间上的函数$f$在$a$点处$n+1$次可导，那么对于区间上的任意x，都有：

$$
f(x)=f(a)+\frac{f^{\prime}(a)}{1 !}(x-a)+\frac{f^{(2)}(a)}{2 !}(x-a)^{2}+\cdots+\frac{f^{(n)}(a)}{n !}(x-a)^{n}+R_{n}(x)
$$

其中的多项式称为函数在$a$点处的**泰勒展开式**，剩余的$R_{n}(x)$是泰勒展开式的余项，是$(x-a)^{n}$的高阶无穷小

## 范数

范数（`norm`）常被用于度量某个向量空间（或矩阵）中的每个向量的长度或大小。通用计算公式如下：

$$
\|x\|_{p}=\left(\sum_{i}\left|x_{i}\right|^{p}\right)^{1 / p}
$$

常用的范数包括`L0/L1/L2`范数

* 0-范数，计算向量中非零元素的个数
* 1-范数，又称为`L1`范数，计算向量中各元素绝对值之和

$$
\left \| X \right \| = \left | X \right | = \left | x_{1} \right |+\left |x_{2}  \right |+...+\left | x_{n} \right |
$$

* 2-范数，又称为`L2`范数，计算向量中各元素之间距离（欧式距离）之和

$$
\left \| X \right \|_{2} = \sqrt{x_{1}^{2}+x_{2}^{2}+...+x_{n}^{2}}
$$

## 正则化

在深度学习或机器学习中，一方面要提高算法泛化能力，另一方面要避免算法过拟合

**正则化（`regularization`）指的是最小化算法结构风险，其目的就是为了防止算法过拟合，提高泛化能力**

常用的方法包括

* 权重惩罚（`weight penalty`）
* 提前停止策略（`early stopping`）
* 随机失活（`dropout`）
* 最大值上限（`max-upper constraint`）

### 权重惩罚

参考：

[机器学习中的范数规则化之（一）L0、L1与L2范数](https://blog.csdn.net/zouxy09/article/details/24971995)

[L1 Norms versus L2 Norms](https://www.kaggle.com/residentmario/l1-norms-versus-l2-norms)

[l1正则与l2正则的特点是什么，各有什么优势？](https://www.zhihu.com/question/26485586)

根据泰勒公式可知，如果函数足够光滑的话，可以用泰勒展开式近似。对于非线性函数而言，可以替换成如下形式

$$
h(x;\theta) 
=w_{0}+w_{1}\cdot x+w_{2}\cdot x^{2}+...+w_{n}\cdot x^{n}
$$

参数$\theta=(w_{0},w_{1},...,w_{n})$，一次项是$w_{1}\cdot x$，$x$次数高于2的项统称为高次项

一方面算法增加高次项能够提高拟合能力；另一方面高次项的系数变化对整个算法的影响更大，过高的系数会导致算法过拟合

为了防止算法过拟合，需要在损失函数中加上正则化项，常用的有`L1/L2`正则化

`L1`正则化使用`L1`范数加上超参数$\lambda$作为正则化项，目标函数实现如下：

$$
obj(x;\theta) = loss(x;\theta)+\lambda \left \| \theta \right \|
=loss(x;\theta)+\lambda \left | \theta \right |
$$

求导如下：

$$
\frac{\varphi O}{\varphi w_{i}}=\frac{\varphi L}{\varphi w_{i}}+\lambda
$$

`L2`正则化使用`L2`范数（*通常去除开根号*）加上超参数$\lambda$作为正则化项，目标函数实现如下：

$$
obj(x;\theta) = loss(x;\theta)+\frac{1}{2} \lambda \left \| \theta \right \|_{2}
=loss(x;\theta)+\frac{1}{2} \lambda \theta^{2}
$$

求导如下

$$
\frac{\varphi O}{\varphi w_{i}}=\frac{\varphi L}{\varphi w_{i}}+\lambda w_{i}
$$

从导函数可知，使用`L1`范数作为正则化项会导致训练后的权重向量变得稀疏（`sparse`），即某些权重非常接近于`0`，因为每次梯度更新都会减去一个固定大小正则化梯度；使用`L2`范数作为正则化项会导致训练后的权重向量变得平均（`diffuse`），因为在梯度更新过程中，权重值越大，其减去的梯度也会更大

**使用`L1`范数更有利于特征选择，使用`L2`范数能够抑制大权值对网络的影响，充分利用所有权重向量。在实践过程中，通常使用`L2`范数作为正则化项**

*不需要惩罚$w_{0}$，因为它没有和输入数据进行乘法交互，不会放大拟合能力。在实际使用中，惩罚$w_{0}$几乎不会导致性能显著下降*

### 提前停止

提前停止是一个训练策略，最开始训练过程中，训练集和验证集的错误率都在下降，但训练多轮之后，模型可能会开始进一步学习噪声，此时训练集的错误率在下降但是验证集的错误率会提高。在这种情况下可以中止训练，通过调整超参数来防止模型过拟合

### 随机失活

针对网络结构模型，通过在训练过程中随机移除网络中的某些节点，能够防止网络的过拟合，提高泛化能力

随机失活的优势如下：

1. 防止节点之间权重依赖性，提高节点自适应能力
2. 实践证明，组合多个网络的模型能够有效提高泛化能力，随机失活操作相当于同时训练多个稀疏网络

### 最大值上限

最大值上限（`max-upper constraint`），也称为最大范数限制（`max-norm constraint`）用于避免权重过大，防止权重爆炸

设置一个最大值$c$（*通常设置为3或4，可通过验证集测试*），计算每个神经元的权重向量$\overrightarrow{w}$的L-2范数（平方和），如果$||\overrightarrow{w}||_{2} > c$，就缩放权重向量到$||\overrightarrow{w}||_{2} = c$

$$
w = \frac {w}{\sqrt{\sum (w^{2})}}\cdot c
$$
