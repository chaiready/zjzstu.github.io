---
title: AlexNet
categories: 
- [算法, 深度学习]
tags: AlexNet
abbrlink: ca9994d1
date: 2019-06-08 10:53:31
---

参考：[Understanding AlexNet](https://www.learnopencv.com/understanding-alexnet/)

`AlexNet`在`ImageNet LSVRC-2010`的`1000`类分类比赛上实现了`37.5% top-1`和`17.0% top-5`的最小误差率，在`LSVRC-2012`上实现了`15.3% top-5`的最小误差率，这些数据是当时最好的识别结果，其实现代码也在`google code`上公开：[cuda-convnet](https://code.google.com/archive/p/cuda-convnet/)

本文学习`AlexNet`网络结构及其训练方法

## 预处理

`AlexNet`除了减去均值以外，没有执行任何预处理操作 

## 网络架构

首先介绍最初的双`GPU AlexNet`架构，再介绍单`GPU`的`AlexNet`架构

### 双GPU AlexNet

在文章[ImageNet Classification with Deep Convolutional Neural Networks](http://xueshu.baidu.com/usercenter/paper/show?paperid=bfdf67dfdf8cea0c47038f63e91b9df1&site=xueshu_se)中，`AlexNet`使用双`GPU`进行训练，其结构如下所示：

![](/imgs/AlexNet/AlexNet.png)

`AlexNet`包含`8`个可学习层：`5`个卷积层 + `3`个全连接层组成

* 第二/四/五个卷积层的核仅与同一GPU的前一层进行连接
* 第三个卷积层的核与第二层全部连接
* 全连接层的核与前一层全部连接
* 局部归一化层跟随在第一二个卷积层后
* 重叠池化层跟随在局部归一化层后以及第`5`个卷积层后
* 第三、第四和第五个卷积层直接连接
* 卷积层和全连接层都使用`ReLU`作为激活函数

### 单GPU AlexNet

**单`GPU`训练的`AlexNet`模型如下（*已去除LRN*）：**

|       |  输入大小 | 滤波器大小 | 步长 | 零填充 | 滤波器个数 |  输出大小 |
|:-----:|:---------:|:----------:|:----:|:------:|:----------:|:---------:|
| CONV1 | 227x227x3 |   11x11x3  |   4  |    0   |     96     |  55x55x96 |
| POOL2 |  55x55x96 |     3x3    |   2  |    /   |     96     |  27x27x96 |
| CONV3 |  27x27x96 |   5x5x96   |   1  |    2   |     256    | 27x27x256 |
| POOL4 | 27x27x256 |     3x3    |   2  |    /   |     256    | 13x13x256 |
| CONV5 | 13x13x256 |   3x3x256  |   1  |    1   |     384    | 13x13x384 |
| CONV6 | 13x13x384 |   3x3x384  |   1  |    1   |     384    | 13x13x384 |
| CONV7 | 13x13x384 |   3x3x384  |   1  |    1   |     256    | 13x13x256 |
| POOL8 | 13x13x256 |     3x3    |   2  |    /   |     256    |  6x6x256  |
|  FC9  |  1x1x9216 |     1x1    |   /  |    /   |    4096    |  1x1x4096 |
|  FC10 |  1x1x4096 |     1x1    |   /  |    /   |    4096    |  1x1x4096 |
|  FC11 |  1x1x4096 |     1x1    |   /  |    /   |    1000    |  1x1x1000 |

### `224`还是`227`

在原文中作者输入图像大小为`224x224`，不过经过推算不符合网络计算，应该使用`227x227`作为输入图像大小

### 神经元和参数个数

参考：

[AlexNet中的参数数量](https://vimsky.com/article/3664.html)

[How to calculate the number of parameters of AlexNet?](https://stackoverflow.com/questions/40060949/how-to-calculate-the-number-of-parameters-of-alexnet)

整个网络约有`6`千万个参数和`65`万个神经元，计算如下：

* 输入层大小`227x227x3`，输入维数是`15,4587`

* 第一层卷积层卷积核大小为`11x11x3`，步长`4`，滤波器个数`96`，所以参数个数是`(11x11x3)x96+96=3,4944`，输出大小为`55x55x96=29,0400`

    * 池化层滤波器大小为`3x3`，步长`2`，所以输出大小为`27x27x96`

* 第二层卷积层卷积核大小为`5x5x96`，零填充`2`，滤波器个数`256`，所以参数个数是`(5x5x96)x256+256=61,4656`，输出大小为`27x27x256=18,6624`

    * 池化层滤波器大小为`3x3`，步长`2`，所以输出大小为`13x13x256`

* 第三层卷积层卷积核大小为`3x3x256`，零填充`1`，滤波器个数是`384`个，所以参数个数是`(3x3x256)x384+384=88,5120`，输出大小为`13x13x384=6,4896`

* 第四层卷积层卷积核大小为`3x3x384`，零填充`1`，滤波器个数是`384`个，所以参数个数是`(3x3x384)x384+384=132,7488`，输出大小为`13x13x384=6,4896`

* 第五层卷积层卷积核大小为`3x3x384`，零填充`1`，滤波器个数是`256`，所以参数个数是`(3x3x384)x256+256=88,4992`，输出大小为`13x13x256=4,3264`

    * 池化层滤波器大小为`3x3`，步长`2`，所以输出大小为`6x6x256`

* 第一层全连接层大小为`4096`，所以参数个数是`(6x6x256)x4096+4096=3775,2832`，输出大小为`4096`

* 第二层全连接层大小为`4096`，所以参数个数是`4096x4096+4096=1678,1312`，输出大小为`4096`

* 输出层大小为`1000`，所以参数个数是`4096x1000+1000=409,7000`

神经元总个数是`15,4587+29,0400+18,6624+6,4896+6,4896+4,3264+4096+4096+1000=81,3859`（*不包括输入层就是`65,9272`*）

参数总个数是`3,4944+61,4656+88,5120+132,7488+88,4992+3775,2832+1678,1312+409,7000=6237,8344`

## 特性

主要有`4`点：

1. 使用`ReLU`作为激活函数提高训练速度
2. 多`GPU`训练
3. 使用局部响应归一化(`LRN`)方法增加泛化能力
4. 使用重叠池化层（`Overlapping Pool`）提高泛化能力

### ReLU

之前标准的激活函数是`tanh()`和`sigmoid()`函数，文章中使用`ReLU(Rectified Linear Units，修正线性单元)`作为神经元激活函数

使用4层卷积神经网络训练`CIFAR-10`数据集，比较达到`25%`训练误差率的时间，使用`ReLU`能够比`tanh`快`6`倍

![](/imgs/AlexNet/relu.png)

### 多GPU训练

`AlexNet`使用两块`GTX 580 GPU`，在深度上进行切分，将每层一半神经元分别放置在不同`GPU`上进行训练

### 局部响应归一化

参考：[深度学习: 局部响应归一化 (Local Response Normalization，LRN)](https://cloud.tencent.com/developer/article/1347792)

数学实现如下：

![](/imgs/AlexNet/lrn.png)

经过`ReLU`激活后的卷积图，第i层上的位置为`(x,y)`的神经元值`a`，需要除以其相邻`n`个层相同位置的神经元值之和。常量k，n，$\alpha$，$\beta$都是超参数，需要通过验证集设定，当前设定为$k=2，n=5，\alpha=10^{-4}，beta=0.75$

其目的是**实现神经元的侧抑制**（`lateral inhibition`），在不同层之间进行竞争，使响应值大的神经元变得更大，并抑制其他较小的神经元

`LRN（Local Response Normalization，局部响应归一化）`能够提高泛化能力：在`ImageNet 1000`类分类任务中，`LRN`减少了`1.4% top-1`和`1.2% top5`的错误率；在`cifar-10`数据集测试中，一个`4`层神经网络能达到`13%`测试误差率（没有`LRN`）和`11%`测试误差率（有`LRN`）

***在之后其他网络的测试中发现`LRN`对训练结果提高几乎没有影响，所以不再使用***

### 重叠池化层

传统的池化层步长和滤波器大小相同（`s=z=2`），所以滤波器操作不会重叠

`alexnet`使用重叠池化（`Overlapping Pool`）操作，步长小于滤波器大小（`s=2,x=3,s<z`），这在`1000`类分类任务上能够实现`0.4% top-1`和`0.3% top-5`的提高。在训练过程中能够发现重叠池化模型更难以过拟合

## 避免过拟合手段

1. 数据扩充（`data augmentation`）
2. 随机失活（`dropout`）

### 数据扩充

文章中提到了两种方式

第一种方式是生成图像转换和水平映射。在训练阶段，获取`256x256`大小的数据集，再从中随机获取`227x227`大小的训练图像，同时通过水平映像(`horizontal reflection`)操作来扩大数据集；在测试阶段，使用同样方式裁剪5个$227\times 227$大小图像，并进行水平翻转，平均这10个图像预测结果作为输出

第二种方式是改变训练数据的通道强度，对于每个`RGB`图像像素$I_{xy}=[I_{xy}^{B},I_{xy}^{G},I_{xy}^{B}]^{T}$，添加如下值：

![](/imgs/AlexNet/pca.png)

其中$p_{i}$是第i个特征向量（`eigenvector`），$\lambda_{i}$是RGB像素值3x3协方差矩阵（`covariance matrix`）的特征值，$\alpha_{i}$是符合零均值(`mean zero`)和`0.1`标准方差(`standard deviation`)的服从高斯分布的随机变量，特定训练图像上的每个像素使用的$\alpha_{i}$都不相同，再次训练时需要重新设置$\alpha_{i}$

使用`PCA`改变图像强度的理论基础是自然图像的一个重要特性：物体同一性不随照明强度和颜色的变化而变化

这种方法减少了至少`1% top-1`误差率

### 随机失活

集合不同网络模型进行预测能够很好的减少测试误差，但是对于大网络而言需要耗费很多时间进行训练。随机失活（`dropout`）操作对中间隐含层进行操作，以`0.5`的概率判断该神经元是否失效，即这个神经元不进行前向操作，也不进行反向更新

有两点优势：

1. 每次进行训练都是在不同的网络架构上，与此同时这些不同的网络架构共享同一套权重
2. 减少神经元复杂的共适应性（`co-adaptation`），神经元不能依赖于某个特定的神经元

在测试阶段，对所有神经元的输出都乘以`0.5`，以获取指数多个`dropout`网络产生的预测分布的几何平均值

**在`alexnet`模型中，对前两个全连接层进行`dropout`操作**。如果没有`dropout`，整个网络会严重过拟合，并且训练过程达到收敛的时间大致增加了一倍

## 学习细节

批量大小为`128`，使用动量值为`0.9`，权重衰减率为`5e-4`，权重更新公式如下：

$$
v_{i+1} = 0.9\cdot v_{i} - 0.0005\cdot lr\cdot w_{i} - lr \triangledown w_{i}\\
w_{i+1} = w_{i} + v_{i+1}
$$

权重初始化为`0`均值，`0.01`标准差的高斯分布

第二、第四、第五个卷积层层和全连接层的偏置值初始化为$1$，其他层的偏置值初始化为$0$

学习率初始化为`0.01`，在终止之前共减少了`3`次
